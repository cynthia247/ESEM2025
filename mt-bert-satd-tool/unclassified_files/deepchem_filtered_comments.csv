Commit Message
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Select a device.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
Save the checkpoint to a file.
Rename and delete older files.
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
Annotate start/stop of sequence
Open index file
create an empty list to store lines in files.
iterate through each line in the input file
If the number of lines iterated through is equal or less than the shard size:
append to list
else yield the list
set the line_number variable to the last line number (num) before 'yield' was called
yield list (shard/batch)
Re-initialize list with the index line to begin a new shard.
Set attributes
Handle special featurizer cases
Set self.featurizer
Set self.return_quality_scores
Featurize sequences
"(X, y , w, ids)"
Featurize sequences
"(X, y , w, ids)"
Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
First line : header description
second line : sequence
third line : more description usually the same as the first line
fourth line: quality scores of the sequence
Second line : add sequence to the sequence array
Fourth line
Handle empty sequence
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
Sort image files
Sometimes zip files contain directories within. Traverse directories
Sort label image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
The line here assumes that y generated by shard_generator is a numpy array
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Try specifying particular columns
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create directory of multiple image files
Zip directory of multiple image files
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
These are the known dimensions of a_image.tif
These are the known dimensions of a_image.tif
"Since the different files have different shapes, makes an object array"
Test that the order of the contents of an unzipped file is preserved.
Load the zip file
Load multi_path directly
Check that the order of the files is the same
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"Expected shape is now (4, 192, 5)"
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
safe operations
occupation number gradients
get the length of the tensor output
other tensor ops
add the diagonal with a small eps to safeguard from nan
replace the diagonal with infinite (usually used for coulomb matrix)
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
data.shape and segment_ids.shape should be equal
"return V.set_(V.storage(), V.storage_offset(), V.size(), tuple(reversed(V.stride())))"
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
isotope-averaged atom masses in a.m.u.
from https://www.angelo.edu/faculty/kboudrea/periodic/structure_mass.htm
"JCP 41, 3199 (1964); DOI:10.1063/1.1725697"
taken from PySCF:
https://github.com/pyscf/pyscf/blob/45582e915e91890722fcae2bc30fb04867d5c95f/pyscf/data/radii.py#L23
I don't know why H has 0.35 while in the reference it is 0.
"They are in angstrom, so we need to convert it to Bohr"
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
NOTE We have lot of type ignores here since grover mol-graph which is of type
"GraphData have kwargs which are not attributes of GraphData. Hence, in these"
cases mypy raises GraphData does not have attributes `..`.
max with 1 to fix a crash in rare case of all single-heavy-atom mols
graph_index indicates which atom belongs to which molecule
padding
computing a2b
only needed if using atom messages
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
"For a node-prediction task, label is not added to edge features and other global features"
because label here is a node-level attribute and not a graph-level attribute
"In this case, the 'y' attribute of GraphData will contain the"
node-level labels.
not a self-loop
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
for each node (E[(X-E[X])^n])^{1/n}
EPS is added to the absolute value of expectation before taking the nth root for stability
"each scaler is a function that takes as input X (B x N x Din), adj (B x N x N) and"
avg_d (dictionary containing averages over training set) and returns X_scaled (B x N x Din) as output
Generate the raising operator matrix
Generate the lowering operator matrix
Generate the z-generator matrix
"Combine the matrices to form the x, z, and y generators"
Stack the generators along the first dimension to create a tensor
The transformation matrix generated is used to change the basis of a vector of
real spherical harmonics with representation index 1 to complex spherical harmonics.
"Construct the transformation matrix Q for m in range(-k, 0)"
Set the diagonal elements for m = 0
"Construct the transformation matrix Q for m in range(1, k + 1)"
Apply the factor of (-1j)**k to make the Clebsch-Gordan coefficients real
Handle dtype and device options
Ensure the tensor is contiguous and on the specified device
Get the SU(2) generators for the given quantum angular momentum (spin) value.
Get the transformation matrix to change the basis from real to complex spherical harmonics.
Convert the SU(2) generators to the SO(3) basis using the transformation matrix Q.
"X represents the SU(2) generators, and Q is the transformation matrix from real to complex spherical harmonics."
The resulting X matrix will be the SO(3) generators in the complex basis.
Return the real part of the SO(3) generators to ensure they are purely real.
"Ensure that alpha, beta, and gamma have the same shape for broadcasting."
"Ensure the angles are within the range [0, 2*pi) using modulo."
Get the SO(3) generators for the given quantum angular momentum (spin) value 'k'.
Calculate the Wigner D matrix using the matrix exponential of the generators
"and the rotation angles alpha, beta, and gamma in the appropriate order."
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
Warnings
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Calculation of Step Size and steps
Number of atoms per molecule is calculated by counting all the non zero elements(numbers) of every molecule.
"It loops over the molecules in the Coulomb matrix and takes the ""2.4"" root of the diagonal of ""2X"" of each molecule's representation."
Calculates the Gaussian Distance by passing distance by a gaussian function.
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
dtype=object allows for arrays(images here) of arbitrary size
Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
Structures are stored in .sdf file
"Note: Here, the order of columns is based on the order in which the values"
"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
"tasks above, they occur after `tasks` here."
"FIXME Ideally, we should use something like a dictionary here to keep it independent"
of column ordering.
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
"pattern to split the names, e.g. ""model.params[1]"" into [""model"", ""params"", ""[1]""]"
"return: (nao, nao)"
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
get the unique parameters of A
separate the parameters for A and for M
"grad_x: (*BABEM, nr, ncols)"
"x: (*BABEM, nr, ncols)"
solve (A-biases*M)^T v = grad_x
this is the grad of B
calculate the grad of matrices parameters
calculate the biases gradient
calculate the gradient to the biases matrices
Hidden
"NOTE: currently only works for batched B (1 batch dim), but unbatched A"
check the parameters
convert the numpy/scipy
NOTE: The line below is very inefficient for large na and ncols
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
correct the residual calculation
check for the stopping condition
move to the next index
"x: (ncols, *, nr, 1)"
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
correct the residual calculation regularly
calculate the residual
save the best results
check for the stopping conditions
"x: (ncols, *, nr, 1)"
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
res = res * B_norm
save the best results
general helpers
get the linear operator (including the MXE part)
"A: (*BA, nr, nr) linop"
"B: (*BB, nr, ncols)"
"E: (*BE, ncols)"
"M: (*BM, nr, nr) linop"
"x: (ncols, *BX, nr, 1)"
"x: (ncols, *BX, nr, 1)"
estimate if it's posdef with power iteration
set posdef to False to make the operator becomes AT * A so it is
hermitian
TODO: the posdef check by largest eival only works for Hermitian/symmetric
"matrix, but it doesn't always work for non-symmetric matrix."
"In non-symmetric case, one need to do Cholesky LDL decomposition"
"if the largest eigenvalue is negative, then it's not posdef"
"otherwise, calculate the lowest eigenvalue to check if it's positive"
get the linear operation if it is not a posdef (A -> AT.A)
cg and bicgstab helpers
rootfinder-based
using rootfinder algorithm
set up the function for the rootfinding
"xi: (*BX, nr*ncols)"
setup the initial guess (the batch dimension must be the largest)
check if xnorm is converging
Broadcast Utilities
check the hermitian
check which methods are implemented
@abstractmethod
@abstractmethod # (optional)
@abstractmethod
@abstractmethod
implemented functions
use batched mv as mm
move the last dimension to the very first dimension to be broadcasted
apply batched mv and restore the initial shape
use batched mv as mm
move the last dimension to the very first dimension to be broadcasted
apply batched mv and restore the initial shape
special functions
properties
implementation
private functions
"xt: (*BY, p)"
"xdummy: (*BY, q)"
calculate y = Ax
calculate (dL/dx)^T = A^T (dL/dy)^T with (dL/dy)^T = xt
Helper Classes
"if it is a method from an object, unroll the parameters and add"
the object's parameters as well
get the unique ids
search the id if it has been added to the list
debugging
check the method input
assert if the method preserve the float tensors of the object
now assert if all_params0 == all_params1
get all tensor parameters in the object
get the parameter tensors used in the operation and the tensors specified by the developer
check if the userparams contains non-tensor
"check if there are missing parameters (present in operating params, but not in the user params)"
if oper_names[i] not in user_names:
"if there are missing parameters, give a warning (because the program"
"can still run correctly, e.g. missing parameters are parameters that"
are never set to require grad)
"check if there are excessive parameters (present in the user params, but not in the operating params)"
if user_names[i] not in oper_names:
"if there are excess parameters, give warnings"
get all the tensors recursively
copy the tensors and require them to be differentiable
run the method and see which one has the gradients
return the original tensor
traversing functions
None is set as default arg to avoid expanding list for multiple
invokes of _get_tensors without exception_ids argument
add exception to avoid infinite loop if there is a mutual dependant on objects
get the tensors recursively towards torch.nn.Module
traverse down the object to collect the tensors
traverse down the object to collect the tensors
flake8: noqa
TODO: implement robust LOBPCG and put it here
get the unique parameters of A & M
adapted from scipy.sparse.linalg.svds
clamp the eigenvalues to a small positive values to avoid numerical
instability
separate the sets of parameters
options for calculating the backward (not for `solve`)
save for the backward
"evals: (*BAM, neig)"
"evecs: (*BAM, na, neig)"
get the variables from ctx
set the default values of degen_*tol
check the degeneracy
"idx_degen: (*BAM, neig, neig)"
the loss function where the gradient will be retrieved
"warnings: if not all params have the connection to the output of A,"
it could cause an infinite loop because pytorch will keep looking
for the *params node and propagate further backward via the `evecs`
path. So make sure all the *params are all connected in the graph.
"if degenerate, check the conditions for finite derivative"
"if the requirements are not satisfied, raises a warning"
calculate the contributions from the eigenvalues
calculate the contributions from the eigenvectors
orthogonalize the grad_evecs with evecs
"Based on test cases, complex datatype is more likely to suffer from"
"singularity error when doing the inverse. Therefore, I add a small"
offset here to prevent that from happening
orthogonalize gevecs w.r.t. evecs
accummulate the gradient contributions
the contribution from the parallel elements
"evals: (*BAM, neig)"
get the index of degeneracies
contracted using opt_einsum
"evals, evecs = torch.linalg.eigh(Amatrix, eigenvectors=True)  # (*BA, q), (*BA, q, q)"
M decomposition to make A symmetric
it is done this way to make it numerically stable in avoiding
complex eigenvalues for (near-)degenerate case
calculate the eigenvalues and eigenvectors
(the eigvecs are normalized in M-space)
"evals, evecs = torch.linalg.eigh(A2, eigenvectors=True)  # (*BAM, q, q)"
temporary solution to https://github.com/pytorch/pytorch/issues/47599
remove the degenerate part
see https://arxiv.org/pdf/2011.04366.pdf
take the contribution from the eivec
calculate the contribution from the eival
symmetrize to reduce numerical instability
TODO: optimize for large linear operator and strict min_eps
Ideas:
(1) use better strategy to get the estimate on eigenvalues
(2) use restart strategy
get the shape of the transformation
set up the initial guess
Can be optimized by saving AV from the previous iteration and only
operate AV for the new V. This works because the old V has already
"been orthogonalized, so it will stay the same"
"AV = A.mm(V) # (*BAM,na,nguess)"
eigvals are sorted from the lowest
"eval: (*BAM, nguess), evec: (*BAM, nguess, nguess)"
calculate the eigenvectors of A
calculate the residual
print information and check convergence
apply the preconditioner
orthogonalize t with the rest of the V
orthogonalize V
check idxs
make the function a functional (depends on all parameters in the object)
params tensor is the LinearOperator's parameters
"if the object parameter is still the same, then use the pre-calculated values"
"otherwise, reevaluate by replacing the parameters with the new tensor params"
self.yfcn: (*nin)
file mostly from SciPy:
https://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/optimize/nonlin.py#L221
and converted to PyTorch for GPU efficiency
jacobian parameters
stopping criteria
algorithm parameters
misc parameters
"solving complex rootfinder by concatenating real and imaginary part,"
making the variable twice as long
represents x as a long real vector
pack a long real vector into the shape accepted by fcn
shorthand for the function
set up the jacobian
solver tolerance
save the best results
print out dx and df
adjust forcing parameters for inexact solve
jacobian parameters
stopping criteria
algorithm parameters
misc parameters
"No suitable step length found. Take the full Newton step,"
and hope for the best.
"Otherwise, compute the minimizer of a quadratic interpolant:"
"Otherwise, loop with cubic interpolation until we find an alpha which"
"satisfies the first Wolfe condition (since we are backtracking, we will"
assume that the value of alpha is not too small and satisfies the second
condition.
Failed to find a suitable step length
gd parameters
stopping conditions
misc parameters
update the step
check the stopping conditions
gd parameters
stopping conditions
misc parameters
update the step
check the stopping conditions
get the best values
usually user set maxiter == 0 just to wrap the minimizer backprop
taking most of the part from SciPy
setup the approximate inverse Jacobian
update Gm
keep the rank small
keep the rank small
raise RuntimeError
"u: (n, 1), s: (1,), vh: (1, n)"
"equilibrium can use all rootfinder methods, but there are several methods developed specifically for"
equilibrium (or fixed-point iterations). This dictionary gives the list of those special methods.
"minimization can use rootfinder algorithm, so check if it is actually"
"using the optimization algorithm, not the rootfinder algorithm"
the rootfinder algorithms are designed to move to the opposite direction
"of the output of the function, so the output of this function is just"
the grad of z w.r.t. y
"if it is going to optimization method, then also returns the value"
"if using the optimization algorithm, then the forward function is the one"
that returns f and grad
"if it is just using the rootfinder algorithm, then the forward function"
is the one that returns only the grad
set default options
split tensors and non-tensors params
merge the tensor and nontensor parameters
dL/df
get the grad for the params
anderson_acc parameters
stopping criteria
misc options
"x0: (..., *nfeats)"
"reshape x to have a shape of (batch_size, feats_dim)"
"x: (..., *nfeats)"
"xn: (..., feats_tot)"
"x: (..., feats_dim)"
"torch.bmm(g, g.transpose(-2, -1))"
"alpha: (batch_size, nsize)"
check the stopping condition
update the xn
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
"6 atoms: CC -> 2, CCC -> 3"
"7 bonds: CC -> 2, CCC -> 4 (bonds are considered as undirected"
and a single bond contributes to 2 bonds)
graph-level labels
node-level labels
graph.y contains node-labels and graph.node_features.shape[0]
holds number of nodes in that graph
Get Data
Checks that all atoms exits in array
Checks shape of gaussian distance
Checks all molecule membership exist
Check Distance Membership shape
Prepare Data
Run
Prepare Data
Inputs property
Without reverse input
With revercse input
Prepare Data
Inputs property
TODO test more formats for ligand
Test if the output has the correct shape.
Test for the case of zero momentum (j=0).
Test for the case of momentum j=1 (spin-1).
"Expected J_x, J_z, J_y matrices for j=1"
"Test for j = 0, which means we have a 1x1 transformation matrix"
"Test for j = 2, which means we have a 5x5 transformation matrix"
Test for device placement (CPU to CUDA)
Test for dtype conversion (complex128 to complex64)
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
dummy function which can be passed as the parameter f to simultaneous_move and single_move
test for gauss_initialize_position
testing symmetric simultaneous_move
testing asymmetric simultaneous_move
testing symmetric single_move
testing asymmetric single_move
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(3)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
get the location and weights of the integration in its original
coordinate
get the coordinate in Cartesian
integration element
Grid Transformations
"xnew is from [xmin, xmax]"
"r is approximately from [rmin, rmax]"
type of the atom Z
input types
"if the basis has been normalized before, then do nothing"
normalize to have individual gaussian integral to be 1 (if coeff is 1)
normalize the coefficients in the basis (because some basis such as
def2-svp-jkfit is not normalized to have 1 in overlap)
input basis type
QR decomposition's solution is not unique in a way that every column
can be multiplied by -1 and it still a solution
"So, to remove the non-uniqueness, we will make the sign of the sum"
positive.
construct the rotation parameters
calculate the orthogonal orbital
"orb: (*, nao, norb)"
the orbital becomes the coefficients while params is all zeros (no rotation)
properties
setups
fock matrix components
interface to dm
energy of the Hamiltonian
free parameters for variational method
Editable module
1 / cell.vol == det(b) / (2 pi)^3
drop ls that has norm > rcut * 1.05
System Properties
all-time calculations
(i.e. meaning it does not have to be executed to run the functions below)
"densinfo.value & lapl: (*BD, nr)"
"densinfo.grad: (*BD, ndim, nr)"
"return: (*BD, nr)"
"densinfo.value & lapl: (*BD, nr)"
"densinfo.grad: (*BD, ndim, nr)"
return:
"potentialinfo.value & lapl: (*BD, nr)"
"potentialinfo.grad: (*BD, ndim, nr)"
mark the densinfo components as requiring grads
"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
set the vars to require grad and returns the previous state of the vars
restore the state of requiring grad based on reqgrads list
all vars before this function requires grad
getting which parameters should require grad
set the params to require grad
special operations
properties
TODO: use regex!
convert the atomz to tensor
convert the atompos to tensor
"convert to dtype if atomzs is a floating point tensor, not an integer tensor"
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
Break the loop if max_records is set
Break the loop if max_records is set
Break the loop if max_records is set
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
biological sequence featurizers
tokenizers
support classes
dqc dependencies
get the density matrix from PySCF's CCSD calculation
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
condition if the original atom order is required
"mol must be a RDKit Mol object, so parse a SMILES"
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
"np.max() method works only for a non-empty array, so size of the array should be non-zero"
Adding shapes of kwargs
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
Create a mapping from the original node indices to the new node indices
Filter and reindex node features
Filter and reindex edge indices and edge features
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
number of nodes in each graph
cumulative number of nodes for each graph. This is necessary because the values in edge_index are node indices of all of the graphs in graph_list and so we need to offset the indices by the number of nodes in the previous graphs.
"columns are the edge index, values are the node index"
graph_index indicates which nodes belong to which graph
Batch user defined attributes
Convert edge_index to adjacency list
Breadth-first search
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Dimension of atom feature vector
len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
+ 2 at end for is_in_aromatic and mass
dictionary of available feature generators
for H2
"not all features are equally long, so used methane as dummy molecule to determine length"
Fix nans in features
add edge list considering a directed graph
get atom features
get edge(bond) features
get edge index
get global features
0 represents a masked bond
atoms
bonds
"Graph connectivity in COO format with shape [2, num_edges]"
"Edge feature matrix with shape [num_edges, num_edge_features]"
Always treat the bond as directed.
add mapping between bond b1 and atom a2 (destination atom)
add mapping between bond id and atom id (a1)
add mapping between bond id and atom a1 (source atom)
update index on bond and reverse bond mappings
generate SMILES for fragments
Featurize data using featurize() in parent class
Featurize str data
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
"row, col = edge_index"
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
get atom features
get edge index
user has not specified a descriptor list
creates normalized functions dictionary if normalized features are required
get cdf(feature) for that descriptor
get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
get required distribution_ from `scipy.stats` module.
cdf => cumulative density functions
make the cdf with the parameters.
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
similar to SNAP featurizer. both taken from Open Graph Benchmark (OGB) github.com/snap-stanford/ogb
"The difference between this and the SNAP features is the lack of masking tokens, possible_implicit_valence_list, possible_bond_dirs"
"and the prescence of possible_bond_stereo_list,  possible_is_conjugated_list, possible_is_in_ring_list,"
FIXME Add support for multiple conformers (wip)
"def __init__(self, num_conformers: int = 1, rmsd_cutoff: float = 2):"
""""""""
Initialize the RDKitConformerFeaturizer with the given parameters.
Parameters
----------
"num_conformers : int, optional, default=1"
The number of conformers to generate for each molecule.
"rmsd_cutoff : float, optional, default=2"
The root-mean-square deviation (RMSD) cutoff value. Conformers with an RMSD
greater than this value will be discarded.
""""""""
self.num_conformers = num_conformers
self.rmsd_cutoff = rmsd_cutoff
Derived from https://github.com/HannesStark/3DInfomax/blob/5cd32629c690e119bcae8726acedefdb0aa037fc/datasets/qm9_dataset_rdkit_conformers.py#L377
add hydrogen bonds to molecule because they are not in the smiles representation
FIXME Add support for multiple conformers (wip)
"AllChem.EmbedMultipleConfs(mol, self.num_conformers)"
AllChem.MMFFOptimizeMolecule(mol)
rmsd_list = []
"rdMolAlign.AlignMolConformers(mol, RMSlist=rmsd_list)"
# insert 0 RMSD for first conformer
"rmsd_list.insert(0, 0)"
conformers = [
mol.GetConformer(i)
for i in range(self.num_conformers)
if rmsd_list[i] < self.rmsd_cutoff
]
"# if conformer list is less than num_conformers, pad by repeating conformers"
conf_idx = 0
while len(conformers) < self.num_conformers:
conformers.append(conformers[conf_idx])
conf_idx += 1
coordinates = [conf.GetPositions() for conf in conformers]
add edges in both directions
"Graph connectivity in COO format with shape [2, num_edges]"
FIXME Add support for multiple conformers (wip)
graph_list = []
for i in range(self.num_conformers):
graph_list.append(
"GraphData(node_pos_features=np.array(coordinates[i]),"
"node_features=np.array(atom_features_list),"
"edge_features=np.array(edge_features_list),"
edge_index=np.array(edges_list).T))
return graph_list
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
"sort first by frequency, then alphabetically"
"sort first by frequency, then alphabetically"
sorting the atoms neighbors
concatenating the sorted neighbors
"sort first by frequency, then alphabetically"
"sort first by frequency, then alphabetically"
flake8: noqa
superclass accepts a DeepChem dataset while huggingface vocabulary builders
reads data from file
test with max size
test build from csv
test with max size
load the vocabulary by passing filename
test tokenization of a single point
load the vocabulary by passing the filename
test tokenization of a single point
Build vocabulary by wrapping in huggingface vocabulary builder
Load vocabulary and do a basic sanity check on the vocabulary
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
number of indices where feature count is more than 1
no normalized feature value should be greater than 1.0
"151 = 133 + 18 (133 -> one hot encoding from _ATOM_FEATURES, 18 other features)"
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
NOTE: The test depends on the the pretrained vocabulary
(seyonec/PubChem10M_SMILES_BPE_60k). If the pretrained vocabulary is modified
"(which can be since it is an external resource), the test might fail."
construct edge (bond) index
add edge list considering a directed graph
test for 'MolGraphConvFeaturizer' class
"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
"Requirements - transformers, tokenizers"
no normalized feature value should be greater than 1.0
these are the properties used in grover
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
Test featurizer with atom 3-D coordinates as kwargs
load sample dataset
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"number of indices, where feature count is more than 1, should be 0"
number of indices where feature count is more than 1
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
both reactant and product are null
reactant is null
product is null
valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
z is kwargs
check convert function
z is kwargs
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Set up tests.
Set up testing parameters.
the atom order for 'C' is same in case of canonical and original ordering
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
"bond and its reverse bond, therefore num_bonds * 2 edges"
9 atom features
"Graph connectivity in COO format with shape [2, num_edges]"
3 bond features
3 xyz coordinates for each atom in the conformer
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
Calculate frequency
flake8: noqa
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Return identity if no tuple to split to
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Invalid because valence for atom 5 N is greater than permitted (4)
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
NOTE The below comment is from original source code
dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss
return av_loss + fg_loss + dist_coff * dist_loss
"return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss"
We just return overall_loss since TorchModel can handle only a single loss
loss for nodes
converting the binary classification to multiclass classification
positive context prediction is the dot product of substructure representation and true context representation
negative context prediction is the dot product of substructure representation and negative (random) context representation.
positive substructure prediction is the dot product of expanded substructure representation and true overlapped node representation.
shift indices of substructures to create negative examples
negative substructure prediction is the dot product of shifted expanded substructure representation and true overlapped node representation.
Compute the loss for positive and negative context representations
The final loss is the sum of positive and negative context losses
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Dependency Models
scikit-learn model
PyTorch models
Pytorch models with torch-geometric dependency
TODO We should clean up DMPNN and remove torch_geometric dependency during import
Pytorch-lightning modules import
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
"Create an identical model, do a single step of fitting with restore=True and make sure it got restored correctly."
check that the first layer is still the same between the two models
check that the predictions are different because of the fine tuning
check that the first layer is different between the two models
Conv2d and Linear layers test(CNN classification)
Fit trained model
Eval model on train
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
testing batch size > 1
testing true values
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
https://github.com/diffqc/dqc/blob/742eb2576418464609f942def4fb7c3bbdc0cd82/dqc/test/test_xc.py#L15
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
.8 to save resources for a difficult task
.2 to save resources for a difficult task
first iteration loss is around 50
The first pass of the transformation should be 0
Test sampling method
Test log_prob method (this method is used when inverse pass)
Output must be a Nth zero array since nothing is being learned yet
Featurize to assert for tests
Assert errors for sample method
Assert errors for log_prob method
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec]"
atom features
Try with compression
"Outputs should be [mol1_vec, mol2_vec]"
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
fit the model
reload the model
index of pair features
number of pairs for each atom
atom features
pair features
Assigning tensorflow equivalent weights to torch layer
"Outputs should be [A, P]"
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = Metric(
"roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
xgboost test
fit trained model
"If ES rounds are more than total epochs, it will never trigger"
Find the number of boosting rounds in the model
"If rounds boosted are less than total estimators, it means ES was triggered"
prepare dataset
lightgbm test
fit trained model
"If ES rounds are more than total epochs, it will never trigger"
Find the number of boosting rounds in the model
"If rounds ran are less than estimators, it means ES was triggered"
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Testing graphnet for a single graph
Testing for consistency
Testing with a batch of Graphs
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
TODO The test is skipped as FakeGraphGenerator has to be updated
to generate regression labels
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
"Linear layers for making query, key, value"
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Set up tests.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Finetune model weights should not match before loading from pretrained model
Finetune model weights should match after loading from pretrained model
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
Test for the init function of FerminetModel class
Testing ionic initialization
Testing whether error throws up when spin is wrong
Testing the spin values
Testing ionic initialization
Test for the evaluate_hf_solution function of FerminetModel class
"The solution should be of the shape (number of electrons, number of electrons)"
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
The highest recorded score should match get_best_score().
Reload the save model and confirm that it matches the best logged score.
Make sure get_best_score() still works when save_dir is not specified
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
"n_atoms = 4  # In CCC and C, there are 4 atoms"
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
TODO(rbharath): What is the shape of outputs supposed to be?
"I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
"By setting the `box_size` to effectively zero, the result should only contain `nan`."
Check that layer has three trainable parameters.
Check when `box_size` is of wrong dimensionality.
Check when `inputs` is of wrong length.
Create random local node representations and global graph representations
Compute similarity scores using the discriminator
Check if the output has the correct shape and dtype
total_n_atoms = 4
n_atom_feat = 4
"atom_feat = np.random.rand(total_n_atoms, n_atom_feat)"
Embeddings and results from Tensorflow implementation
Weights and Embeddings from Tensorflow implementation
init parameters
generate features for testing
index of pair features
atom features
pair features
tensors for torch layer
assigning tensorflow layer weights to torch layer
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Dataset of SMILES strings for testing SeqToSeq models.
Train the model on random sequences. We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Initialize the model
Fit the Model
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
Note: Following are some changes
compared to the TensorFlow unit test:
1. Changed nb_epoch to 300.
2. Increased the learning_rate to 0.0003.
Note: This needs to be inspected in future to understand low score as compared to a score of 0.9 in tensorflow unit test.
Note: Following are some changes
compared to the TensorFlow unit test:
1. Changed nb_epoch to 400.
2. Reduced the number of data points to 2.
3. Increased batch_size to 20.
4. Increased the learning_rate to 0.0003.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Note: This needs to be inspected in future to understand low score.
Load mini log-solubility dataset.
Fit trained model
Note: Compared to the tensorflow unit test increased the nb_epoch to 600.
Eval model on train
Note: This needs to be inspected in future to understand low score as compared to a score of 0.8 in tensorflow unit test.
Create input tensor with values within full_atom_feature_dims
Create input tensor with values within full_bond_feature_dims
Compute the global graph representation
Compute positive and negative scores
Check if the loss is a scalar and has the correct dtype
Check if the loss is a scalar and non-negative
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
Get Data
Check Shape
Check number of parameters
Eval model on train
load sample dataset
initialize the model
fit the model
reload the model
atom features
Gaussian histograms expands into 11 Gaussian buckets.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
edges logits used during training
nodes logits used during training
edges logits
nodes logits
For training
For sample generation
Define the dense layers
"Ignoring type. For TorchModel, loss is a required argument but HuggingFace computes"
"loss during the forward iteration, removing the need for a loss function."
FIXME Transformers library has an api like AutoModel.from_pretrained. It allows to
initialise and create a model instance directly without requiring a class instance initialisation step.
"To use `load_from_pretrained` in DeepChem, we need to follow a two step process"
of initialising class instance and then loading weights via `load_from_pretrained`.
y is None during predict
Main training loop.
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
copy the input graph to avoid in-place operations
"FIXME For pretraining task, both model2d and model3d but the super class"
"can't handle two models for contrastive learning, hence we pass only model2d"
torch's one-hot encoding works with integer data types.
"We convert labels to integer, one-hot encode and convert it back to float"
for making it suitable to loss function
graphs = [[
graph_data.to_dgl_graph().to(self.device) for graph_data in row
] for row in inputs]
convert the GraphData objects to DGL graphs
"TODO Ideally, we should use a lr schedule but we need to update lr_scheduler.step() method"
in ModularTorchModel.fit_generator to accept a metric.
"self._lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(self._pytorch_optimizer,"
mode='min')
Initialize buffers
Accumulate statistics for Fisher matrices
Initialize buffers
p_grad_mat is of output_dim * input_dim
inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
we always put gradient w.r.t weight in [0]
and w.r.t bias in [1]
do kl clip
TODO Explain in detail what the four outcompes are
The bond and rev bond have even and odd ids respectively.
FIXME This layer is similar to DMPNNEncoderLayer and they
must be unified.
Shared weight matrix across depths (default)
Except reverse bond its-self(w) ! \sum_{k\in N(u) \ w}
"FIXME When input_layer is none, for the first iteration of message passing, we should ideally"
be using different weight matrix since message will be of shape num_bonds x f_bonds_dim
"in the first iteration and in the subsequent iterations, it will be num_bonds x hidden_size"
FIXME We assume that we are using a hidden layer to transform the initial atom message
and bond messages to hidden dimension size.
self.atom_messages is False
Note: Elementwise affine has to be consistent with the pre-training phase
multi-headed attention
support no residual connection in MTBlock.
atom input to atom output
bond to atom
atom input to bond output
bond input to bond output
Inputs
Noise Input
Data Input
Data Inputs
Conditional Input
Conditional Inputs
Generators
Discriminators
Forward pass through generators
Forward pass through discriminators
Compute loss functions
Compute the weighted errors
Create learnable weights for the generators and discriminators.
Compute the weighted errors
Add an entropy term to the loss.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
PyTorch layers require input and output channels as parameter
"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
initializing layer bias with nn.init gives mypy typecheck error
using the following workaround
residual blocks can only be used when successive layers have the same output shape
Used for converting edges back to their original shape
Compute mean edge features for each node by dst_index (each node
"receives information from edges which have that node as its destination,"
hence the computation uses dst_index to aggregate information)
holding bi-directional edges in case of undirected graphs
coonverting edge features to its original shape
Input
Shared weight matrix across depths (default):
For messages hidden states
For atom hidden states
num_atoms x hidden_size
num_molecules x hidden_size
concat global features
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
No other forget biases supported right now.
Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
Define the layers
Create the final layers
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set `pad_batches=True`
"z = torch.mm(h, self.U) + self.b"
create a boolean mask for each partition
partition the input tensor using the masks
Case when >2 inputs are passed
Loop over the remaining convolution layers
Apply the current layer to the outputs from the previous layer
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
Construct internal trainable weights
Weight matrix and bias matrix required to compute new atom layer from the previous atom layer
Weight matrix and bias matrix required to compute new atom layer from the previous pair layer
Weight matrix and bias matrix required to compute new pair layer from the previous atom layer
Weight matrix and bias matrix required to compute new pair layer from the previous pair layer
flake8: noqa
Converting the input to torch tensors
"AA is a tensor with shape[total_num_atoms,n_hidden_AA]"
"PA is a tensor with shape[total number of pairs,n_hidden_PA]"
Split the PA tensor according to the 'pair_split' tensor
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"PP is a tensor with shape [total number of pairs,n_hidden_PP]"
Integrate the Cross Layer Mapping inside the Global Message Passing
Message Passing operation
Update function f_u
Message Passing operation
Teacher forcing: Feed the target as the next input
Without teacher forcing: use its own predictions as the next input
Initializing the first layer (first layer has different dims than others)
filling the weights with xavier uniform method for the linear weights and random assignment for the bias
Calculating one-electron feature's average
temporary lists containing each electron's embeddings which will be torch.stack on the end
Calculating two-electron feature's average
"initialized weights with torch.zeros, torch.eye and using xavier init."
temporary list to stack upon electrons axis at the end
Integrate the Cross Layer Mapping inside the Local Message Passing
Message Passing 1
Message Passing 2
Aggregation
Update function f_u
Output Module
import torch.nn as nn
creating one and two electron features
setting the fermient layer and fermient envelope layer batch size to be that of the current batch size of the model. This enables for vectorized calculations of hessians and jacobians.
using functorch to calcualte hessian and jacobian in one go
using index tensors to index out the hessian elemennts corresponding to the same variable (cross-variable derivatives are ignored)
"doing all the calculation and detaching from graph to save memory, which allows larger batch size"
cloning self.input which will serve as the new input for the vectorized functions.
lambda function for calculating the log of absolute value of the wave function.
using jacrev for the jacobian and jacrev twice for to calculate the hessian. The functorch's hessian function if directly used does not give stable results.
making the batch size temporarily as 1 for the vectorization of hessian and jacobian.
Initialization for ionic molecules
hook function below is an efficient way modifying the gradients on the go rather than looping
using non-local variables as a means of parameter passing
the move function calculates the energy of sampled electrons and samples new set of electrons (does not calculate loss)
clipping local energies which are away 5 times the variance from the median
using the sampled electrons from the electron sampler for bacward pass and modifying gradients
going through each step of random walk and calculating the modified gradients with local energies
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
mypy check is ignored for global_features as it is not a default attribute
of GraphData. It is created during runtime using **kwargs.
mapping from bond index to the index of the atom (where the bond is coming from)
"mapping from bond index to concat(in_atom, bond) features"
mapping from atom index to list of indicies of incoming bonds
mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
zero padded at the end
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
padded with -1 at the end
mapping from bond index to the index of the atom (where the bond if going to)
mapping from atom index to list of indicies of incoming bonds
get maximum number of incoming bonds
Make number of incoming bonds equal to maximum number of bonds.
This is done by appending -1 to fill remaining space at each atom indices.
mapping from bond index to the index of the reverse bond
get encoder
get input size for ffn
get output size for ffn
get ffn
Steps to get `molecules_unbatch_key`:
1. Get the tensor containing the indices of first atoms of each molecule
2. Get the tensor containing number of atoms of each molecule
by taking the difference between consecutive indices.
3. Convert the tensor to a list.
num_molecules x (enc_hidden + global_features_size)
ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
"atom feature matrix with shape [number of atoms, number of features]"
concatenated feature vector which contains concatenation of initial atom and bond features
mapping from atom index to list of indicies of incoming bonds
mapping that maps bond index to 'array of indices of the bonds'
incoming at the initial atom of the bond (excluding the reverse bonds)
array of global molecular features
maximum number of incoming bonds in the batch
generate concatenated feature vector and mappings
pad all mappings to maximum number of incoming bonds in the batch
the hidden size here is the output size of last layer in mol_atom_from_atom_ffn and mol_atom_from_bond_ffn components.
it is necessary that aforementioned components produces output tensor of same size.
"In training mode, we return atom level aggregated output and bond level aggregated output."
The training has an additional objective which ensures that atom and bond level aggregated outputs
are similar to each other apart from the objective of making the aggregated output closer to each
other.
"FIXME In the above step, we initialize modular torch model but"
something is missing here. The attribute loss from TorchModel gets assigned `loss_func`
by super class initialization in ModularTorchModel but here we reinitialize it.
in eval mode.
"Also adding features, this is optional"
FIXME I am rewriting restore because the restore method in parent class
does not restore layers which are not components. This restore method
can restore an full model.
may mess with loading pretrained weights
remove relu for the last layer
"reshapes node_representation to (num_nodes, num_layers * emb_dim)"
"for supervised tasks, add prediction head"
unsupervised tasks do not need a pred head
negative contexts are obtained by shifting the indicies of context embeddings
"sample x distinct nodes to be masked, based on mask rate. But"
will sample at least 1 node
create mask node label by copying node feature of mask node
modify the original node feature of the masked node
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by the"
the last feature token 119.
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L241
create mask edge labels by copying edge features of edges that are connected to
mask nodes
create mask edge labels by copying edge features of the edges connected to
the mask nodes
edge ordering is such that two directions of a single
"edge occur in pairs, so to get the unique undirected"
"edge indices, we take every 2nd edge index from list"
modify the original edge features of the edges connected to the mask nodes
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by the"
the last feature token 4.
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L268
"sample x distinct edges to be masked, based on mask rate. But"
will sample at least 1 edge
"during sampling, we only pick the 1st direction of a particular"
edge pair
create ground truth edge features for the edges that correspond to
the masked indices
"created new masked edge_attr, where both directions of the masked"
edges have masked edge type. For message passing in gcn
append the 2nd direction of the masked edges
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by 0s and"
an additional mask feature
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/bio/util.py#L101
"Take the entire graph, but can be modified to take a subgraph of k-hops from the root node"
Get node idx between root and the inner diameter l1
Get node idx between root and the outer diameter l2
takes a ring around the root node outside of l1 and inside of l2
"Get indices of overlapping nodes between substruct and context, WRT context ordering"
Decide first number of GAT layers
set2set doubles the dimensionality of the embedding
n_tasks is Optional[int] while argument 2 of nn.Linear has to be of type int
original implementation also includes an option if not using a separate encoder:
loss = sup_loss + local_unsup_loss * self.learning_rate
Below functions were taken from DeepChem TextCNN tensorflow implementation
Transform SMILES sequence to integers
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
We convert deepchem.feat.GraphData to a PyG graph and then
batch it.
The default_generator method returns an array of dc.feat.GraphData objects
"nested inside a list. To access the nested array of graphs, we are"
indexing by 0 here.
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
flake8:noqa
get DTNNEmbedding
get DTNNSteps
get DTNNGather
get Final Linear Layer
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
pretransformation
aggregation
post-transformation
The model predicts unnormalized probabilities for each class and task
"print (logits, proba)"
FIXME self.loss_func is an incorrect argument for TorchModel.loss because
it performs more than computing loss
FIXME This line is not needed as loss is computed inside the call to loss_func
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Ensure weights for both models are built.
Rename and delete older files.
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
True will be assigned up successful training attempt
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
import torch.nn as nn
import torch.nn.functional as F
Testing Shapes
Testing values
Dense1 is a list of dense layers
Testing Values
Testing Shapes with TF Model Output
Testing Shapes
Testing Values
testing first convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
testing rest of the convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Loading input tensors
Testing output
Testing MultiConvolution Layer
Testing First Convolution Layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Testing rest of the Multi convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Testing Aggregation Layer
Loading input tensors
Testing output
9: number of atoms
6: number of bonds
3: number of molecules
logits for class 1
logits for class 2
since pretraining is a self-supervision task where labels are generated during
"preparing batch, we mock _prepare_batch_for_pretraining to set all labels to 0."
The test here is checking whether the model predict 0's after overfitting.
preparing for test by setting 0 labels
arranging test - preparing dataset
acting - tests
asserting
arranging for tests
checking weights don't match before restore
norm layers and cached zero vectors have constant weights
restoring model
checking matching of weights after restore
asserting that weights are not same before reloading
"notm and bias layers have constant weights, hence they are not checked"
acting - loading pretrained weights
asserting that weight matches after loading
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"helper classes that depend on torch, they need to be in the try/catch block"
Define the dense layers
Define the dense layers
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Create dummy data
Call forward
Asserts
Create dummy data
Call forward
"Since the penalty is a squared norm of the gradients minus 1, multiplied by a constant,"
it should be non-negative
Create pretrained model
Create finetuning model
Load pretrained model
check weights match
all keys should match
keys should not match
"In a batched graph, atoms and bonds belonging to different graphs are differentiated"
"via scopes. In the below scenario, we assume a batched mol graph of three molecules"
"with 10 atoms, 20 bonds. On the 10 atoms, we consider the first 3 belonging to mol1,"
next 3 belonging to mol2 and remaining 4 belonging to mol4.
"Hence, the atom scope is [(0, 3), (3, 3), (6, 4)]. Similarly, for bonds, we have first 5 bonds belonging to mol1, next 4 to mol2 and remaining 11 to bond3."
"TODO Write tests for undirected = True case, currently fails. for this case, we have"
"to generate inputs (a2b, b2a, b2revb) for undirected graph."
The shapes should match the earlier shapes because message passing only updates node features.
The following variables are utility variables used during message passing to compute neighbors. Here we are asserting that MTBlock layer is not modifying these variables.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load dataset
Intiliaze torch TextCNN
Copy other layer weights
Run prediction
Pretraining in MLM mode
Pretraining in Multitask Regression Mode
test regression
test multitask regression
test classification
logit scores
check weights match
all keys values should match
new model's model attribute is an entirely new model initiated by AutoModel.load_from_pretrained
and hence it should have a different identifier
testing a simple scenario where each embedding corresponds to an unique graph
"here embeddings 0, 1 belong to a scope, 2, 3 to another scope and 4, 5 to another scope"
"thus, we sill have 3 graphs"
Porting the weights from TF to PyTorch
task 0 layer 0
task 0 layer 1
task 0 output layer
task 1 layer 0
task 1 layer 1
task 1 output layer
task 1 adapter 0
task 1 adapter 1
Generate dummy dataset
Generate dummy dataset
model is None when reloading a model
Some scikit-learn models don't use weights.
flake8: ignore
This will not work for ModularTorchModel as it is directly uses `loss_func` to compute loss.
flake8:noqa
flake8: noqa
"torch.nn.module prefix has no ending dot, while xt prefix has"
neural network xc functional of GGA (receives the density and grad as inputs)
"densinfo.value: (*BD, nr)"
"densinfo.grad : (*BD, nr, 3)"
"collect the total density (n), spin density (xi), and normalized gradients (s)"
normalize the gradient
decide how to transform the density to be the input of nn
get the neural network output
QR decomposition's solution is not unique in a way that every column
can be multiplied by -1 and it still a solution
"So, to remove the non-uniqueness, we will make the sign of the sum"
positive.
construct the rotation parameters
calculate the orthogonal orbital
"orb: (*, nao, norb)"
the orbital becomes the coefficients while params is all zeros (no rotation)
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Read the zip file
Get the labels from filenames
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
Update best validation score so far
"if `hyp_str` not in `all_scores`, store it in `all_scores`"
arbitrarily return last model trained
"If callable, sample it for a maximum n times"
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
"max_iter model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Select a device.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
Save the checkpoint to a file.
Rename and delete older files.
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
Annotate start/stop of sequence
Open index file
create an empty list to store lines in files.
iterate through each line in the input file
If the number of lines iterated through is equal or less than the shard size:
append to list
else yield the list
set the line_number variable to the last line number (num) before 'yield' was called
yield list (shard/batch)
Re-initialize list with the index line to begin a new shard.
Set attributes
Handle special featurizer cases
Set self.featurizer
Set self.return_quality_scores
Featurize sequences
"(X, y , w, ids)"
Featurize sequences
"(X, y , w, ids)"
Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
First line : header description
second line : sequence
third line : more description usually the same as the first line
fourth line: quality scores of the sequence
Second line : add sequence to the sequence array
Fourth line
Handle empty sequence
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
Sort image files
Sometimes zip files contain directories within. Traverse directories
Sort label image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
The line here assumes that y generated by shard_generator is a numpy array
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Try specifying particular columns
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create directory of multiple image files
Zip directory of multiple image files
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
These are the known dimensions of a_image.tif
These are the known dimensions of a_image.tif
"Since the different files have different shapes, makes an object array"
Test that the order of the contents of an unzipped file is preserved.
Load the zip file
Load multi_path directly
Check that the order of the files is the same
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"Expected shape is now (4, 192, 5)"
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
safe operations
occupation number gradients
get the length of the tensor output
other tensor ops
add the diagonal with a small eps to safeguard from nan
replace the diagonal with infinite (usually used for coulomb matrix)
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
data.shape and segment_ids.shape should be equal
"return V.set_(V.storage(), V.storage_offset(), V.size(), tuple(reversed(V.stride())))"
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
isotope-averaged atom masses in a.m.u.
from https://www.angelo.edu/faculty/kboudrea/periodic/structure_mass.htm
"JCP 41, 3199 (1964); DOI:10.1063/1.1725697"
taken from PySCF:
https://github.com/pyscf/pyscf/blob/45582e915e91890722fcae2bc30fb04867d5c95f/pyscf/data/radii.py#L23
I don't know why H has 0.35 while in the reference it is 0.
"They are in angstrom, so we need to convert it to Bohr"
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
NOTE We have lot of type ignores here since grover mol-graph which is of type
"GraphData have kwargs which are not attributes of GraphData. Hence, in these"
cases mypy raises GraphData does not have attributes `..`.
max with 1 to fix a crash in rare case of all single-heavy-atom mols
graph_index indicates which atom belongs to which molecule
padding
computing a2b
only needed if using atom messages
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
"For a node-prediction task, label is not added to edge features and other global features"
because label here is a node-level attribute and not a graph-level attribute
"In this case, the 'y' attribute of GraphData will contain the"
node-level labels.
not a self-loop
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
for each node (E[(X-E[X])^n])^{1/n}
EPS is added to the absolute value of expectation before taking the nth root for stability
"each scaler is a function that takes as input X (B x N x Din), adj (B x N x N) and"
avg_d (dictionary containing averages over training set) and returns X_scaled (B x N x Din) as output
Generate the raising operator matrix
Generate the lowering operator matrix
Generate the z-generator matrix
"Combine the matrices to form the x, z, and y generators"
Stack the generators along the first dimension to create a tensor
The transformation matrix generated is used to change the basis of a vector of
real spherical harmonics with representation index 1 to complex spherical harmonics.
"Construct the transformation matrix Q for m in range(-k, 0)"
Set the diagonal elements for m = 0
"Construct the transformation matrix Q for m in range(1, k + 1)"
Apply the factor of (-1j)**k to make the Clebsch-Gordan coefficients real
Handle dtype and device options
Ensure the tensor is contiguous and on the specified device
Get the SU(2) generators for the given quantum angular momentum (spin) value.
Get the transformation matrix to change the basis from real to complex spherical harmonics.
Convert the SU(2) generators to the SO(3) basis using the transformation matrix Q.
"X represents the SU(2) generators, and Q is the transformation matrix from real to complex spherical harmonics."
The resulting X matrix will be the SO(3) generators in the complex basis.
Return the real part of the SO(3) generators to ensure they are purely real.
"Ensure that alpha, beta, and gamma have the same shape for broadcasting."
"Ensure the angles are within the range [0, 2*pi) using modulo."
Get the SO(3) generators for the given quantum angular momentum (spin) value 'k'.
Calculate the Wigner D matrix using the matrix exponential of the generators
"and the rotation angles alpha, beta, and gamma in the appropriate order."
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
Warnings
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Calculation of Step Size and steps
Number of atoms per molecule is calculated by counting all the non zero elements(numbers) of every molecule.
"It loops over the molecules in the Coulomb matrix and takes the ""2.4"" root of the diagonal of ""2X"" of each molecule's representation."
Calculates the Gaussian Distance by passing distance by a gaussian function.
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
dtype=object allows for arrays(images here) of arbitrary size
Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
Structures are stored in .sdf file
"Note: Here, the order of columns is based on the order in which the values"
"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
"tasks above, they occur after `tasks` here."
"FIXME Ideally, we should use something like a dictionary here to keep it independent"
of column ordering.
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
"pattern to split the names, e.g. ""model.params[1]"" into [""model"", ""params"", ""[1]""]"
"return: (nao, nao)"
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
get the unique parameters of A
separate the parameters for A and for M
"grad_x: (*BABEM, nr, ncols)"
"x: (*BABEM, nr, ncols)"
solve (A-biases*M)^T v = grad_x
this is the grad of B
calculate the grad of matrices parameters
calculate the biases gradient
calculate the gradient to the biases matrices
Hidden
"NOTE: currently only works for batched B (1 batch dim), but unbatched A"
check the parameters
convert the numpy/scipy
NOTE: The line below is very inefficient for large na and ncols
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
correct the residual calculation
check for the stopping condition
move to the next index
"x: (ncols, *, nr, 1)"
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
correct the residual calculation regularly
calculate the residual
save the best results
check for the stopping conditions
"x: (ncols, *, nr, 1)"
"if B is all zeros, then return zeros"
setup the preconditioning and the matrix problem
get the stopping matrix
prepare the initial guess (it's just all zeros)
res = res * B_norm
save the best results
general helpers
get the linear operator (including the MXE part)
"A: (*BA, nr, nr) linop"
"B: (*BB, nr, ncols)"
"E: (*BE, ncols)"
"M: (*BM, nr, nr) linop"
"x: (ncols, *BX, nr, 1)"
"x: (ncols, *BX, nr, 1)"
estimate if it's posdef with power iteration
set posdef to False to make the operator becomes AT * A so it is
hermitian
TODO: the posdef check by largest eival only works for Hermitian/symmetric
"matrix, but it doesn't always work for non-symmetric matrix."
"In non-symmetric case, one need to do Cholesky LDL decomposition"
"if the largest eigenvalue is negative, then it's not posdef"
"otherwise, calculate the lowest eigenvalue to check if it's positive"
get the linear operation if it is not a posdef (A -> AT.A)
cg and bicgstab helpers
rootfinder-based
using rootfinder algorithm
set up the function for the rootfinding
"xi: (*BX, nr*ncols)"
setup the initial guess (the batch dimension must be the largest)
check if xnorm is converging
Broadcast Utilities
check the hermitian
check which methods are implemented
@abstractmethod
@abstractmethod # (optional)
@abstractmethod
@abstractmethod
implemented functions
use batched mv as mm
move the last dimension to the very first dimension to be broadcasted
apply batched mv and restore the initial shape
use batched mv as mm
move the last dimension to the very first dimension to be broadcasted
apply batched mv and restore the initial shape
special functions
properties
implementation
private functions
"xt: (*BY, p)"
"xdummy: (*BY, q)"
calculate y = Ax
calculate (dL/dx)^T = A^T (dL/dy)^T with (dL/dy)^T = xt
Helper Classes
"if it is a method from an object, unroll the parameters and add"
the object's parameters as well
get the unique ids
search the id if it has been added to the list
debugging
check the method input
assert if the method preserve the float tensors of the object
now assert if all_params0 == all_params1
get all tensor parameters in the object
get the parameter tensors used in the operation and the tensors specified by the developer
check if the userparams contains non-tensor
"check if there are missing parameters (present in operating params, but not in the user params)"
if oper_names[i] not in user_names:
"if there are missing parameters, give a warning (because the program"
"can still run correctly, e.g. missing parameters are parameters that"
are never set to require grad)
"check if there are excessive parameters (present in the user params, but not in the operating params)"
if user_names[i] not in oper_names:
"if there are excess parameters, give warnings"
get all the tensors recursively
copy the tensors and require them to be differentiable
run the method and see which one has the gradients
return the original tensor
traversing functions
None is set as default arg to avoid expanding list for multiple
invokes of _get_tensors without exception_ids argument
add exception to avoid infinite loop if there is a mutual dependant on objects
get the tensors recursively towards torch.nn.Module
traverse down the object to collect the tensors
traverse down the object to collect the tensors
flake8: noqa
TODO: implement robust LOBPCG and put it here
get the unique parameters of A & M
adapted from scipy.sparse.linalg.svds
clamp the eigenvalues to a small positive values to avoid numerical
instability
separate the sets of parameters
options for calculating the backward (not for `solve`)
save for the backward
"evals: (*BAM, neig)"
"evecs: (*BAM, na, neig)"
get the variables from ctx
set the default values of degen_*tol
check the degeneracy
"idx_degen: (*BAM, neig, neig)"
the loss function where the gradient will be retrieved
"warnings: if not all params have the connection to the output of A,"
it could cause an infinite loop because pytorch will keep looking
for the *params node and propagate further backward via the `evecs`
path. So make sure all the *params are all connected in the graph.
"if degenerate, check the conditions for finite derivative"
"if the requirements are not satisfied, raises a warning"
calculate the contributions from the eigenvalues
calculate the contributions from the eigenvectors
orthogonalize the grad_evecs with evecs
"Based on test cases, complex datatype is more likely to suffer from"
"singularity error when doing the inverse. Therefore, I add a small"
offset here to prevent that from happening
orthogonalize gevecs w.r.t. evecs
accummulate the gradient contributions
the contribution from the parallel elements
"evals: (*BAM, neig)"
get the index of degeneracies
contracted using opt_einsum
"evals, evecs = torch.linalg.eigh(Amatrix, eigenvectors=True)  # (*BA, q), (*BA, q, q)"
M decomposition to make A symmetric
it is done this way to make it numerically stable in avoiding
complex eigenvalues for (near-)degenerate case
calculate the eigenvalues and eigenvectors
(the eigvecs are normalized in M-space)
"evals, evecs = torch.linalg.eigh(A2, eigenvectors=True)  # (*BAM, q, q)"
temporary solution to https://github.com/pytorch/pytorch/issues/47599
remove the degenerate part
see https://arxiv.org/pdf/2011.04366.pdf
take the contribution from the eivec
calculate the contribution from the eival
symmetrize to reduce numerical instability
TODO: optimize for large linear operator and strict min_eps
Ideas:
(1) use better strategy to get the estimate on eigenvalues
(2) use restart strategy
get the shape of the transformation
set up the initial guess
Can be optimized by saving AV from the previous iteration and only
operate AV for the new V. This works because the old V has already
"been orthogonalized, so it will stay the same"
"AV = A.mm(V) # (*BAM,na,nguess)"
eigvals are sorted from the lowest
"eval: (*BAM, nguess), evec: (*BAM, nguess, nguess)"
calculate the eigenvectors of A
calculate the residual
print information and check convergence
apply the preconditioner
orthogonalize t with the rest of the V
orthogonalize V
check idxs
make the function a functional (depends on all parameters in the object)
params tensor is the LinearOperator's parameters
"if the object parameter is still the same, then use the pre-calculated values"
"otherwise, reevaluate by replacing the parameters with the new tensor params"
self.yfcn: (*nin)
file mostly from SciPy:
https://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/optimize/nonlin.py#L221
and converted to PyTorch for GPU efficiency
jacobian parameters
stopping criteria
algorithm parameters
misc parameters
"solving complex rootfinder by concatenating real and imaginary part,"
making the variable twice as long
represents x as a long real vector
pack a long real vector into the shape accepted by fcn
shorthand for the function
set up the jacobian
solver tolerance
save the best results
print out dx and df
adjust forcing parameters for inexact solve
jacobian parameters
stopping criteria
algorithm parameters
misc parameters
"No suitable step length found. Take the full Newton step,"
and hope for the best.
"Otherwise, compute the minimizer of a quadratic interpolant:"
"Otherwise, loop with cubic interpolation until we find an alpha which"
"satisfies the first Wolfe condition (since we are backtracking, we will"
assume that the value of alpha is not too small and satisfies the second
condition.
Failed to find a suitable step length
gd parameters
stopping conditions
misc parameters
update the step
check the stopping conditions
gd parameters
stopping conditions
misc parameters
update the step
check the stopping conditions
get the best values
usually user set maxiter == 0 just to wrap the minimizer backprop
taking most of the part from SciPy
setup the approximate inverse Jacobian
update Gm
keep the rank small
keep the rank small
raise RuntimeError
"u: (n, 1), s: (1,), vh: (1, n)"
"equilibrium can use all rootfinder methods, but there are several methods developed specifically for"
equilibrium (or fixed-point iterations). This dictionary gives the list of those special methods.
"minimization can use rootfinder algorithm, so check if it is actually"
"using the optimization algorithm, not the rootfinder algorithm"
the rootfinder algorithms are designed to move to the opposite direction
"of the output of the function, so the output of this function is just"
the grad of z w.r.t. y
"if it is going to optimization method, then also returns the value"
"if using the optimization algorithm, then the forward function is the one"
that returns f and grad
"if it is just using the rootfinder algorithm, then the forward function"
is the one that returns only the grad
set default options
split tensors and non-tensors params
merge the tensor and nontensor parameters
dL/df
get the grad for the params
anderson_acc parameters
stopping criteria
misc options
"x0: (..., *nfeats)"
"reshape x to have a shape of (batch_size, feats_dim)"
"x: (..., *nfeats)"
"xn: (..., feats_tot)"
"x: (..., feats_dim)"
"torch.bmm(g, g.transpose(-2, -1))"
"alpha: (batch_size, nsize)"
check the stopping condition
update the xn
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
"6 atoms: CC -> 2, CCC -> 3"
"7 bonds: CC -> 2, CCC -> 4 (bonds are considered as undirected"
and a single bond contributes to 2 bonds)
graph-level labels
node-level labels
graph.y contains node-labels and graph.node_features.shape[0]
holds number of nodes in that graph
Get Data
Checks that all atoms exits in array
Checks shape of gaussian distance
Checks all molecule membership exist
Check Distance Membership shape
Prepare Data
Run
Prepare Data
Inputs property
Without reverse input
With revercse input
Prepare Data
Inputs property
TODO test more formats for ligand
Test if the output has the correct shape.
Test for the case of zero momentum (j=0).
Test for the case of momentum j=1 (spin-1).
"Expected J_x, J_z, J_y matrices for j=1"
"Test for j = 0, which means we have a 1x1 transformation matrix"
"Test for j = 2, which means we have a 5x5 transformation matrix"
Test for device placement (CPU to CUDA)
Test for dtype conversion (complex128 to complex64)
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
dummy function which can be passed as the parameter f to simultaneous_move and single_move
test for gauss_initialize_position
testing symmetric simultaneous_move
testing asymmetric simultaneous_move
testing symmetric single_move
testing asymmetric single_move
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(3)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
get the location and weights of the integration in its original
coordinate
get the coordinate in Cartesian
integration element
Grid Transformations
"xnew is from [xmin, xmax]"
"r is approximately from [rmin, rmax]"
type of the atom Z
input types
"if the basis has been normalized before, then do nothing"
normalize to have individual gaussian integral to be 1 (if coeff is 1)
normalize the coefficients in the basis (because some basis such as
def2-svp-jkfit is not normalized to have 1 in overlap)
input basis type
QR decomposition's solution is not unique in a way that every column
can be multiplied by -1 and it still a solution
"So, to remove the non-uniqueness, we will make the sign of the sum"
positive.
construct the rotation parameters
calculate the orthogonal orbital
"orb: (*, nao, norb)"
the orbital becomes the coefficients while params is all zeros (no rotation)
properties
setups
fock matrix components
interface to dm
energy of the Hamiltonian
free parameters for variational method
Editable module
1 / cell.vol == det(b) / (2 pi)^3
drop ls that has norm > rcut * 1.05
System Properties
all-time calculations
(i.e. meaning it does not have to be executed to run the functions below)
"densinfo.value & lapl: (*BD, nr)"
"densinfo.grad: (*BD, ndim, nr)"
"return: (*BD, nr)"
"densinfo.value & lapl: (*BD, nr)"
"densinfo.grad: (*BD, ndim, nr)"
return:
"potentialinfo.value & lapl: (*BD, nr)"
"potentialinfo.grad: (*BD, ndim, nr)"
mark the densinfo components as requiring grads
"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
set the vars to require grad and returns the previous state of the vars
restore the state of requiring grad based on reqgrads list
all vars before this function requires grad
getting which parameters should require grad
set the params to require grad
special operations
properties
TODO: use regex!
convert the atomz to tensor
convert the atompos to tensor
"convert to dtype if atomzs is a floating point tensor, not an integer tensor"
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
Break the loop if max_records is set
Break the loop if max_records is set
Break the loop if max_records is set
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
biological sequence featurizers
tokenizers
support classes
dqc dependencies
get the density matrix from PySCF's CCSD calculation
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
condition if the original atom order is required
"mol must be a RDKit Mol object, so parse a SMILES"
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
"np.max() method works only for a non-empty array, so size of the array should be non-zero"
Adding shapes of kwargs
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
Create a mapping from the original node indices to the new node indices
Filter and reindex node features
Filter and reindex edge indices and edge features
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
number of nodes in each graph
cumulative number of nodes for each graph. This is necessary because the values in edge_index are node indices of all of the graphs in graph_list and so we need to offset the indices by the number of nodes in the previous graphs.
"columns are the edge index, values are the node index"
graph_index indicates which nodes belong to which graph
Batch user defined attributes
Convert edge_index to adjacency list
Breadth-first search
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Dimension of atom feature vector
len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
+ 2 at end for is_in_aromatic and mass
dictionary of available feature generators
for H2
"not all features are equally long, so used methane as dummy molecule to determine length"
Fix nans in features
add edge list considering a directed graph
get atom features
get edge(bond) features
get edge index
get global features
0 represents a masked bond
atoms
bonds
"Graph connectivity in COO format with shape [2, num_edges]"
"Edge feature matrix with shape [num_edges, num_edge_features]"
Always treat the bond as directed.
add mapping between bond b1 and atom a2 (destination atom)
add mapping between bond id and atom id (a1)
add mapping between bond id and atom a1 (source atom)
update index on bond and reverse bond mappings
generate SMILES for fragments
Featurize data using featurize() in parent class
Featurize str data
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
"row, col = edge_index"
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
get atom features
get edge index
user has not specified a descriptor list
creates normalized functions dictionary if normalized features are required
get cdf(feature) for that descriptor
get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
get required distribution_ from `scipy.stats` module.
cdf => cumulative density functions
make the cdf with the parameters.
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
similar to SNAP featurizer. both taken from Open Graph Benchmark (OGB) github.com/snap-stanford/ogb
"The difference between this and the SNAP features is the lack of masking tokens, possible_implicit_valence_list, possible_bond_dirs"
"and the prescence of possible_bond_stereo_list,  possible_is_conjugated_list, possible_is_in_ring_list,"
FIXME Add support for multiple conformers (wip)
"def __init__(self, num_conformers: int = 1, rmsd_cutoff: float = 2):"
""""""""
Initialize the RDKitConformerFeaturizer with the given parameters.
Parameters
----------
"num_conformers : int, optional, default=1"
The number of conformers to generate for each molecule.
"rmsd_cutoff : float, optional, default=2"
The root-mean-square deviation (RMSD) cutoff value. Conformers with an RMSD
greater than this value will be discarded.
""""""""
self.num_conformers = num_conformers
self.rmsd_cutoff = rmsd_cutoff
Derived from https://github.com/HannesStark/3DInfomax/blob/5cd32629c690e119bcae8726acedefdb0aa037fc/datasets/qm9_dataset_rdkit_conformers.py#L377
add hydrogen bonds to molecule because they are not in the smiles representation
FIXME Add support for multiple conformers (wip)
"AllChem.EmbedMultipleConfs(mol, self.num_conformers)"
AllChem.MMFFOptimizeMolecule(mol)
rmsd_list = []
"rdMolAlign.AlignMolConformers(mol, RMSlist=rmsd_list)"
# insert 0 RMSD for first conformer
"rmsd_list.insert(0, 0)"
conformers = [
mol.GetConformer(i)
for i in range(self.num_conformers)
if rmsd_list[i] < self.rmsd_cutoff
]
"# if conformer list is less than num_conformers, pad by repeating conformers"
conf_idx = 0
while len(conformers) < self.num_conformers:
conformers.append(conformers[conf_idx])
conf_idx += 1
coordinates = [conf.GetPositions() for conf in conformers]
add edges in both directions
"Graph connectivity in COO format with shape [2, num_edges]"
FIXME Add support for multiple conformers (wip)
graph_list = []
for i in range(self.num_conformers):
graph_list.append(
"GraphData(node_pos_features=np.array(coordinates[i]),"
"node_features=np.array(atom_features_list),"
"edge_features=np.array(edge_features_list),"
edge_index=np.array(edges_list).T))
return graph_list
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
"sort first by frequency, then alphabetically"
"sort first by frequency, then alphabetically"
sorting the atoms neighbors
concatenating the sorted neighbors
"sort first by frequency, then alphabetically"
"sort first by frequency, then alphabetically"
flake8: noqa
superclass accepts a DeepChem dataset while huggingface vocabulary builders
reads data from file
test with max size
test build from csv
test with max size
load the vocabulary by passing filename
test tokenization of a single point
load the vocabulary by passing the filename
test tokenization of a single point
Build vocabulary by wrapping in huggingface vocabulary builder
Load vocabulary and do a basic sanity check on the vocabulary
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
number of indices where feature count is more than 1
no normalized feature value should be greater than 1.0
"151 = 133 + 18 (133 -> one hot encoding from _ATOM_FEATURES, 18 other features)"
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
NOTE: The test depends on the the pretrained vocabulary
(seyonec/PubChem10M_SMILES_BPE_60k). If the pretrained vocabulary is modified
"(which can be since it is an external resource), the test might fail."
construct edge (bond) index
add edge list considering a directed graph
test for 'MolGraphConvFeaturizer' class
"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
"Requirements - transformers, tokenizers"
no normalized feature value should be greater than 1.0
these are the properties used in grover
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
Test featurizer with atom 3-D coordinates as kwargs
load sample dataset
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"number of indices, where feature count is more than 1, should be 0"
number of indices where feature count is more than 1
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
both reactant and product are null
reactant is null
product is null
valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
z is kwargs
check convert function
z is kwargs
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Set up tests.
Set up testing parameters.
the atom order for 'C' is same in case of canonical and original ordering
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
"bond and its reverse bond, therefore num_bonds * 2 edges"
9 atom features
"Graph connectivity in COO format with shape [2, num_edges]"
3 bond features
3 xyz coordinates for each atom in the conformer
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
Calculate frequency
flake8: noqa
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Return identity if no tuple to split to
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Invalid because valence for atom 5 N is greater than permitted (4)
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
NOTE The below comment is from original source code
dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss
return av_loss + fg_loss + dist_coff * dist_loss
"return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss"
We just return overall_loss since TorchModel can handle only a single loss
loss for nodes
converting the binary classification to multiclass classification
positive context prediction is the dot product of substructure representation and true context representation
negative context prediction is the dot product of substructure representation and negative (random) context representation.
positive substructure prediction is the dot product of expanded substructure representation and true overlapped node representation.
shift indices of substructures to create negative examples
negative substructure prediction is the dot product of shifted expanded substructure representation and true overlapped node representation.
Compute the loss for positive and negative context representations
The final loss is the sum of positive and negative context losses
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Dependency Models
scikit-learn model
PyTorch models
Pytorch models with torch-geometric dependency
TODO We should clean up DMPNN and remove torch_geometric dependency during import
Pytorch-lightning modules import
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
"Create an identical model, do a single step of fitting with restore=True and make sure it got restored correctly."
check that the first layer is still the same between the two models
check that the predictions are different because of the fine tuning
check that the first layer is different between the two models
Conv2d and Linear layers test(CNN classification)
Fit trained model
Eval model on train
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
testing batch size > 1
testing true values
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
https://github.com/diffqc/dqc/blob/742eb2576418464609f942def4fb7c3bbdc0cd82/dqc/test/test_xc.py#L15
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
.8 to save resources for a difficult task
.2 to save resources for a difficult task
first iteration loss is around 50
The first pass of the transformation should be 0
Test sampling method
Test log_prob method (this method is used when inverse pass)
Output must be a Nth zero array since nothing is being learned yet
Featurize to assert for tests
Assert errors for sample method
Assert errors for log_prob method
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec]"
atom features
Try with compression
"Outputs should be [mol1_vec, mol2_vec]"
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
fit the model
reload the model
index of pair features
number of pairs for each atom
atom features
pair features
Assigning tensorflow equivalent weights to torch layer
"Outputs should be [A, P]"
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = Metric(
"roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
xgboost test
fit trained model
"If ES rounds are more than total epochs, it will never trigger"
Find the number of boosting rounds in the model
"If rounds boosted are less than total estimators, it means ES was triggered"
prepare dataset
lightgbm test
fit trained model
"If ES rounds are more than total epochs, it will never trigger"
Find the number of boosting rounds in the model
"If rounds ran are less than estimators, it means ES was triggered"
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Testing graphnet for a single graph
Testing for consistency
Testing with a batch of Graphs
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
TODO The test is skipped as FakeGraphGenerator has to be updated
to generate regression labels
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
"Linear layers for making query, key, value"
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Set up tests.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Finetune model weights should not match before loading from pretrained model
Finetune model weights should match after loading from pretrained model
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
Test for the init function of FerminetModel class
Testing ionic initialization
Testing whether error throws up when spin is wrong
Testing the spin values
Testing ionic initialization
Test for the evaluate_hf_solution function of FerminetModel class
"The solution should be of the shape (number of electrons, number of electrons)"
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Test for the init function of FerminetModel class
Testing ionic initialization
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
The highest recorded score should match get_best_score().
Reload the save model and confirm that it matches the best logged score.
Make sure get_best_score() still works when save_dir is not specified
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
"n_atoms = 4  # In CCC and C, there are 4 atoms"
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
TODO(rbharath): What is the shape of outputs supposed to be?
"I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
"By setting the `box_size` to effectively zero, the result should only contain `nan`."
Check that layer has three trainable parameters.
Check when `box_size` is of wrong dimensionality.
Check when `inputs` is of wrong length.
Create random local node representations and global graph representations
Compute similarity scores using the discriminator
Check if the output has the correct shape and dtype
total_n_atoms = 4
n_atom_feat = 4
"atom_feat = np.random.rand(total_n_atoms, n_atom_feat)"
Embeddings and results from Tensorflow implementation
Weights and Embeddings from Tensorflow implementation
init parameters
generate features for testing
index of pair features
atom features
pair features
tensors for torch layer
assigning tensorflow layer weights to torch layer
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Dataset of SMILES strings for testing SeqToSeq models.
Train the model on random sequences. We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Initialize the model
Fit the Model
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
Note: Following are some changes
compared to the TensorFlow unit test:
1. Changed nb_epoch to 300.
2. Increased the learning_rate to 0.0003.
Note: This needs to be inspected in future to understand low score as compared to a score of 0.9 in tensorflow unit test.
Note: Following are some changes
compared to the TensorFlow unit test:
1. Changed nb_epoch to 400.
2. Reduced the number of data points to 2.
3. Increased batch_size to 20.
4. Increased the learning_rate to 0.0003.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Note: This needs to be inspected in future to understand low score.
Load mini log-solubility dataset.
Fit trained model
Note: Compared to the tensorflow unit test increased the nb_epoch to 600.
Eval model on train
Note: This needs to be inspected in future to understand low score as compared to a score of 0.8 in tensorflow unit test.
Create input tensor with values within full_atom_feature_dims
Create input tensor with values within full_bond_feature_dims
Compute the global graph representation
Compute positive and negative scores
Check if the loss is a scalar and has the correct dtype
Check if the loss is a scalar and non-negative
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
Get Data
Check Shape
Check number of parameters
Eval model on train
load sample dataset
initialize the model
fit the model
reload the model
atom features
Gaussian histograms expands into 11 Gaussian buckets.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
edges logits used during training
nodes logits used during training
edges logits
nodes logits
For training
For sample generation
Define the dense layers
"Ignoring type. For TorchModel, loss is a required argument but HuggingFace computes"
"loss during the forward iteration, removing the need for a loss function."
FIXME Transformers library has an api like AutoModel.from_pretrained. It allows to
initialise and create a model instance directly without requiring a class instance initialisation step.
"To use `load_from_pretrained` in DeepChem, we need to follow a two step process"
of initialising class instance and then loading weights via `load_from_pretrained`.
y is None during predict
Main training loop.
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
copy the input graph to avoid in-place operations
"FIXME For pretraining task, both model2d and model3d but the super class"
"can't handle two models for contrastive learning, hence we pass only model2d"
torch's one-hot encoding works with integer data types.
"We convert labels to integer, one-hot encode and convert it back to float"
for making it suitable to loss function
graphs = [[
graph_data.to_dgl_graph().to(self.device) for graph_data in row
] for row in inputs]
convert the GraphData objects to DGL graphs
"TODO Ideally, we should use a lr schedule but we need to update lr_scheduler.step() method"
in ModularTorchModel.fit_generator to accept a metric.
"self._lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(self._pytorch_optimizer,"
mode='min')
Initialize buffers
Accumulate statistics for Fisher matrices
Initialize buffers
p_grad_mat is of output_dim * input_dim
inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
we always put gradient w.r.t weight in [0]
and w.r.t bias in [1]
do kl clip
TODO Explain in detail what the four outcompes are
The bond and rev bond have even and odd ids respectively.
FIXME This layer is similar to DMPNNEncoderLayer and they
must be unified.
Shared weight matrix across depths (default)
Except reverse bond its-self(w) ! \sum_{k\in N(u) \ w}
"FIXME When input_layer is none, for the first iteration of message passing, we should ideally"
be using different weight matrix since message will be of shape num_bonds x f_bonds_dim
"in the first iteration and in the subsequent iterations, it will be num_bonds x hidden_size"
FIXME We assume that we are using a hidden layer to transform the initial atom message
and bond messages to hidden dimension size.
self.atom_messages is False
Note: Elementwise affine has to be consistent with the pre-training phase
multi-headed attention
support no residual connection in MTBlock.
atom input to atom output
bond to atom
atom input to bond output
bond input to bond output
Inputs
Noise Input
Data Input
Data Inputs
Conditional Input
Conditional Inputs
Generators
Discriminators
Forward pass through generators
Forward pass through discriminators
Compute loss functions
Compute the weighted errors
Create learnable weights for the generators and discriminators.
Compute the weighted errors
Add an entropy term to the loss.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
PyTorch layers require input and output channels as parameter
"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
initializing layer bias with nn.init gives mypy typecheck error
using the following workaround
residual blocks can only be used when successive layers have the same output shape
Used for converting edges back to their original shape
Compute mean edge features for each node by dst_index (each node
"receives information from edges which have that node as its destination,"
hence the computation uses dst_index to aggregate information)
holding bi-directional edges in case of undirected graphs
coonverting edge features to its original shape
Input
Shared weight matrix across depths (default):
For messages hidden states
For atom hidden states
num_atoms x hidden_size
num_molecules x hidden_size
concat global features
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
No other forget biases supported right now.
Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
Define the layers
Create the final layers
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set `pad_batches=True`
"z = torch.mm(h, self.U) + self.b"
create a boolean mask for each partition
partition the input tensor using the masks
Case when >2 inputs are passed
Loop over the remaining convolution layers
Apply the current layer to the outputs from the previous layer
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
Construct internal trainable weights
Weight matrix and bias matrix required to compute new atom layer from the previous atom layer
Weight matrix and bias matrix required to compute new atom layer from the previous pair layer
Weight matrix and bias matrix required to compute new pair layer from the previous atom layer
Weight matrix and bias matrix required to compute new pair layer from the previous pair layer
flake8: noqa
Converting the input to torch tensors
"AA is a tensor with shape[total_num_atoms,n_hidden_AA]"
"PA is a tensor with shape[total number of pairs,n_hidden_PA]"
Split the PA tensor according to the 'pair_split' tensor
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"PP is a tensor with shape [total number of pairs,n_hidden_PP]"
Integrate the Cross Layer Mapping inside the Global Message Passing
Message Passing operation
Update function f_u
Message Passing operation
Teacher forcing: Feed the target as the next input
Without teacher forcing: use its own predictions as the next input
Initializing the first layer (first layer has different dims than others)
filling the weights with xavier uniform method for the linear weights and random assignment for the bias
Calculating one-electron feature's average
temporary lists containing each electron's embeddings which will be torch.stack on the end
Calculating two-electron feature's average
"initialized weights with torch.zeros, torch.eye and using xavier init."
temporary list to stack upon electrons axis at the end
Integrate the Cross Layer Mapping inside the Local Message Passing
Message Passing 1
Message Passing 2
Aggregation
Update function f_u
Output Module
import torch.nn as nn
creating one and two electron features
setting the fermient layer and fermient envelope layer batch size to be that of the current batch size of the model. This enables for vectorized calculations of hessians and jacobians.
using functorch to calcualte hessian and jacobian in one go
using index tensors to index out the hessian elemennts corresponding to the same variable (cross-variable derivatives are ignored)
"doing all the calculation and detaching from graph to save memory, which allows larger batch size"
cloning self.input which will serve as the new input for the vectorized functions.
lambda function for calculating the log of absolute value of the wave function.
using jacrev for the jacobian and jacrev twice for to calculate the hessian. The functorch's hessian function if directly used does not give stable results.
making the batch size temporarily as 1 for the vectorization of hessian and jacobian.
Initialization for ionic molecules
hook function below is an efficient way modifying the gradients on the go rather than looping
using non-local variables as a means of parameter passing
the move function calculates the energy of sampled electrons and samples new set of electrons (does not calculate loss)
clipping local energies which are away 5 times the variance from the median
using the sampled electrons from the electron sampler for bacward pass and modifying gradients
going through each step of random walk and calculating the modified gradients with local energies
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
mypy check is ignored for global_features as it is not a default attribute
of GraphData. It is created during runtime using **kwargs.
mapping from bond index to the index of the atom (where the bond is coming from)
"mapping from bond index to concat(in_atom, bond) features"
mapping from atom index to list of indicies of incoming bonds
mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
zero padded at the end
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
padded with -1 at the end
mapping from bond index to the index of the atom (where the bond if going to)
mapping from atom index to list of indicies of incoming bonds
get maximum number of incoming bonds
Make number of incoming bonds equal to maximum number of bonds.
This is done by appending -1 to fill remaining space at each atom indices.
mapping from bond index to the index of the reverse bond
get encoder
get input size for ffn
get output size for ffn
get ffn
Steps to get `molecules_unbatch_key`:
1. Get the tensor containing the indices of first atoms of each molecule
2. Get the tensor containing number of atoms of each molecule
by taking the difference between consecutive indices.
3. Convert the tensor to a list.
num_molecules x (enc_hidden + global_features_size)
ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
"atom feature matrix with shape [number of atoms, number of features]"
concatenated feature vector which contains concatenation of initial atom and bond features
mapping from atom index to list of indicies of incoming bonds
mapping that maps bond index to 'array of indices of the bonds'
incoming at the initial atom of the bond (excluding the reverse bonds)
array of global molecular features
maximum number of incoming bonds in the batch
generate concatenated feature vector and mappings
pad all mappings to maximum number of incoming bonds in the batch
the hidden size here is the output size of last layer in mol_atom_from_atom_ffn and mol_atom_from_bond_ffn components.
it is necessary that aforementioned components produces output tensor of same size.
"In training mode, we return atom level aggregated output and bond level aggregated output."
The training has an additional objective which ensures that atom and bond level aggregated outputs
are similar to each other apart from the objective of making the aggregated output closer to each
other.
"FIXME In the above step, we initialize modular torch model but"
something is missing here. The attribute loss from TorchModel gets assigned `loss_func`
by super class initialization in ModularTorchModel but here we reinitialize it.
in eval mode.
"Also adding features, this is optional"
FIXME I am rewriting restore because the restore method in parent class
does not restore layers which are not components. This restore method
can restore an full model.
may mess with loading pretrained weights
remove relu for the last layer
"reshapes node_representation to (num_nodes, num_layers * emb_dim)"
"for supervised tasks, add prediction head"
unsupervised tasks do not need a pred head
negative contexts are obtained by shifting the indicies of context embeddings
"sample x distinct nodes to be masked, based on mask rate. But"
will sample at least 1 node
create mask node label by copying node feature of mask node
modify the original node feature of the masked node
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by the"
the last feature token 119.
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L241
create mask edge labels by copying edge features of edges that are connected to
mask nodes
create mask edge labels by copying edge features of the edges connected to
the mask nodes
edge ordering is such that two directions of a single
"edge occur in pairs, so to get the unique undirected"
"edge indices, we take every 2nd edge index from list"
modify the original edge features of the edges connected to the mask nodes
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by the"
the last feature token 4.
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L268
"sample x distinct edges to be masked, based on mask rate. But"
will sample at least 1 edge
"during sampling, we only pick the 1st direction of a particular"
edge pair
create ground truth edge features for the edges that correspond to
the masked indices
"created new masked edge_attr, where both directions of the masked"
edges have masked edge type. For message passing in gcn
append the 2nd direction of the masked edges
zeros are meant to represent the masked features. This is distinct from the
"original implementation, where the masked features are represented by 0s and"
an additional mask feature
link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/bio/util.py#L101
"Take the entire graph, but can be modified to take a subgraph of k-hops from the root node"
Get node idx between root and the inner diameter l1
Get node idx between root and the outer diameter l2
takes a ring around the root node outside of l1 and inside of l2
"Get indices of overlapping nodes between substruct and context, WRT context ordering"
Decide first number of GAT layers
set2set doubles the dimensionality of the embedding
n_tasks is Optional[int] while argument 2 of nn.Linear has to be of type int
original implementation also includes an option if not using a separate encoder:
loss = sup_loss + local_unsup_loss * self.learning_rate
Below functions were taken from DeepChem TextCNN tensorflow implementation
Transform SMILES sequence to integers
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
We convert deepchem.feat.GraphData to a PyG graph and then
batch it.
The default_generator method returns an array of dc.feat.GraphData objects
"nested inside a list. To access the nested array of graphs, we are"
indexing by 0 here.
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
flake8:noqa
get DTNNEmbedding
get DTNNSteps
get DTNNGather
get Final Linear Layer
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
pretransformation
aggregation
post-transformation
The model predicts unnormalized probabilities for each class and task
"print (logits, proba)"
FIXME self.loss_func is an incorrect argument for TorchModel.loss because
it performs more than computing loss
FIXME This line is not needed as loss is computed inside the call to loss_func
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Ensure weights for both models are built.
Rename and delete older files.
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
True will be assigned up successful training attempt
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
import torch.nn as nn
import torch.nn.functional as F
Testing Shapes
Testing values
Dense1 is a list of dense layers
Testing Values
Testing Shapes with TF Model Output
Testing Shapes
Testing Values
testing first convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
testing rest of the convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Loading input tensors
Testing output
Testing MultiConvolution Layer
Testing First Convolution Layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Testing rest of the Multi convolution layer
dense1 layer - list of dense layers
dense2 layer - single dense layer
Testing Aggregation Layer
Loading input tensors
Testing output
9: number of atoms
6: number of bonds
3: number of molecules
logits for class 1
logits for class 2
since pretraining is a self-supervision task where labels are generated during
"preparing batch, we mock _prepare_batch_for_pretraining to set all labels to 0."
The test here is checking whether the model predict 0's after overfitting.
preparing for test by setting 0 labels
arranging test - preparing dataset
acting - tests
asserting
arranging for tests
checking weights don't match before restore
norm layers and cached zero vectors have constant weights
restoring model
checking matching of weights after restore
asserting that weights are not same before reloading
"notm and bias layers have constant weights, hence they are not checked"
acting - loading pretrained weights
asserting that weight matches after loading
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"helper classes that depend on torch, they need to be in the try/catch block"
Define the dense layers
Define the dense layers
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Create dummy data
Call forward
Asserts
Create dummy data
Call forward
"Since the penalty is a squared norm of the gradients minus 1, multiplied by a constant,"
it should be non-negative
Create pretrained model
Create finetuning model
Load pretrained model
check weights match
all keys should match
keys should not match
"In a batched graph, atoms and bonds belonging to different graphs are differentiated"
"via scopes. In the below scenario, we assume a batched mol graph of three molecules"
"with 10 atoms, 20 bonds. On the 10 atoms, we consider the first 3 belonging to mol1,"
next 3 belonging to mol2 and remaining 4 belonging to mol4.
"Hence, the atom scope is [(0, 3), (3, 3), (6, 4)]. Similarly, for bonds, we have first 5 bonds belonging to mol1, next 4 to mol2 and remaining 11 to bond3."
"TODO Write tests for undirected = True case, currently fails. for this case, we have"
"to generate inputs (a2b, b2a, b2revb) for undirected graph."
The shapes should match the earlier shapes because message passing only updates node features.
The following variables are utility variables used during message passing to compute neighbors. Here we are asserting that MTBlock layer is not modifying these variables.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load dataset
Intiliaze torch TextCNN
Copy other layer weights
Run prediction
Pretraining in MLM mode
Pretraining in Multitask Regression Mode
test regression
test multitask regression
test classification
logit scores
check weights match
all keys values should match
new model's model attribute is an entirely new model initiated by AutoModel.load_from_pretrained
and hence it should have a different identifier
testing a simple scenario where each embedding corresponds to an unique graph
"here embeddings 0, 1 belong to a scope, 2, 3 to another scope and 4, 5 to another scope"
"thus, we sill have 3 graphs"
Porting the weights from TF to PyTorch
task 0 layer 0
task 0 layer 1
task 0 output layer
task 1 layer 0
task 1 layer 1
task 1 output layer
task 1 adapter 0
task 1 adapter 1
Generate dummy dataset
Generate dummy dataset
model is None when reloading a model
Some scikit-learn models don't use weights.
flake8: ignore
This will not work for ModularTorchModel as it is directly uses `loss_func` to compute loss.
flake8:noqa
flake8: noqa
"torch.nn.module prefix has no ending dot, while xt prefix has"
neural network xc functional of GGA (receives the density and grad as inputs)
"densinfo.value: (*BD, nr)"
"densinfo.grad : (*BD, nr, 3)"
"collect the total density (n), spin density (xi), and normalized gradients (s)"
normalize the gradient
decide how to transform the density to be the input of nn
get the neural network output
QR decomposition's solution is not unique in a way that every column
can be multiplied by -1 and it still a solution
"So, to remove the non-uniqueness, we will make the sign of the sum"
positive.
construct the rotation parameters
calculate the orthogonal orbital
"orb: (*, nao, norb)"
the orbital becomes the coefficients while params is all zeros (no rotation)
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Read the zip file
Get the labels from filenames
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
Update best validation score so far
"if `hyp_str` not in `all_scores`, store it in `all_scores`"
arbitrarily return last model trained
"If callable, sample it for a maximum n times"
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
"max_iter model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
Annotate start/stop of sequence
Open index file
create an empty list to store lines in files.
iterate through each line in the input file
If the number of lines iterated through is equal or less than the shard size:
append to list
else yield the list
set the line_number variable to the last line number (num) before 'yield' was called
yield list (shard/batch)
Re-initialize list with the index line to begin a new shard.
Set attributes
Handle special featurizer cases
Set self.featurizer
Set self.return_quality_scores
Featurize sequences
"(X, y , w, ids)"
Featurize sequences
"(X, y , w, ids)"
Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
First line : header description
second line : sequence
third line : more description usually the same as the first line
fourth line: quality scores of the sequence
Second line : add sequence to the sequence array
Fourth line
Handle empty sequence
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
The line here assumes that y generated by shard_generator is a numpy array
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Try specifying particular columns
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"Expected shape is now (4, 192, 5)"
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
"For a node-prediction task, label is not added to edge features and other global features"
because label here is a node-level attribute and not a graph-level attribute
"In this case, the 'y' attribute of GraphData will contain the"
node-level labels.
not a self-loop
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
Structures are stored in .sdf file
"Note: Here, the order of columns is based on the order in which the values"
"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
"tasks above, they occur after `tasks` here."
"FIXME Ideally, we should use something like a dictionary here to keep it independent"
of column ordering.
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
graph-level labels
node-level labels
graph.y contains node-labels and graph.node_features.shape[0]
holds number of nodes in that graph
TODO test more formats for ligand
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
dummy function which can be passed as the parameter f to simultaneous_move and single_move
test for gauss_initialize_position
testing symmetric simultaneous_move
testing asymmetric simultaneous_move
testing symmetric single_move
testing asymmetric single_move
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(3)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
tokenizers
support classes
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
condition if the original atom order is required
"mol must be a RDKit Mol object, so parse a SMILES"
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
"np.max() method works only for a non-empty array, so size of the array should be non-zero"
Adding shapes of kwargs
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Dimension of atom feature vector
len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
+ 2 at end for is_in_aromatic and mass
dictionary of available feature generators
number of atoms
number of bonds
"mapping from bond index to concat(in_atom, bond) features | initial input is a zero padding"
mapping from atom index to list of indices of incoming bonds
mapping from bond index to the index of the atom the bond is coming from
mapping from bond index to the index of the reverse bond
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
get bond features
for H2
"not all features are equally long, so used methane as dummy molecule to determine length"
Fix nans in features
add edge list considering a directed graph
get atom features
get edge(bond) features
get edge index
get global features
generate SMILES for fragments
Featurize data using featurize() in parent class
Featurize str data
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
initialize
check initialization
creates normalized functions dictionary if normalized features are required
get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
get required distribution_ from `scipy.stats` module.
cdf => cumulative density functions
make the cdf with the parameters.
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
number of indices where feature count is more than 1
no normalized feature value should be greater than 1.0
get atom features
mapping from atom index to atom features | initial input is a zero padding
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
construct edge (bond) index
add edge list considering a directed graph
test for 'MolGraphConvFeaturizer' class
"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
"Requirements - transformers, tokenizers"
no normalized feature value should be greater than 1.0
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
Test featurizer with atom 3-D coordinates as kwargs
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"number of indices, where feature count is more than 1, should be 0"
number of indices where feature count is more than 1
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
both reactant and product are null
reactant is null
product is null
valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
@pytest.mark.linux_only
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
z is kwargs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Set up tests.
Set up testing parameters.
the atom order for 'C' is same in case of canonical and original ordering
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
Calculate frequency
flake8: noqa
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Return identity if no tuple to split to
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Dependency Models
scikit-learn model
PyTorch models
Pytorch models with torch-geometric dependency
TODO We should clean up DMPNN and remove torch_geometric dependency during import
Pytorch-lightning modules import
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Conv2d and Linear layers test(CNN classification)
Fit trained model
Eval model on train
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
The first pass of the transformation should be 0
Test sampling method
Test log_prob method (this method is used when inverse pass)
Output must be a Nth zero array since nothing is being learned yet
Featurize to assert for tests
Assert errors for sample method
Assert errors for log_prob method
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
fit the model
reload the model
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Testing graphnet for a single graph
Testing for consistency
Testing with a batch of Graphs
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
TODO The test is skipped as FakeGraphGenerator has to be updated
to generate regression labels
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Set up tests.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
test for the prepare_input_stream function of Ferminet class
ionic charge initialization test
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
The highest recorded score should match get_best_score().
Reload the save model and confirm that it matches the best logged score.
Make sure get_best_score() still works when save_dir is not specified
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
"By setting the `box_size` to effectively zero, the result should only contain `nan`."
Check that layer has three trainable parameters.
Check when `box_size` is of wrong dimensionality.
Check when `inputs` is of wrong length.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Initialize buffers
Accumulate statistics for Fisher matrices
Initialize buffers
p_grad_mat is of output_dim * input_dim
inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
we always put gradient w.r.t weight in [0]
and w.r.t bias in [1]
do kl clip
PyTorch layers require input and output channels as parameter
"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
initializing layer bias with nn.init gives mypy typecheck error
using the following workaround
residual blocks can only be used when successive layers have the same output shape
Used for converting edges back to their original shape
Compute mean edge features for each node by dst_index (each node
"receives information from edges which have that node as its destination,"
hence the computation uses dst_index to aggregate information)
holding bi-directional edges in case of undirected graphs
coonverting edge features to its original shape
Input
Shared weight matrix across depths (default):
For messages hidden states
For atom hidden states
num_atoms x hidden_size
num_molecules x hidden_size
concat global features
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
No other forget biases supported right now.
Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
import torch.nn as nn
from deepchem.models.torch_models import TorchModel
import deepchem.models.optimizers as optim
TODO look for the loss function(Hamiltonian)
dummy function which can be passed as the parameter f. f gives the log probability
TODO replace this function with forward pass of the model in future
"super(Ferminet, self).__init__()"
Initialization for ionic molecules
concatenating distance and vectors arrays
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
mypy check is ignored for global_features as it is not a default attribute
of GraphData. It is created during runtime using **kwargs.
mapping from bond index to the index of the atom (where the bond is coming from)
"mapping from bond index to concat(in_atom, bond) features"
mapping from atom index to list of indicies of incoming bonds
mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
zero padded at the end
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
padded with -1 at the end
mapping from bond index to the index of the atom (where the bond if going to)
mapping from atom index to list of indicies of incoming bonds
get maximum number of incoming bonds
Make number of incoming bonds equal to maximum number of bonds.
This is done by appending -1 to fill remaining space at each atom indices.
mapping from bond index to the index of the reverse bond
get encoder
get input size for ffn
get output size for ffn
get ffn
Steps to get `molecules_unbatch_key`:
1. Get the tensor containing the indices of first atoms of each molecule
2. Get the tensor containing number of atoms of each molecule
by taking the difference between consecutive indices.
3. Convert the tensor to a list.
num_molecules x (enc_hidden + global_features_size)
ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
"atom feature matrix with shape [number of atoms, number of features]"
concatenated feature vector which contains concatenation of initial atom and bond features
mapping from atom index to list of indicies of incoming bonds
mapping that maps bond index to 'array of indices of the bonds'
incoming at the initial atom of the bond (excluding the reverse bonds)
array of global molecular features
maximum number of incoming bonds in the batch
generate concatenated feature vector and mappings
pad all mappings to maximum number of incoming bonds in the batch
Decide first number of GAT layers
We convert deepchem.feat.GraphData to a PyG graph and then
batch it.
The default_generator method returns an array of dc.feat.GraphData objects
"nested inside a list. To access the nested array of graphs, we are"
indexing by 0 here.
flake8:noqa
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
model is None when reloading a model
Some scikit-learn models don't use weights.
flake8: ignore
flake8:noqa
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
Update best validation score so far
"if `hyp_str` not in `all_scores`, store it in `all_scores`"
arbitrarily return last model trained
"If callable, sample it for a maximum n times"
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
"max_iter model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
Annotate start/stop of sequence
Open index file
create an empty list to store lines in files.
iterate through each line in the input file
If the number of lines iterated through is equal or less than the shard size:
append to list
else yield the list
set the line_number variable to the last line number (num) before 'yield' was called
yield list (shard/batch)
Re-initialize list with the index line to begin a new shard.
Set attributes
Handle special featurizer cases
Set self.featurizer
Set self.return_quality_scores
Featurize sequences
"(X, y , w, ids)"
Featurize sequences
"(X, y , w, ids)"
Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
First line : header description
second line : sequence
third line : more description usually the same as the first line
fourth line: quality scores of the sequence
Second line : add sequence to the sequence array
Fourth line
Handle empty sequence
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
The line here assumes that y generated by shard_generator is a numpy array
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Try specifying particular columns
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"Expected shape is now (4, 192, 5)"
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
"For a node-prediction task, label is not added to edge features and other global features"
because label here is a node-level attribute and not a graph-level attribute
"In this case, the 'y' attribute of GraphData will contain the"
node-level labels.
not a self-loop
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
Structures are stored in .sdf file
"Note: Here, the order of columns is based on the order in which the values"
"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
"tasks above, they occur after `tasks` here."
"FIXME Ideally, we should use something like a dictionary here to keep it independent"
of column ordering.
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
graph-level labels
node-level labels
graph.y contains node-labels and graph.node_features.shape[0]
holds number of nodes in that graph
TODO test more formats for ligand
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
dummy function which can be passed as the parameter f to simultaneous_move and single_move
test for gauss_initialize_position
testing symmetric simultaneous_move
testing asymmetric simultaneous_move
testing symmetric single_move
testing asymmetric single_move
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(3)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
tokenizers
support classes
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
condition if the original atom order is required
"mol must be a RDKit Mol object, so parse a SMILES"
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
"np.max() method works only for a non-empty array, so size of the array should be non-zero"
Adding shapes of kwargs
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Dimension of atom feature vector
len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
+ 2 at end for is_in_aromatic and mass
dictionary of available feature generators
number of atoms
number of bonds
"mapping from bond index to concat(in_atom, bond) features | initial input is a zero padding"
mapping from atom index to list of indices of incoming bonds
mapping from bond index to the index of the atom the bond is coming from
mapping from bond index to the index of the reverse bond
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
get bond features
for H2
"not all features are equally long, so used methane as dummy molecule to determine length"
Fix nans in features
add edge list considering a directed graph
get atom features
get edge(bond) features
get edge index
get global features
generate SMILES for fragments
Featurize data using featurize() in parent class
Featurize str data
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
load_sdf_files returns pos as strings but user can also specify
numpy arrays for atom coordinates
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
initialize
check initialization
creates normalized functions dictionary if normalized features are required
get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
get required distribution_ from `scipy.stats` module.
cdf => cumulative density functions
make the cdf with the parameters.
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
number of indices where feature count is more than 1
no normalized feature value should be greater than 1.0
get atom features
mapping from atom index to atom features | initial input is a zero padding
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
construct edge (bond) index
add edge list considering a directed graph
test for 'MolGraphConvFeaturizer' class
"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
"Requirements - transformers, tokenizers"
no normalized feature value should be greater than 1.0
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
Test featurizer with atom 3-D coordinates as kwargs
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"number of indices, where feature count is more than 1, should be 0"
number of indices where feature count is more than 1
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
both reactant and product are null
reactant is null
product is null
valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
@pytest.mark.linux_only
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
z is kwargs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Set up tests.
Set up testing parameters.
the atom order for 'C' is same in case of canonical and original ordering
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
Calculate frequency
flake8: noqa
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Return identity if no tuple to split to
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Dependency Models
scikit-learn model
PyTorch models
Pytorch models with torch-geometric dependency
TODO We should clean up DMPNN and remove torch_geometric dependency during import
Pytorch-lightning modules import
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Conv2d and Linear layers test(CNN classification)
Fit trained model
Eval model on train
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
The first pass of the transformation should be 0
Test sampling method
Test log_prob method (this method is used when inverse pass)
Output must be a Nth zero array since nothing is being learned yet
Featurize to assert for tests
Assert errors for sample method
Assert errors for log_prob method
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
get data
prepare batch (size 1)
initialize the model
get output
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
overfit test
load sample dataset
initialize the model
fit the model
reload the model
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Testing graphnet for a single graph
Testing for consistency
Testing with a batch of Graphs
"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
the above import raises a ModuleNotFoundError. It is safe to ignore it
since the below tests only run in an environment with pytorch installed.
TODO The test is skipped as FakeGraphGenerator has to be updated
to generate regression labels
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Set up tests.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
test for the prepare_input_stream function of Ferminet class
ionic charge initialization test
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
The highest recorded score should match get_best_score().
Reload the save model and confirm that it matches the best logged score.
Make sure get_best_score() still works when save_dir is not specified
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
"By setting the `box_size` to effectively zero, the result should only contain `nan`."
Check that layer has three trainable parameters.
Check when `box_size` is of wrong dimensionality.
Check when `inputs` is of wrong length.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Initialize buffers
Accumulate statistics for Fisher matrices
Initialize buffers
p_grad_mat is of output_dim * input_dim
inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
we always put gradient w.r.t weight in [0]
and w.r.t bias in [1]
do kl clip
PyTorch layers require input and output channels as parameter
"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
initializing layer bias with nn.init gives mypy typecheck error
using the following workaround
residual blocks can only be used when successive layers have the same output shape
Used for converting edges back to their original shape
Compute mean edge features for each node by dst_index (each node
"receives information from edges which have that node as its destination,"
hence the computation uses dst_index to aggregate information)
holding bi-directional edges in case of undirected graphs
coonverting edge features to its original shape
Input
Shared weight matrix across depths (default):
For messages hidden states
For atom hidden states
num_atoms x hidden_size
num_molecules x hidden_size
concat global features
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
No other forget biases supported right now.
Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
import torch.nn as nn
from deepchem.models.torch_models import TorchModel
import deepchem.models.optimizers as optim
TODO look for the loss function(Hamiltonian)
dummy function which can be passed as the parameter f. f gives the log probability
TODO replace this function with forward pass of the model in future
"super(Ferminet, self).__init__()"
Initialization for ionic molecules
concatenating distance and vectors arrays
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
mypy check is ignored for global_features as it is not a default attribute
of GraphData. It is created during runtime using **kwargs.
mapping from bond index to the index of the atom (where the bond is coming from)
"mapping from bond index to concat(in_atom, bond) features"
mapping from atom index to list of indicies of incoming bonds
mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
zero padded at the end
get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
padded with -1 at the end
mapping from bond index to the index of the atom (where the bond if going to)
mapping from atom index to list of indicies of incoming bonds
get maximum number of incoming bonds
Make number of incoming bonds equal to maximum number of bonds.
This is done by appending -1 to fill remaining space at each atom indices.
mapping from bond index to the index of the reverse bond
get encoder
get input size for ffn
get output size for ffn
get ffn
Steps to get `molecules_unbatch_key`:
1. Get the tensor containing the indices of first atoms of each molecule
2. Get the tensor containing number of atoms of each molecule
by taking the difference between consecutive indices.
3. Convert the tensor to a list.
num_molecules x (enc_hidden + global_features_size)
ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
"atom feature matrix with shape [number of atoms, number of features]"
concatenated feature vector which contains concatenation of initial atom and bond features
mapping from atom index to list of indicies of incoming bonds
mapping that maps bond index to 'array of indices of the bonds'
incoming at the initial atom of the bond (excluding the reverse bonds)
array of global molecular features
maximum number of incoming bonds in the batch
generate concatenated feature vector and mappings
pad all mappings to maximum number of incoming bonds in the batch
Decide first number of GAT layers
We convert deepchem.feat.GraphData to a PyG graph and then
batch it.
The default_generator method returns an array of dc.feat.GraphData objects
"nested inside a list. To access the nested array of graphs, we are"
indexing by 0 here.
flake8:noqa
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
model is None when reloading a model
Some scikit-learn models don't use weights.
flake8: ignore
flake8:noqa
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
Update best validation score so far
"if `hyp_str` not in `all_scores`, store it in `all_scores`"
arbitrarily return last model trained
"If callable, sample it for a maximum n times"
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
"max_iter model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
TODO log attempts to add empty sequences every shard
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
TODO test more formats for ligand
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
take into account special tokens in max length
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
tokenizers
support classes
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
generate SMILES for fragments
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
initialize
check initialization
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
"Requirements - transformers, tokenizers"
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
@pytest.mark.linux_only
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Add the input features.
Add the convolutional layers
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Depedency Models
scikit-learn model
PyTorch models
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
Reload the save model and confirm that it matches the best logged score.
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
Decide first number of GAT layers
flake8:noqa
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
Some scikit-learn models don't use weights.
flake8: ignore
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
Environment-specific dependencies.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
TODO The below line should be fixes
See: https://github.com/deepchem/deepchem/issues/2373
model.save()
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of output files
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Process legacy toggle
Set attributes
Handle special featurizer cases
Set self.featurizer
"(X, y, w, ids)"
TODO don't convert all sequences into np array (allow shards)
Check if line is a header
Handle empty sequence
TODO log attempts to add empty sequences every shard
Annotate start/stop of sequence
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
determine the shard sizes of the datasets to merge
"otherwise the entire dataset is the ""shard size"""
we must reshard the dataset to have a uniform size
choose the smallest shard size
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Test merging of numpy datasets
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
TODO: test with full uniprot file once sharding support is added.
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
class ANITransformer(Transformer):
"""""""Performs transform from 3D coordinates to ANI symmetry functions"
Note
----
This class requires TensorFlow to be installed.
""""""""
"def __init__(self,"
"max_atoms=23,"
"radial_cutoff=4.6,"
"angular_cutoff=3.1,"
"radial_length=32,"
"angular_length=8,"
"atom_cases=[1, 6, 7, 8, 16],"
"atomic_number_differentiated=True,"
coordinates_in_bohr=True):
""""""""
Only X can be transformed
""""""""
import tensorflow as tf
self.max_atoms = max_atoms
self.radial_cutoff = radial_cutoff
self.angular_cutoff = angular_cutoff
self.radial_length = radial_length
self.angular_length = angular_length
self.atom_cases = atom_cases
self.atomic_number_differentiated = atomic_number_differentiated
self.coordinates_in_bohr = coordinates_in_bohr
self.compute_graph = self.build()
self.sess = tf.Session(graph=self.compute_graph)
self.transform_batch_size = 32
"super(ANITransformer, self).__init__(transform_X=True)"
"def transform_array(self, X, y, w):"
if self.transform_X:
X_out = []
num_transformed = 0
start = 0
batch_size = self.transform_batch_size
while True:
"end = min((start + 1) * batch_size, X.shape[0])"
X_batch = X[(start * batch_size):end]
output = self.sess.run(
"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
X_out.append(output)
num_transformed = num_transformed + X_batch.shape[0]
logger.info('%i samples transformed' % num_transformed)
start += 1
if end >= len(X):
break
"X_new = np.concatenate(X_out, axis=0)"
assert X_new.shape[0] == X.shape[0]
"return (X_new, y, w)"
"def untransform(self, z):"
raise NotImplementedError(
"""Cannot untransform datasets with ANITransformer."")"
def build(self):
""""""" tensorflow computation graph for transform """""""
import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
self.inputs = tf.keras.Input(
"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
flags = tf.sign(atom_numbers)
flags = tf.cast(
"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
"coordinates = self.inputs[:, :, 1:]"
if self.coordinates_in_bohr:
coordinates = coordinates * 0.52917721092
"d = self.distance_matrix(coordinates, flags)"
"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
coordinates)
self.outputs = tf.concat(
[
"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
angular_sym
"],"
axis=2)
return graph
"def distance_matrix(self, coordinates, flags):"
""""""" Generate distance matrix """""""
import tensorflow as tf
max_atoms = self.max_atoms
"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
# Calculate pairwise distance
"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
# Masking for valid atom index
d = d * flags
return d
"def distance_cutoff(self, d, cutoff, flags):"
""""""" Generate distance matrix with trainable cutoff """""""
import tensorflow as tf
# Cutoff with threshold Rc
d_flag = flags * tf.sign(cutoff - d)
d_flag = tf.nn.relu(d_flag)
d_flag = d_flag * tf.expand_dims(
"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
return d * d_flag
"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
""""""" Radial Symmetry Function """""""
import tensorflow as tf
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
length = ita.get_shape().as_list()[-1]
"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
"d = tf.stack([d] * length, axis=3)"
out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
if self.atomic_number_differentiated:
out_tensors = []
for atom_type in self.atom_cases:
selected_atoms = tf.expand_dims(
"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
axis=3)
"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out, axis=2)"
"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
""""""" Angular Symmetry Function """""""
import tensorflow as tf
max_atoms = self.max_atoms
embedding = tf.eye(np.max(self.atom_cases) + 1)
"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
ita = 3 / (Rs[1] - Rs[0])**2
"thetas = np.linspace(0., np.pi, self.angular_length)"
zeta = float(self.angular_length**2)
"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
length = zeta.get_shape().as_list()[-1]
"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
"[coordinates] * max_atoms, 2)"
"R_ij = tf.stack([d] * max_atoms, axis=3)"
"R_ik = tf.stack([d] * max_atoms, axis=2)"
"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
"R_ij = tf.stack([R_ij] * length, axis=4)"
"R_ik = tf.stack([R_ik] * length, axis=4)"
"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
"theta = tf.stack([theta] * length, axis=4)"
"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
if self.atomic_number_differentiated:
out_tensors = []
"for id_j, atom_type_j in enumerate(self.atom_cases):"
for atom_type_k in self.atom_cases[id_j:]:
"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
selected_atoms = tf.expand_dims(
"tf.expand_dims(selected_atoms, axis=1), axis=4)"
out_tensors.append(
"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
"return tf.concat(out_tensors, axis=2)"
else:
"return tf.reduce_sum(out_tensor, axis=(2, 3))"
def get_num_feats(self):
n_feat = self.outputs.get_shape().as_list()[-1]
return n_feat
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a layer: either a function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
TODO test more formats for ligand
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
take into account special tokens in max length
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
tokenizers
support classes
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Turn atomic coordinates into image positions
Set the bond line coordinates to the bond property used.
Set the atom positions in image to different atomic properties in channels
With fixed res and img_size some molecules (e.g. long chains) may not fit.
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
generate SMILES for fragments
Extend shorter strings with padding
Padding before and after
Featurize data using featurize() in parent class
Featurize str data
Featurize mol data
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
The 1.0 float value represents True Boolean
This will return a boolean vector with all entries False
To get the shortest paths between two nodes.
To get info if two nodes belong to the same ring.
Featurizer
initialize
check initialization
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
decode the source in the mixed and separated cases
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
"Requirements - transformers, tokenizers"
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
@pytest.mark.linux_only
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untransform
untranform
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Add the input features.
Add the convolutional layers
edges logits used during training
nodes logits used during training
edges logits
nodes logits
training of the model
generating compounds
nodes logits used during compound generation
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B flag support (DEPRECATED)
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Define the PyTorch Module that implements the model.
Define the PyTorch Module that implements the model.
Run fit transformers on dummy dataset to determine n_features after transformation
set wandb init arguments
Dataset ids are used to differentiate datasets seen by the logger
log data
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Log data to Wandb
flake8: noqa
Tensorflow Depedency Models
scikit-learn model
PyTorch models
Jax models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
but turns out to be slightly faster
JAX depend
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
""""""""
"Predict the model's outputs, along with the uncertainty in each one."
The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
It involves repeating the prediction many times with different dropout masks.
The prediction is computed as the average over all the predictions.  The
uncertainty includes both the variation among the predicted values (epistemic
uncertainty) and the model's own estimates for how well it fits the data
(aleatoric uncertainty).  Not all models support uncertainty prediction.
Parameters
----------
dataset: dc.data.Dataset
Dataset to make prediction on
masks: int
the number of dropout masks to average over
Returns
-------
"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
"value of the output, and each element of y_std estimates the standard"
deviation of the corresponding element of y_pred
""""""""
sum_pred: List[np.ndarray] = []
sum_sq_pred: List[np.ndarray] = []
sum_var: List[np.ndarray] = []
for i in range(masks):
generator = self.default_generator(
"dataset, mode='uncertainty', pad_batches=False)"
"results = self._predict(generator, [], True, None)"
if len(sum_pred) == 0:
"for p, v in results:"
sum_pred.append(p)
sum_sq_pred.append(p * p)
sum_var.append(v)
else:
"for j, (p, v) in enumerate(results):"
sum_pred[j] += p
sum_sq_pred[j] += p * p
sum_var[j] += v
output = []
std = []
for i in range(len(sum_pred)):
p = sum_pred[i] / masks
output.append(p)
std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
if len(output) == 1:
"return (output[0], std[0])"
else:
"return list(zip(output, std))"
JAX dependencies
Main training loop
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Apply tranformers and record results.
Concatenate arrays to create the final results.
flake8:noqa
The PINNModel requires you to create two functions
`create_eval`_fn for letting the model know how to compute the model in inference and
`gradient_fn` for letting model know how to compute the gradient and different regulariser
equation loss depending on the differential equation
defining the Haiku model
"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
which will be used as the differential loss(regulariser)
The expected solution must be as close to cos(x)
Initialize the weights with random values
Forward function which takes the params
Loss Function
JaxModel Working
sample network
Model Initialization
Loss Function
JaxModel Working
sample network
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Model Initilisation
Loss Function
JaxModel Working
Each epoch is a single step for this model
@pytest.mark.jax
@pytest.mark.slow
def test_uncertainty():
"""""""Test estimating uncertainty a TorchModel."""""""
n_samples = 30
n_features = 1
noise = 0.1
"X = np.random.rand(n_samples, n_features)"
"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
"dataset = dc.data.NumpyDataset(X, y)"
class Net(hk.Module):
"def __init__(self, output_size: int = 1):"
super().__init__()
"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
self.output = hk.Linear(output_size)
self.log_var = hk.Linear(output_size)
"def __call__(self, x):"
x = self._network1(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
x = self._network2(x)
"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
output = self.output(x)
log_var = self.log_var(x)
var = jnp.exp(log_var)
"return output, var, output, log_var"
def f(x):
net = Net(1)
return net(x)
"def loss(outputs, labels, weights):"
diff = labels[0] - outputs[0]
log_var = outputs[1]
var = jnp.exp(log_var)
return jnp.mean(diff * diff / var + log_var)
class UncertaintyModel(JaxModel):
"def default_generator(self,"
"dataset,"
"epochs=1,"
"mode='fit',"
"deterministic=True,"
pad_batches=True):
for epoch in range(epochs):
"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
"batch_size=self.batch_size,"
"deterministic=deterministic,"
pad_batches=pad_batches):
"yield ([X_b], [y_b], [w_b])"
jm_model = hk.transform(f)
rng = jax.random.PRNGKey(500)
"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
modified_inputs = jnp.array(
[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
"params = jm_model.init(rng, modified_inputs)"
model = UncertaintyModel(
"jm_model.apply,"
"params,"
"loss,"
"output_types=['prediction', 'variance', 'loss', 'loss'],"
learning_rate=0.003)
"model.fit(dataset, nb_epochs=2500)"
"pred, std = model.predict_uncertainty(dataset)"
assert np.mean(np.abs(y - pred)) < 2.0
assert noise < np.mean(std) < 1.0
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
test if adjacency matrix input is correctly set
test if nodes features matrix input is correctly set
check discriminator shape
check training edges logits shape
check training nodes logits shapes
True will be assigned up successful training attempt
force clear tensor flow backend
create new model
to avoid flake8 E125/yapf incompatibility
generate input
train model
generate sample
check how many valid molecules were created and add to list
finally test if there was at least one valid training session
as the model structure improves this should become more and more strict
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
Check predictions match on random sample
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
load datasets
initialize model
overfit test
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Most weights should be close to zero.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
Reload the save model and confirm that it matches the best logged score.
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Load dataset and Models
call model.fit again to test multiple fit() calls
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
Decide first number of GAT layers
flake8:noqa
Select a device.
W&B logging
"If `wandb=True` and no logger is provided, initialize default logger"
Setup and initialize W&B logging
Update config with KerasModel params
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
Some scikit-learn models don't use weights.
flake8: ignore
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
"TODO For this dataset and model, the R2-scores are less than 0.3."
This has to be improved.
See: https://github.com/deepchem/deepchem/issues/2776
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
TIMING
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
TIMING
TIMING
Set some global variables up top
Featurize KAGGLE dataset
TIMING
TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
TIMING
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
TIMING
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
We have already evaluated the model for these hyperparameters.
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
flake8: noqa
"2 model variants, 1 results.txt file"
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
check filetypes
Define locations of log and output files
Write GNINA conf file
Run GNINA
read output and log
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare ligand
Write Vina conf file
Define locations of log and output files
"I'm not sure why specifying the args as a list fails on other platforms,"
but for some reason it only works if I pass it as a string.
FIXME: Incompatible types in assignment
FIXME: We should use `subprocess.run` instead of `call`
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
"(X, y, w, ids)"
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
These functions have moved to deepchem.utils_docking_utils
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a Keras layer: either a TensorFlow function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
"A NumPy array, or an object that can be converted to one.  Once we move to"
"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
TODO test more formats for ligand
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
"|-|\+|\\|\/|:|~|@|\?|>>?|\*|\$|\%[0-9]{2}|[0-9])"""""""
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
take into account special tokens in max length
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
create temporary valid ids serving to filter out failed featurizations from every sublist
"of features (i.e. every molecules' frags list), and also totally failed sublists."
This makes output digestable by Loaders
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
TODO (VIGS25): Complete the description
Handle loading failures which return None
Fit atomic conv model
Add the Atomic Convolution layers to fetches
Extract the atomic convolution features
"SMILES is unique, so set a canonical order of atoms"
Add hydrogens and generate a conformation.
Record properties of the molecules.
Create the output object.
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
support classes
for str
for list
validation
skip list
skip path string
main logic
Find a successful featurization
Replace failed featurizations with appropriate array
Special case handling of single molecule
Convert iterables to list
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Set the bond line coordinates to the bond property used.
Turn atomic coordinates into image positions
Set the atom positions in image to different atomic properties in channels
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
generate SMILES for fragments
Extend shorter strings with padding
Padding before and after
validation
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
initialize
check initialization
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
bond labels
atom labels
create bond encoders and decoders
create atom encoders and decoders
Special case handling of single molecule
Convert iterables to list
Set up site environment matcher
Graphical option
tolerance for grouping nodes
determine minimum distance between sitetypes.
This is used to determine the existence of an edge
Sort by bond
You want to maximize this in order to make sure every node gets an edge
construct graph
matcher options
construct graph
Add nodes
Add edge. distance is edge attribute
construct graph
Gets the isomorphic mapping. Also the most time consuming part of the code
reconstruct graph after alinging point order
RMSD
Construct one hot encoding
get mapping between all site index to active site index
Get Neighbors
Read Data
get map between two environment
align input to the primitive cell (reference)
apply permutations
remove spectators
map it to active sites
Extract the right number of sites by distance
if PBC condition is fulfilled..
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
pad outputs
Deprecation warnings for old atomic conv featurizer name #
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
"Requirements - transformers, tokenizers"
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
check for separate count and SMILES entries for each fragment
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"Coords are padded, neighbor list and Z are not"
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
@pytest.mark.linux_only
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignored if sanitize=False
test flattened voxel features
test voxel features
test flat features
test rotations
not support array style inputs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
test featurization
test defeaturization
sanity check; see if something weird does not happen with rdkit
check if original smiles match defeaturized smiles
sanity check; see if something weird does not happen with rdkit
test featurization
test defeaturization
check if original smiles match defeaturized smiles
untranform
untranform
untranform
Check the SDF file.
Check the PDB file.
Check the SMILES string.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Add the input features.
Add the convolutional layers
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
means that this is second loop of convolution
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B logging
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Add the input features.
Add the dense layers
Add the input features.
Add the dense layers
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
"layer_sizes=[32, 32, 16],"
Add the dense layers
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
flake8: noqa
scikit-learn model
PyTorch models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Predict the output and uncertainty.
predict datset with no y (ensured by tasks = [])
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Eval model on train
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
check predict shape
check overfit
needs change
check predict shape
check overfit
reload
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rate=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
Reload the save model and confirm that it matches the best logged score.
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
Decide first number of GAT layers
flake8:noqa
Select a device.
W&B logging
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
Some scikit-learn models don't use weights.
flake8: ignore
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
#### TIMING ######
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
#### TIMING ###########
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
######### TIMING ################
Most reaction dataset ML tasks train the prediction of products from
"ractants. Both of these are contained in the rxn object that is output,"
"so there is no ""tasks"" field."
Download USPTO dataset
Unzip
Unzipped file is a tap seperated values file (despite the .txt)
The first element in the row is the reaction smarts
"Sometimes smarts have extraneous information at end of form """
"|f:0"" that causes parsing to fail. Not sure what this information"
"is, but just ignoring for now."
Make up dummy labels since DiskDataset.from_numpy doesn't allow
creation from just features for now.
TODO: This dataset isn't saved to disk so reload doesn't happen.
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
def test_qm9_loader():
current_dir = os.path.dirname(os.path.abspath(__file__))
"tasks, datasets, transformers = load_qm9("
"reload=False,"
"data_dir=current_dir,"
"featurizer='ECFP',"
splitter_kwargs={
"'seed': 42,"
"'frac_train': 0.6,"
"'frac_valid': 0.2,"
'frac_test': 0.2
})
""
assert len(tasks) == 12
assert tasks[0] == 'mu'
"assert datasets[0].X.shape == (8, 1024)"
def test_zinc15_loader():
current_dir = os.path.dirname(os.path.abspath(__file__))
""
"tasks, datasets, transformers = load_zinc15("
"reload=False,"
"data_dir=current_dir,"
splitter_kwargs={
"'seed': 42,"
"'frac_train': 0.6,"
"'frac_valid': 0.2,"
'frac_test': 0.2
})
""
test_vec = np.array([
"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,"
"0.0, -1.224744871391589, 0.0, 0.0, 0.0, 0.0, 2.0, -0.5, 0.0, 0.0, 0.0,"
"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0"
])
""
"train, val, test = datasets"
"assert tasks == ['mwt', 'logp', 'reactive']"
"assert train.X.shape == (3, 100, 35)"
"assert np.allclose(train.X[0][0], test_vec, atol=0.01)"
""
"if os.path.exists(os.path.join(current_dir, 'zinc15_250K_2D.csv')):"
"os.remove(os.path.join(current_dir, 'zinc15_250K_2D.csv'))"
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
Add it on to the information needed for the constructor
Not all models have nb_epoch
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
Demarcating internal function for readability
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
"mypy test throws error, so ignoring it in try"
Not all models have nb_epoch
Some models autosave
arbitrarily return last model
flake8: noqa
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Define nb_epoch in hyperparam_search function call
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
top actions).
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
Build a nightly package by default.
get the version from deepchem/__init__.py
nightly version : .devYearMonthDayHourMinute
Force to add `.dev` if `--release` option isn't passed when building
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Load Delaney dataset
Get Metric
Fit trained model
Fit trained model
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
This example shows how to use Pandas to load data directly
without using a CSVLoader object. This may be useful if you
want the flexibility of processing your data with Pandas
directly.
Now let's convert from a dataset back to a pandas dataframe
"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Get Metric
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Load delaney dataset
Fit models
Load delaney dataset
Fit models
Fit models
Load delaney dataset
Fit models
TODO: Once improved splitting API is merged in swap to simpler API
The return values are dc.data.Dataset objects so we need to extract
the ids
TODO once improved splitting API is merged in swap out for simpler
API
The return values are dc.data.Dataset objects so we need to extract
the ids
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
TODO: This should be swapped for simpler splitter API once that's merged in.
The return values are dc.data.Dataset objects so we need to extract
the ids
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Configuration file for the Sphinx documentation builder.
""
This file only contains a selection of the most common options. For a full
list see the documentation:
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
Options for autodoc directives
How to represents typehints.
"Add any paths that contain templates here, relative to this directory."
The suffix of source filenames.
The master toctree document.
autosectionlabel setting
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
The name of an image file (relative to this directory) to place at the top
of the sidebar.
Customize the sphinx theme
-- Source code links ---------------------------------------------------
Resolve function for the linkcode extension.
"try to find the file and line number, based on code from numpy:"
https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
"If you push the tag, please remove `.dev`"
Record inputs.
Create the output directory if necessary.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
flake8: noqa
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
We know use_pose_generator_scores == False in this case
check whether self.featurizer is instance of ComplexFeaturizer or not
TODO: How to handle the failure here?
TODO(rbharath): The autodock vina source computes surface distances
which take into account the van der Waals radius of each atom type.
"Shape (N, M)"
"Shape (N, M)"
Parse complex
Prepare protein
Get protein centroid and range
TODO(rbharath: Does vina divide box dimensions by 2?
Prepare protein
Write Vina conf file
Define locations of log and output files
"I'm not sure why specifying the args as a list fails on other platforms,"
but for some reason it only works if I pass it as a string.
FIXME: Incompatible types in assignment
FIXME: We should use `subprocess.run` instead of `call`
flake8: noqa
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
We provide no scoring model so the docker won't score
Check only one output since num_modes==1
Let's turn on logging since this test will run for a while
Check returned files exist
Let's turn on logging since this test will run for a while
Check returned files exist
"Where d is greater than zero, the repulsion is just zeros"
"When d is 0, this should just be 1"
"When d == 0, the hbond interaction is 0"
The exponential returns 1 when input 0.
This exponential returns 1 when input 3
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Let's turn on logging since this test will run for a while
Note this may download autodock Vina...
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
Test that every atom in pocket maps exists
scalar case
per-example case
This is a little arcane but it repeats w across tasks.
"If w.shape == (n_samples, 1) handle it as 1D"
"w.shape == (n_samples, n_tasks)"
scalar case
Handle n_classes/n_task shape ambiguity
Add in task dimension
Insert a task dimension (we know n_tasks=1 from above0
"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
Handle classification. We need to convert labels into one-hot representation.
check whether n_classes is int or not
Handle n_classes/n_task shape ambiguity
Add in task dimension
Make everything 2D so easy to handle
Handle each task separately.
Handle continuous class probabilites of positive class for binary
Fill in class 0 probabilities
Add a task dimension to concatenate on
Handle binary labels
"make y_hot of shape (N, n_classes)"
Add a task dimension to concatenate on
Insert a task dimension
"Now of shape (N,)"
"Now of shape (N, 1)"
"Returns shape (N, n_tasks)"
"Now of shape (N,)"
"Now of shape (N, n_classes)"
"Now of shape (N, 1, n_classes)"
"Returns shape (N, n_tasks, n_classes)"
These are some smart defaults
These are some smart defaults corresponding to sklearn's required
behavior
Attempt some limited shape imputation to find n_tasks
check whether n_tasks is int or not
This is because `normalize_weight_shape` require int value.
FIXME: Incompatible types in assignment
DEPRECATED. WILL BE REMOVED IN NEXT DEEPCHEM VERSION
DEPRECATED. WILL BE REMOVED IN NEXT DEEPCHEM VERSION
Attempt to convert both into the same type
if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, num_tasks)"
check whether wild_type_predictions is np.ndarray or not
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
check whether wild_type_predictions is np.ndarray or not
kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
validation
flake8: noqa
metric class
metrics utils
sklearn & scipy score function
original score function
Get a random prediction matrix
"Of shape (N, n_classes)"
"Of shape (N, 1, n_classes)"
This has w for each task.
Best score case
Worst score case
best case
duplicate prediction value
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
Call in-silico mutagenesis
Construct and train SequenceDNN model
Call in-silico mutagenesis
Check nonzero elements exist
Special case handling of single input
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
Featurize task results if they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it"
makes no sense to have y values or weights.
The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
The field in which load_sdf_files return value stores smiles
"(X, y, w, ids)"
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Sort image files
"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Ensure that every worker will pick the same random order for each epoch.
Ensure that every worker will pick the same random order for each epoch.
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset.
Find the X values.
Find the y values.
Find the w values.
Find the ids.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
Create temp directory to store resharded version
Get correct shapes for y/w
Write data in new shards
Handle shapes
Note that this means that DiskDataset resharding currently doesn't
work for datasets that aren't regression/classification.
Handle spillover from last shard
Should have updated to non-legacy metadata
Note that this resets the cache internally
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
mp.dummy aliases ThreadPool to Pool
(ytz): this skips everything except possibly the last shard
"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
make a NumpyDataset under the hood
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
Reset cache
See if we have a cached copy of this shard.
"We don't, so load it from disk."
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Try to cache this shard for later use.  Since the normal usage pattern is
"a series of passes through the whole dataset, there's no point doing"
anything fancy.  It never makes sense to evict another shard from the
"cache to make room for this one, because we'll probably want that other"
shard again before the next time we want this one.  So just cache as many
as we can and then stop.
"When outputting a NumpyDataset, we have 1 in-memory shard"
Handle edge case with empty indices
We use two loops here. The outer while loop walks over selection shards
(the chunks of the indices to select that should go into separate
"output shards), while the inner for loop walks over the shards in the"
source datasets to select out the shard indices from that  source shard
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle empty case where no data from this shard needed
Handle the case of datasets with y/w missing
Break if all indices have been used up already
Note these will be in the sorted order
We need to recover the original ordering. We can do this by using
np.where to find the locatios of the original indices in the sorted
indices.
We know there's only one match for np.where since this is a
"permutation, so the [0][0] pulls out the exact match location."
If shape metadata is available use it to directly compute shape from
metadata
"In absense of shape metadata, fall back to loading data from disk to"
find shape.
Case n_samples should be 1
flake8: noqa
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Generate data
legacy_dataset_reshard is a shared dataset in the legacy format kept
around for testing resharding.
Set cache to 0 size to avoid cache hiding errors
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
This is necessary since from_numpy adds in shape information
Generate data
Generate data
Generate data
Should now have 10 shards
This is the shape of legacy_data
legacy_dataset is a dataset in the legacy format kept around for testing
purposes.
This is the shape of legacy_data_reshard
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing
Should now have 10 shards
legacy_dataset is a dataset in the legacy format kept around for testing purposes.
Test constructor reload works for legacy format
legacy_dataset_reshard is a sharded dataset in the legacy format kept
around for testing resharding.
Reshard copy
Check metadata has been updated
First try using images for X.
Now try using images for y.
Transform it
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Test iterating in order.
Test iterating out of order.
Test iterating in batches.
Test iterating with multiple workers.
A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
Try specifying particular columns.
Test id shrinkage
Test task shrinkage
Test max print size
Create image file
Create zip of image file
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
Now perform move
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
featurization
train/valid split.
singletask load
comparison
Only for debug!
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
multitask load
Do train/valid split.
singletask load
comparison
Get the labels/weights
Normalize shapes
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight.
this works because y is 1D
This is the right ratio since int(N/num_c) * num_c \approx N
for all classes
Flattening is safe because of shape check above
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
Some transformation must happen
Add this case in to handle non-DiskDataset that should be written to disk
Note that transformers have to be undone in reversed order
Handle division by zero
Handle division by zero
Control for pathological case with no variance.
Handle case with 1 task correctly
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Handle 1-D case
Remove labels with zero weights
Note that we may have 0 elements of a given class since we remove those
labels with zero weight. This typically happens in multitask datasets
where some datapoints only have labels for some tasks.
this works because task_y is 1D
This is the right ratio since N_task/num_c * num_c = N_task
for all classes
Set to the class weight computed previously
Need this for transform_y
Handle 1D case
THis reshape is safe because of guard above.
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
flake8: noqa
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Load mini log-solubility dataset.
The transformer generates n DAGs for a molecule with n
"atoms. These are denoted the ""parents"""
extract only the images (no need of the labels)
reshaping the vector to image
Check Blurring
Check center crop
Check crop
Check convert2gray
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
Check median filter
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Note that nothing should change in this dataset since weights balance!
Check that still we have length 6
Check shapes
Check that we have 2 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
Check that we have length 8 now with duplication
Check shapes
Check that we have 4 positives and 4 negatives
Check that sum of 0s equals sum of 1s in transformed for each task
6-1 imbalance in favor of class 0
Check that we have length 30 now with duplication
Check shapes
Check that we have 6 of each class
Check that sum of all class weights is equal by comparing to 0 weight
Note class imbalance. This will round to 2x duplication for 1
Check that we have length 13 now with duplication
Check shapes
Check that we have 6 positives and 7 negatives
################################################################
save.py is out of date. You should not import any functions from here.
################################################################
flake8: noqa
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
Walk through the original file and extract ATOM/HETATM lines and
add PDBQT charge annotations.
Remove rotatable bonds from this molecule
Get the connected components now that the rotatable bonds have
been removed.
The root is the largest connected component.
Write the root component
"We've looked at the root, so take note of that"
Compute partial charges on molecule if RDKit Mol
indices to atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
TODO: This is duplicated! Clean up
Updates charges in place
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
False here specifies that water is to be removed
Updates charges in place
TODO: This is wrong. Should return all molecules
Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
This updates in place
indices of atoms to keep
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
####################################################
Compute partial charges on molecule if rdkit
####################################################
Number of voxels per one edge of box to voxelize.
"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
If interval1 < interval2 entirely
If interval2 < interval1 entirely
Each triangle in the simplices is a set of 3 atoms from
coordinates which forms the vertices of an exterior triangle on
the convex hull of the macromolecule.
Points is the set of atom coordinates that make up this
triangular face on the convex hull
Let's extract x/y/z coords for this face
Let's compute min/max points
"Nitrogen has atomic number 7, and oxygen 8."
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
If atom is a hydrogen
"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
if ring is aromatic
"save its indices, center, and normal"
remember mol1-mol2 pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
find interacting rings from mol1 and cations from mol2
find interacting cations from mol1 and rings from mol2
merge counters
Make sure input is a list
FIXME: Incompatible types in assignment
"FIXME: Argument 1 to ""enumerate"" has incompatible type"
Ensure that metric is wrapped in a list.
This case checks if input is a function then wraps a
dc.metrics.Metric object around it
Process input metrics
Compute multitask metrics
We use y/w to aggregate labels/weights across generator.
This is a KerasModel.
Some datasets have weights
Process predictions and populate y/w lists
Combine labels/weights
Undo data transformations.
Compute multitask metrics
the line has format
REMARK VINA RESULT: score ...
There is only 1 such line per model so we can append it
"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
Apply common fixes to PDB files
Optimize ligand
flake8: noqa
The number of elements to print for dataset ids/tasks
"If a dataset contains more than this number of elements, it won't"
print any dataset ids
An activation function for a Keras layer: either a TensorFlow function or the name of a standard activation
"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
"A single value of some type, or multiple values of that type"
The shape of a NumPy array
type of RDKit object
type of Pymatgen object
Generate a random temporary file name
Ensure the file is created
Open the file in the given mode
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
Reset aggregator
Handle final leftovers for this file
First line of user-specified CSV *must* be header.
"If gzipped, need to compute extension again"
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
init an one-hot vector
"If include_unknown_set is True, set the last index is 1."
################################################################
atom (node) featurization
################################################################
################################################################
bond (edge) featurization
################################################################
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
Loosening atol to see if tests stop failing sporadically
string set
integer set
include_unknown_set is False
include_unknown_set is True
check unknown atoms
check original set
"Generally, =O behaves as an electron acceptor"
we must compute partial charges before using `get_atom_partial_charge`
The C-N bond is a single bond
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
self.ligand_file is for 3ws9_ligand.sdf
simple flat ring
self.cycle4.Compute2DCoords()
load and sanitize two real molecules
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
def test_compute_cation_pi(self):
"# TODO(rbharath): find better example, currently dicts are empty"
"dicts1 = compute_cation_pi(self.prot, self.lig)"
"dicts2 = compute_cation_pi(self.lig, self.prot)"
"TODO find better example, currently dicts are empty"
TODO test more formats for ligand
Test on RDKit
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
TODO test more formats for ligand
Construct a random class probability matrix
Construct a random class probability matrix
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
"Note that since no name as provided, metrics are index by order"
given.
TODO: Fix this case with correct thresholding
TODO: Fix this case with correct thresholding
There are 4 faces to the shape created by coords
flake8: noqa
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Combine the features, then sort them by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Create a map from the original atom indices within each molecule to the
indices in the combined object.
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any specific molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Determine the membership (atom i belongs to molecule membership[i])
Initialize the new degree separated adjacency lists
Update the old adjacency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
"Requriments - transformers, tokenizers"
"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
The vocab may be expanded in the near future
"|-|\+|\\|\/|:|~|@|\?|>>?|\*|\$|\%[0-9]{2}|[0-9])"""""""
add vocab_file dict
"unk_token=""[UNK]"","
"sep_token=""[SEP]"","
"pad_token=""[PAD]"","
"cls_token=""[CLS]"","
"mask_token=""[MASK]"","
take into account special tokens in max length
flake8: noqa
Initalize with 1
Replace the hybridization
global possible_hybridization_list
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
"Handle edge case of self-pairs (i, i)"
Increment by 1 since we don't want 0-indexing
"This creates a matrix of shape (2, num_pairs)"
Get mapping
first `bt_len` features are bond features(if applicable)
For ring pairs outside max pairs distance continue
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
distance is a matrix of 1-hot encoded distances for all atoms
For ring pairs outside max pairs distance continue
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
TODO (VIGS25): Complete the description
Handle loading failures which return None
Fit atomic conv model
Add the Atomic Convolution layers to fetches
Extract the atomic convolution features
flake8: noqa
base classes for featurizers
molecule featurizers
complex featurizers
material featurizers
for str
for list
validation
skip list
skip path string
main logic
Special case handling of single molecule
Convert iterables to list
"mol must be a RDKit Mol object, so parse a SMILES"
"SMILES is unique, so set a canonical order of atoms"
"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
validate params
This assumes that the edge features for self loops are full-zero tensors
In the future we may want to support featurization for self loops
stack features
"before stacking edge_features or node_pos_features,"
we should check whether these are None or not
create new edge index
graph_index indicates which nodes belong to which graph
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Set the bond line coordinates to the bond property used.
Turn atomic coordinates into image positions
Set the atom positions in image to different atomic properties in channels
Check whether num_confs >=1 or not
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
generate SMILES for fragments
Extend shorter strings with padding
Padding before and after
validation
load pretrained models
convert errors to zero
flake8: noqa
If partial charges were not computed
construct atom (node) feature
construct edge (bond) index
add edge list considering a directed graph
construct edge (bond) feature
initialize
check initialization
"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
Check whether num_confs >=1 or not
Convert AtomPositions from Angstrom to bohr (atomic units)
"`(1, max_atoms)` -> `(max_atoms,)`"
Get full N x N SCM
flake8: noqa
load atom_init.json
check whether the atom feature exists or not
construct bi-directed graph
Increase dimension of distance tensor and apply filter
We compute pairwise contact fingerprints
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"distances = compute_pairwise_distances(frag1[0], frag2[0])"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
We compute pairwise contact fingerprints
"frag1_xyz = subtract_centroid(frag1[0], centroid)"
"frag2_xyz = subtract_centroid(frag2[0], centroid)"
"xyzs = [frag1_xyz, frag2_xyz]"
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
We compute pairwise contact fingerprints
"rdks = [frag1[1], frag2[1]]"
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
flake8: noqa
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
TODO(rbharath): Is this squeeze OK?
flake8: noqa
"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
"contacts[0] is the x_coords, that is the frag1 atoms that have"
nonzero contact.
"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
We compute pairwise contact fingerprints
Get coordinates
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge,"
"voxels_per_edge, num_feat) so we should concatenate on the last"
axis.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
We compute pairwise contact fingerprints
Get coordinates
"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
We compute pairwise contact fingerprints
"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
TODO test more formats for ligand
TODO test more formats for ligand
with one conformer
with multiple conformers
include explicit hydrogens
with one conformer
with multiple conformers
include explicit hydrogens
"Requirements - transformers, tokenizers"
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
"assert ""C1=CC=CN=C1"""
"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
check for separate count and SMILES entries for each fragment
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_counter():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
"features, failures = featurizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
""
""
"# TODO: This is failing, something about the hydrogen bond counting?"
def test_hydrogen_bond_voxelizer():
current_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(current_dir, 'data',"
'3ws9_protein_fixer_rdkit.pdb')
"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
""
cutoff = 4.5
box_width = 16
voxel_width = 1.0
voxelizer = dc.feat.HydrogenBondVoxelizer(
"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
# TODO: Add shape test
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
not support array style inputs
check convert function
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
untranform
untranform
untranform
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
TODO(rbharath): This test will be uncommented in the next PR up on the docket.
def test_full_complex_featurization(self):
"""""""Unit test for ComplexNeighborListFragmentAtomicCoordinates."""""""
dir_path = os.path.dirname(os.path.realpath(__file__))
"ligand_file = os.path.join(dir_path, ""data/3zso_ligand_hyd.pdb"")"
"protein_file = os.path.join(dir_path, ""data/3zso_protein.pdb"")"
"# Pulled from PDB files. For larger datasets with more PDBs, would use"
# max num atoms instead of exact.
frag1_num_atoms = 44  # for ligand atoms
frag2_num_atoms = 2336  # for protein atoms
complex_num_atoms = 2380  # in total
max_num_neighbors = 4
# Cutoff in angstroms
neighbor_cutoff = 4
complex_featurizer = ComplexNeighborListFragmentAtomicCoordinates(
"frag1_num_atoms, frag2_num_atoms, complex_num_atoms, max_num_neighbors,"
neighbor_cutoff)
"(frag1_coords, frag1_neighbor_list, frag1_z, frag2_coords,"
"frag2_neighbor_list, frag2_z, complex_coords,"
"complex_neighbor_list, complex_z) = complex_featurizer._featurize_complex("
"ligand_file, protein_file)"
""
"assert frag1_coords.shape == (frag1_num_atoms, 3)"
self.assertEqual(
"sorted(list(frag1_neighbor_list.keys())), list(range(frag1_num_atoms)))"
"self.assertEqual(frag1_z.shape, (frag1_num_atoms,))"
""
"self.assertEqual(frag2_coords.shape, (frag2_num_atoms, 3))"
self.assertEqual(
"sorted(list(frag2_neighbor_list.keys())), list(range(frag2_num_atoms)))"
"self.assertEqual(frag2_z.shape, (frag2_num_atoms,))"
""
"self.assertEqual(complex_coords.shape, (complex_num_atoms, 3))"
self.assertEqual(
"sorted(list(complex_neighbor_list.keys())),"
list(range(complex_num_atoms)))
"self.assertEqual(complex_z.shape, (complex_num_atoms,))"
Carbon
Test distance 1
Test distance 2
Test alkane
Test distance 1
3 self connections and 2 bonds which are both counted twice because of
symmetry for 7 total
Test distance 2
Everything is connected at this distance
Test alkane
Test distance infinity
Everything is connected at this distance
Test pentane
Test distance infinity
Everything is connected at this distance
Only one carbon
Test feature sizes
"No bonds, so only 1 pair feature (for the self interaction)"
Only 4 atoms
Test feature sizes for chirality
3 carbonds in alkane
Test feature sizes
Should be a 3x3 interaction grid
mol_list = featurizer.featurize(mols)
mol = mol_list[0]
3 carbonds in alkane
Test feature sizes
Should be a 7x14 interaction grid since there are 7 pairs within graph
distance 1 (3 self interactions plus 2 bonds counted twice because of
symmetry)
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
import rdkit.Chem
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
5 atoms in compound
Test feature sizes
Should be a 3x3 interaction grid
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
First example molecule
Artificial feature array.
Second example molecule
Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
calculate coordinates
not zero values
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
validation
skip list
skip path string
main logic
for str
for list
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
Figure out how many positive samples we want for each task in each dataset.
Assign the positive samples to datasets.  Since a sample may be positive
"on more than one task, we need to keep track of the effect of each added"
"sample on each task.  To try to keep everything balanced, we cycle through"
"tasks, assigning one positive sample for each one."
We have a sample that hasn't been assigned yet.  Assign it to
whichever set currently has the lowest fraction of its target for
this task.
The remaining samples are negative for all tasks.  Add them to fill out
each set to the correct total number.
"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
################################################################
Splitter for molecule datasets
################################################################
Sort by increasing MW
calcaulate scaffold sets
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
Compute fingerprints for all molecules.
Split into two groups: test training set and everything else.
Split the second group into validation and test sets.
Begin by assigning the first molecule to the first group.
Decide which group to assign a molecule to.
Identify the unassigned molecule that is least similar to everything in
the other group.
Add it to the group.
Update the data on unassigned molecules.
Sort from largest to smallest scaffold sets
################################################################
Not well supported splitters
################################################################
All datasets share features and identifiers by assumption.
flake8: noqa
basic splitter
molecule splitter
other splitter
################################################################
Removed API
################################################################
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
"task_type = ""regression"""
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition the positives for each task roughly in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
The amount of datapoints has to be the same
The number of scaffolds generated by the splitter
has to be smaller or equal than number of total molecules
Add the input features.
Add the convolutional layers
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Chain of flows is also a normalizing flow
An instance of tfd.TransformedDistribution
TODO: Incompability between TF and TFP means that TF doesn't track
trainable variables in the flow; must override `_create_gradient_fn`
self._variables = self.flow.trainable_variables
"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
"CrossEntropyLoss only supports (batch_size, classes, tasks)"
This is for API consistency
extended one of probabilites to binary distribution
extended one of probabilites to binary distribution
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
"There are no neighbors of this degree, so just create an empty tensor directly."
Expand dims
always deg-1 for deg_adj_lists
Extract graph topology
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
Note that AP_ij and AP_ji share the same self.AP_bn batch
normalization
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
Final atom-layer convolution. Note this differs slightly from the paper
since we use a tanh activation as default. This seems necessary for numerical
stability.
Now fully connected layers
Should this allow for training?
"pair_edges is of shape (2, N)"
number of atoms in each molecule
index of pair features
Get starting pair atoms
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
W&B logging
Backwards compatibility
The optimizer creates internal variables the first time apply_gradients()
is called for a new set of variables.  If that happens inside a function
"annotated with tf.function it throws an exception, so call it once here."
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to
0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Use a GradientTape to compute gradients.
Ensure weights for both models are built.
Add the input features.
Add the dense layers
Add the input features.
Add the dense layers
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
TODO: Turning off queue for now. Safe to re-activate?
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
flake8: noqa
scikit-learn model
PyTorch models
####################################################################################
Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
####################################################################################
#######################################################################################
Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
#######################################################################################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Predict the output and uncertainty.
Predict the output and uncertainty.
The DAG models have high error with dropout
"Despite a lot of effort tweaking it , there appears to be"
a limit to how low the error can go with dropout.
assert mean_error < 0.5 * mean_value
Predict the output and uncertainty.
Fit trained model
Eval model on train
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
load datasets
disable transformer
check train
check predict shape
check overfit
reload
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reloaded Trained Model
Check predictions match on random sample
Eval model on train
3D Multivariate Gaussian base distribution
Check that reloaded model can sample from the distribution
Check that density estimation is same for reloaded model
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload Trained Model
Check predictions match on random sample
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check predictions match on random sample
Check predictions match on random sample
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Eval model on train
Check predictions match on random sample
Fit trained model
Eval model on train
Reload trained model
Check predictions match on random sample
Eval model on train
Reload trained Model
Check predictions match on random sample
Eval model on train
Reload Trained Model
Check predictions match on random sample
TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
Reload Trained Model
Check predictions match on original dataset
TODO: We need a cleaner usage example for this
Fit trained model
Eval model on train
Check predictions match on random sample
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
There are 4 atoms each of which have 75 atom features
There are 10 pairs with infinity distance and 14 pair features
4 atoms in total
10 pairs in total
10 pairs in total each with start/finish
There are 4 atoms each of which have 75 atom features
"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
connections and accounting for symmetry.)
4 atoms in total
10 pairs in total
The center atom is self connected and to both neighbors so it appears
thrice. The canonical ranking used in MolecularFeaturizer means this
central atom is ranked last in ordering.
10 pairs in total each with start/finish
def test_weave_fit_simple_infinity_distance():
featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
"X = featurizer([""C"", ""CCC""])"
"y = np.array([0, 1.])"
"dataset = dc.data.NumpyDataset(X, y)"
batch_size = 20
model = WeaveModel(
"1,"
"batch_size=batch_size,"
"mode='classification',"
"fully_connected_layer_sizes=[2000, 1000],"
"batch_normalize=True,"
batch_normalize_kwargs={
"""fused"": False,"
"""trainable"": True,"
"""renorm"": True"
"},"
learning_rage=0.0005)
"model.fit(dataset, nb_epoch=200)"
transformers = []
metric = dc.metrics.Metric(
"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
"scores = model.evaluate(dataset, [metric], transformers)"
assert scores['mean-roc_auc_score'] >= 0.9
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Predict the output and uncertainty.
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
eval model on test
prepare dataset
global setting
xgboost test
fit trained model
reload
check predictions match on test dataset
eval model on test
prepare dataset
global setting
lightgbm test
fit trained model
reload
check predictions match on test dataset
eval model on test
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"frag2_z = np.random.rand(N_atoms, 3)"
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
No training has been done after reload
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Check that predicting internal layers works.
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
Reload the save model and confirm that it matches the best logged score.
3D Multivariate Gaussian base distribution
Must be float32 for RealNVP
Tests a simple flow of one RealNVP layer.
log likelihoods should be negative
# Fit model
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
x and y are the same tensor (equivalent at every element)
the pairwise inner product of the rows in x and y will always be 1
"the output tensor will be of shape (5,5)"
each row in x1 is orthogonal to each row in x2
the pairwise inner product of the rows in x and y will always be 0
"the output tensor will be of shape (256,256)"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
index of pair features
number of pairs for each atom
atom features
pair features
"Outputs should be [A, P]"
atom features
Try without compression
"Outputs should be [mol1_vec, mol2_vec)"
Try with compression
"Outputs should be [mol1_vec, mol2_vec)"
atom features
"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
Gaussian histograms expands into 11 Gaussian buckets.
"assert np.array(outputs[1]).shape == (11 * 75,)"
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Recall that the DAG layer expects a MultiConvMol as input,"
"so the ""batch"" is a pooled set of atoms from all the"
"molecules in the batch, just as it is for the graph conv."
This means that n_atoms is the batch-size
dropout_switch = False
dropout_switch
# TODO(rbharath): What is the shape of outputs supposed to be?
"# I'm getting (7, 30) here. Where does 7 come from??"
TODO(rbharath): We need more documentation about why
these numbers work.
Create a dataset and an input function for processing it.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Each epoch is a single step for this model
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
overfit test
test on a small MoleculeNet dataset
load datasets
initialize models
embedding node features
convolutional layer
pooling
for n_tasks == 1 case
Decide first number of GAT layers
flake8:noqa
Select a device.
W&B logging
Main training loop.
"Execute the loss function, accumulating the gradients."
Report progress and write checkpoints.
Capture the last avg_loss in case of return since we're resetting to 0 now
Report final results.
Invoke the model.
Apply tranformers and record results.
Concatenate arrays to create the final results.
Compute the gradients.
Save the checkpoint to a file.
Rename and delete older files.
Ensure weights for both models are built.
Some scikit-learn models don't use weights.
flake8: ignore
GDBT doesn't support multi-output(task)
Find optimal n_estimators based on original learning_rate and early_stopping_rounds
retrain model to whole data using best n_estimators * 1.25
GDBT doesn't support multi-output(task)
########################################
Deprecation warnings for XGBoostModel
########################################
flake8: noqa
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
#### TIMING ######
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Build the path to the dataset on disk.
Try to reload cached datasets.
Create the dataset
Split and transform the dataset.
. clinical trial toxicity (or absence of toxicity)
. FDA approval status.
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
#### TIMING ###########
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
######### TIMING ################
Most reaction dataset ML tasks train the prediction of products from
"ractants. Both of these are contained in the rxn object that is output,"
"so there is no ""tasks"" field."
Download USPTO dataset
Unzip
Unzipped file is a tap seperated values file (despite the .txt)
The first element in the row is the reaction smarts
"Sometimes smarts have extraneous information at end of form """
"|f:0"" that causes parsing to fail. Not sure what this information"
"is, but just ignoring for now."
Make up dummy labels since DiskDataset.from_numpy doesn't allow
creation from just features for now.
TODO: This dataset isn't saved to disk so reload doesn't happen.
dict of accepted featurizers for this dataset
modify the returned dicts for your dataset
Names of supported featurizers
dict of accepted transformers
dict of accepted splitters
names of supported splitters
Warning message about this template
Featurize mydataset
Get DeepChem data directory if needed
Check for str args to featurizer and splitter
Reload from disk
First type of supported featurizers
"If featurizer requires a non-CSV file format, load .tar.gz file"
Changer loader to match featurizer and data file type
Featurize dataset
Initialize transformers
"get pdb and sdf filenames, labels and pdbids"
load and featurize each complex
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
"def load_pcba_146(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_146.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
"def load_pcba_2475(featurizer='ECFP',"
"split='random',"
"reload=True,"
"data_dir=None,"
"save_dir=None,"
**kwargs):
return load_pcba_dataset(
"featurizer=featurizer,"
"split=split,"
"reload=reload,"
"assay_file_name=""pcba_2475.csv.gz"","
"data_dir=data_dir,"
"save_dir=save_dir,"
**kwargs)
def test_qm9_loader():
current_dir = os.path.dirname(os.path.abspath(__file__))
"tasks, datasets, transformers = load_qm9("
"reload=False,"
"data_dir=current_dir,"
"featurizer='ECFP',"
splitter_kwargs={
"'seed': 42,"
"'frac_train': 0.6,"
"'frac_valid': 0.2,"
'frac_test': 0.2
})
""
assert len(tasks) == 12
assert tasks[0] == 'mu'
"assert datasets[0].X.shape == (8, 1024)"
def test_zinc15_loader():
current_dir = os.path.dirname(os.path.abspath(__file__))
""
"tasks, datasets, transformers = load_zinc15("
"reload=False,"
"data_dir=current_dir,"
splitter_kwargs={
"'seed': 42,"
"'frac_train': 0.6,"
"'frac_valid': 0.2,"
'frac_test': 0.2
})
""
test_vec = np.array([
"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,"
"0.0, -1.224744871391589, 0.0, 0.0, 0.0, 0.0, 2.0, -0.5, 0.0, 0.0, 0.0,"
"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0"
])
""
"train, val, test = datasets"
"assert tasks == ['mwt', 'logp', 'reactive']"
"assert train.X.shape == (3, 100, 35)"
"assert np.allclose(train.X[0][0], test_vec, atol=0.01)"
""
"if os.path.exists(os.path.join(current_dir, 'zinc15_250K_2D.csv')):"
"os.remove(os.path.join(current_dir, 'zinc15_250K_2D.csv'))"
Range of optimization
We know from guard above that this is an int/float
Specify logfile
Make logdir if it doesn't exist.
setup range
Stores all results
Store all model references so we don't have to reload
Stores all model locations
Demarcating internal function for readability
"param values are always float in BO, so this line converts float to int"
see : https://github.com/josejimenezluna/pyGPGO/issues/10
Record hyperparameters
Add it on to the information needed for the constructor
Some models autosave
Record performances
Store all results
Store reference to model
GPGO maximize performance by default
set performance to its negative value for minimization
execute GPGO
FIXME: Incompatible types in assignment
Let's fetch the model with the best parameters
Compare best model to default hyperparameters
Record hyperparameters
Return default hyperparameters
Construction dictionary mapping hyperparameter names to values
Some models autosave
arbitrarily return last model
flake8: noqa
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Generate dummy dataset
Generate dummy dataset
These are per-example multiplier
Test that 2 parameters were optimized
Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the inputs and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Training loop.
Do checkpointing.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the inputs and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A2C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
"This policy just learns a constant probability for each action, and a constant for the value."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
"default channels are ""conda-forge"" and ""omnia"""
"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Batch size of models
Fit models
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating positives and negatives so each batch will have
equal numbers of both.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load SAMPL(FreeSolv) dataset
Define metric
Batch size of models
Fit trained model
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
plt.show()
"run_benchmark(FILE, DEEPCHEM_DIR)"
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Build the meta-learning model.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Create a Checkpoint for saving.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"If we train on the current task, the loss should go down."
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
Best score case
Worst score case
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
"X = np.random.rand(10, 1, 4, 50)"
"y = np.random.randint(0, 2, size=(10, 1))"
"dataset = dc.data.NumpyDataset(X, y)"
Call in-silico mutagenesis
Construct and train SequenceDNN model
"X = np.random.rand(10, 1, 4, 50)"
"y = np.random.randint(0, 2, size=(10, 1))"
"dataset = dc.data.NumpyDataset(X, y)"
Call in-silico mutagenesis
Check nonzero elements exist
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
"(X, y, w, ids)"
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset and have it create an Iterator.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
(ytz): this skips everything except possibly the last shard
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
First try using images for X.
Now try using images for y.
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Generate data
Transform it
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Create image file
Create zip of image file
"self.zip_path = ""/home/rbharath/misc/cells.zip"""
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
extracting validation set of MNIST for testing the DataTransforms
extract only the images (no need of the labels)
reshaping the vector to image
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Test on random example
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Test if dimensionality expansion is handled correctly by untransform
Check ids are unchanged before and after transformation
Check X is unchanged since transform_y is true
Check w is unchanged since transform_y is true
Check minimum and maximum values of transformed y are 0 and 1
Check untransform works correctly
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check Blurring
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
TODO: mol should be always sanitized when charges are calculated
can't change it now because it would break a lot of examples
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
This is a KerasModel.
This is a TensorGraph.
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
Extend shorter strings with padding
Padding before and after
Setup image
Compute bond properties
Compute atom properties
Setup image
Compute bond properties
Compute atom properties
Reshape done for proper broadcast
"Reshapes, and axes manipulations to facilitate vector processing."
Draw a line between the two atoms.
"The coordinates of this line, are indicated in line_coords"
Turn the line coordinates into image positions
Set the bond line coordinates to the bond property used.
Turn atomic coordinates into image positions
Set the atom positions in image to different atomic properties in channels
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
TODO(rbharath): Is this squeeze OK?
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Results should be sorted by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
TODO (VIGS25): Complete the description
Handle loading failures which return None
Fit atomic conv model
Add the Atomic Convolution layers to fetches
Extract the atomic convolution features
Handle loading failures which return None
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other
splits (or potentially refactor those to match this).
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
calculate percent split for valid (out of test and valid)
"split remaining data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
Create the inputs.
Create the generators.
Create the discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
We pass an input to the Variable layer to work around a bug in TF 1.14.
Compute the weighted errors
Add an entropy term to the loss.
Create the Keras model.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
-*- coding: utf-8 -*-
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
So the Tensor has known dimensions
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Build the model.
Build the model.
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Build the model.
Build the model.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
"The model doesn't specify inputs, so guess the input shapes based on the"
example batch.
The loss doesn't depend on any variables.
Main training loop.
"In eager mode we execute the loss function, accumulating the gradients."
In graph mode we execute the training op.
Report progress and write checkpoints.
Report final results.
In eager mode we invoke the model directly.
In graph mode we execute the output tensors.
Apply tranformers and record results.
Concatenate arrays to create the final results.
"If only one output, just return array"
In eager mode we use a GradientTape to compute gradients.
In graph mode we use tf.gradients().
Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
self.session is used because restore was called in the same session
Add the input features.
Add the dense layers
Add the input features.
Add the dense layers
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
Build the model.
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
TODO: Turning off queue for now. Safe to re-activate?
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
Add the input features.
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
Last layer sequences not returned.
This is needed because ImageDataGenerator does infinite looping
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
This happens when building an estimator.
Calculate what the new shape will be.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
No other forget biases supported right now.
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
Number of grid cells
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
Add in features
Add in labels
Add in all layers
The last layer is the output of the model
TODO(rbharath): Add in support for additional
losses.
TODO(rbharath): The TensorGraph can't be built until
fit is called since the shapes of features/labels
not specified. Need to figure out a good restoration
method for this use case.
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
!/usr/bin/env python2
-*- coding: utf-8 -*-
number of inputs each step
Arguments
Aliases.
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
In eager mode we want an optimizer and a function to compute the
gradient of the loss.
In graph mode we want a training operation.
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
instead of using the **kwargs hack.
Gather results for each output
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"If only one output, just return array"
Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
"The next release of Tensorflow will add a proper jacobian() function, so"
we can remove this then.
"Remove extra dimensions, because I couldn't figure out how to get the"
jacobian() function to not produce them.
"In eager mode, we need to execute every layer once to ensure its variables"
have been created.
"We can't execute Input layers in eager mode, since they would try"
to create placeholders.  Instead create a tensor of the correct
size and type.
Build the layers.
Initialize variables.
In graph mode we need to create the computation graph.
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Check the inputs.
Define a function that recursively creates tensors from layers.
Define the model function.
Define the inputs.
"Create the correct outputs, based on the mode."
Create the Estimator.
Add or remove dimensions of size 1 to match the shape of the layer.
Should we keep a separate global step count for each submodel?
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Prepare Training Data
Train the model
Prepare the Testing data
predict
check output shape
new object of UNet to test if loading the model results in same predictions
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
"If we don't specify the initial state, it should default to zeros."
Explicitly specifying the zero state should give the same result.
Specifying a different initial state should produce a different result.
We should get the same result with either predict_on_batch() or __call__().
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Test for correct value return (normal mode)
Test for shapes (normal mode)
Test for correct value return (eager mode)
Test for shape (eager mode)
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Layer is wrapper around embedding lookup, tested that then"
Expected output
Create a dataset with three tasks.  The first two tasks each depend only
on half the features.  The third task depends on all of them.
Create an OntologyModel.  Two leaf nodes contain half the features.
Train the model on the datase.
It should have learned to predict all of the tasks accurately.
"In addition, it should be able to predict the first task based only on the"
"first leaf node, and the second task based only on the second leaf node."
Create a dataset with three tasks.  The first two tasks each depend only
on half the features.  The third task depends on all of them.
Create an OntologyModel.  Two leaf nodes contain half the features.
Train the model on the datase.
It should have learned to predict all of the tasks accurately.
"In addition, it should be able to predict the first task based only on the"
"first leaf node, and the second task based only on the second leaf node."
Here are mappings for just a few yeast genes.
"Build the ontology, then see if it looks correct."
Should be able to call fit twice without failure.
# TODO(rbharath): Transform these into useful weights.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
# TODO(rbharath): Add a test with per-class weighting.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
Prepare Training Data
Train the model
Prepare the Testing data
predict
check output shape
new object of ResNet to test if loading the model results in same predictions
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
def test_multi_task_classifier(self):
"""""""Test creating an Estimator from a MultitaskClassifier."""""""
n_samples = 10
n_features = 3
n_tasks = 2
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y, w)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"return {'x': x, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
"model = dc.models.MultitaskClassifier(n_tasks, n_features, dropouts=0)"
""
# Create an estimator from it.
""
"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
""
"def accuracy(labels, predictions, weights):"
"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
""
metrics = {'accuracy': accuracy}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(100))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-4
assert results['accuracy'] > 0.9
""
def test_multi_task_regressor(self):
"""""""Test creating an Estimator from a MultitaskRegressor."""""""
n_samples = 10
n_features = 3
n_tasks = 2
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y, w)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"return {'x': x, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
"model = dc.models.MultitaskRegressor(n_tasks, n_features, dropouts=0)"
""
# Create an estimator from it.
""
"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
metrics = {'error': tf.metrics.mean_absolute_error}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(100))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-3
assert results['error'] < 0.1
""
def test_robust_multi_task_classifier(self):
"""""""Test creating an Estimator from a MultitaskClassifier."""""""
n_samples = 10
n_features = 3
n_tasks = 2
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y, w)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"return {'x': x, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
model = dc.models.RobustMultitaskClassifier(
"n_tasks,"
"n_features,"
"layer_sizes=[50],"
"bypass_layer_sizes=[10],"
"dropouts=0,"
"bypass_dropouts=0,"
learning_rate=0.003)
""
# Create an estimator from it.
""
"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
""
"def accuracy(labels, predictions, weights):"
"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
""
metrics = {'accuracy': accuracy}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(500))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-2
assert results['accuracy'] > 0.9
""
def test_robust_multi_task_regressor(self):
"""""""Test creating an Estimator from a MultitaskRegressor."""""""
n_samples = 10
n_features = 3
n_tasks = 2
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y, w)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"return {'x': x, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
model = dc.models.RobustMultitaskRegressor(
"n_tasks,"
"n_features,"
"layer_sizes=[50],"
"bypass_layer_sizes=[10],"
"dropouts=0,"
"bypass_dropouts=0,"
learning_rate=0.003)
""
# Create an estimator from it.
""
"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
metrics = {'error': tf.metrics.mean_absolute_error}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(500))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-2
assert results['error'] < 1e-2
Create a dataset and an input function for processing it.
Create the model.
Create an estimator from it.
Train the model.
Evaluate the model.
def test_irv(self):
"""""""Test creating an Estimator from a IRVClassifier."""""""
n_samples = 50
n_features = 3
n_tasks = 2
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y, w)"
"transformers = [dc.trans.IRVTransformer(10, n_tasks, dataset)]"
""
for transformer in transformers:
dataset = transformer.transform(dataset)
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"return {'x': x, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
model = dc.models.TensorflowMultitaskIRVClassifier(
"n_tasks, K=10, learning_rate=0.001, penalty=0.05, batch_size=50)"
model.build()
# Create an estimator from it.
""
"x_col = tf.feature_column.numeric_column('x', shape=(2 * 10 * n_tasks,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
""
"def accuracy(labels, predictions, weights):"
"return tf.metrics.accuracy(labels, tf.round(predictions[:, :, 1]),"
weights)
""
metrics = {'accuracy': accuracy}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(100))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['accuracy'] > 0.9
""
def test_textcnn_classification(self):
"""""""Test creating an Estimator from TextCNN for classification."""""""
n_tasks = 2
n_samples = 5
""
# Create a TensorGraph model.
seq_length = 20
model = dc.models.TextCNNModel(
"n_tasks=n_tasks,"
"char_dict=default_dict,"
"seq_length=seq_length,"
"kernel_sizes=[5, 5],"
"num_filters=[20, 20])"
""
np.random.seed(123)
"smile_ids = [""CCCCC"", ""CCC(=O)O"", ""CCC"", ""CC(=O)O"", ""O=C=O""]"
X = smile_ids
"y = np.zeros((n_samples, n_tasks))"
"w = np.ones((n_samples, n_tasks))"
"dataset = NumpyDataset(X, y, w, smile_ids)"
""
"def accuracy(labels, predictions, weights):"
"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"smiles_seq = tf.py_func(model.smiles_to_seq_batch, inp=[x], Tout=tf.int32)"
"return {'x': smiles_seq, 'weights': weights}, y"
""
# Create an estimator from it.
x_col = tf.feature_column.numeric_column(
"'x', shape=(seq_length,), dtype=tf.int32)"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
metrics = {'accuracy': accuracy}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
estimator.train(input_fn=lambda: input_fn(100))
""
# Evaluate results
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-2
assert results['accuracy'] > 0.9
""
def test_textcnn_regression(self):
"""""""Test creating an Estimator from TextCNN for regression."""""""
n_tasks = 2
n_samples = 10
""
# Create a TensorGraph model.
seq_length = 20
model = dc.models.TextCNNModel(
"n_tasks=n_tasks,"
"char_dict=default_dict,"
"seq_length=seq_length,"
"kernel_sizes=[5, 5],"
"num_filters=[20, 20],"
"mode=""regression"")"
""
np.random.seed(123)
"smile_ids = [""CCCCC"", ""CCC(=O)O"", ""CCC"", ""CC(=O)O"", ""O=C=O""]"
X = smile_ids
"y = np.zeros((n_samples, n_tasks, 1), dtype=np.float32)"
"w = np.ones((n_samples, n_tasks))"
"dataset = NumpyDataset(X, y, w, smile_ids)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"smiles_seq = tf.py_func(model.smiles_to_seq_batch, inp=[x], Tout=tf.int32)"
"return {'x': smiles_seq, 'weights': weights}, y"
""
# Create an estimator from it.
x_col = tf.feature_column.numeric_column(
"'x', shape=(seq_length,), dtype=tf.int32)"
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
metrics = {'error': tf.metrics.mean_absolute_error}
estimator = model.make_estimator(
"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
""
# Train the model.
estimator.train(input_fn=lambda: input_fn(100))
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 1e-1
assert results['error'] < 0.1
""
def test_scscore(self):
"""""""Test creating an Estimator from a ScScoreModel."""""""
n_samples = 10
n_features = 3
n_tasks = 1
""
# Create a dataset and an input function for processing it.
""
np.random.seed(123)
"X = np.random.rand(n_samples, 2, n_features)"
"y = np.zeros((n_samples, n_tasks))"
"dataset = dc.data.NumpyDataset(X, y)"
""
def input_fn(epochs):
"x, y, weights = dataset.make_iterator("
"batch_size=n_samples, epochs=epochs).get_next()"
"x1 = x[:, 0]"
"x2 = x[:, 1]"
"return {'x1': x1, 'x2': x2, 'weights': weights}, y"
""
# Create a TensorGraph model.
""
"model = dc.models.ScScoreModel(n_features, dropouts=0)"
del model.outputs[:]
model.outputs.append(model.difference)
""
"def accuracy(labels, predictions, weights):"
predictions = tf.nn.relu(tf.sign(predictions))
"return tf.metrics.accuracy(labels, predictions, weights)"
""
# Create an estimator from it.
""
"x_col1 = tf.feature_column.numeric_column('x1', shape=(n_features,))"
"x_col2 = tf.feature_column.numeric_column('x2', shape=(n_features,))"
"weight_col = tf.feature_column.numeric_column('weights', shape=(1,))"
""
estimator = model.make_estimator(
"feature_columns=[x_col1, x_col2],"
"metrics={'accuracy': accuracy},"
weight_column=weight_col)
""
# Train the model.
""
estimator.train(input_fn=lambda: input_fn(100))
""
# Evaluate the model.
""
results = estimator.evaluate(input_fn=lambda: input_fn(1))
assert results['loss'] < 0.5
assert results['accuracy'] > 0.6
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
@flaky
def test_dtnn_regression_model(self):
"""""""Test creating an estimator for DTNNGraphModel for regression"""""""
current_dir = os.path.dirname(os.path.abspath(__file__))
"input_file = os.path.join(current_dir, ""example_DTNN.mat"")"
dataset = loadmat(input_file)
""
num_vals_to_use = 20
""
np.random.seed(123)
X = dataset['X'][:num_vals_to_use]
y = dataset['T'][:num_vals_to_use].astype(np.float32)
w = np.ones_like(y)
"dataset = dc.data.NumpyDataset(X, y, w, ids=None)"
n_tasks = y.shape[1]
n_samples = y.shape[0]
""
"dtypes = [tf.int32, tf.float32, tf.int32, tf.int32, tf.int32]"
""
model = dc.models.DTNNModel(
"n_tasks,"
"n_embedding=20,"
"n_distance=100,"
"learning_rate=1.0,"
"mode=""regression"")"
""
"def mean_relative_error(labels, predictions, weights):"
"error = tf.abs(1 - tf.math.divide(labels, predictions))"
"error_val, update_op = tf.metrics.mean(error)"
"return error_val, update_op"
""
"def input_fn(batch_size, epochs):"
"X, y, weights = dataset.make_iterator("
"batch_size=batch_size, epochs=epochs).get_next()"
features = tf.py_func(
"model.compute_features_on_batch, inp=[X], Tout=dtypes)"
""
assert len(features) == 5
feature_dict = dict()
feature_dict['atom_num'] = features[0]
feature_dict['distance'] = features[1]
feature_dict['dist_mem_i'] = features[2]
feature_dict['dist_mem_j'] = features[3]
feature_dict['atom_mem'] = features[4]
feature_dict['weights'] = weights
""
"return feature_dict, y"
""
atom_number = tf.feature_column.numeric_column(
"'atom_num', shape=[], dtype=dtypes[0])"
distance = tf.feature_column.numeric_column(
"'distance', shape=(model.n_distance,), dtype=dtypes[1])"
atom_mem = tf.feature_column.numeric_column(
"'atom_mem', shape=[], dtype=dtypes[2])"
dist_mem_i = tf.feature_column.numeric_column(
"'dist_mem_i', shape=[], dtype=dtypes[3])"
dist_mem_j = tf.feature_column.numeric_column(
"'dist_mem_j', shape=[], dtype=dtypes[4])"
""
"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
metrics = {'error': mean_relative_error}
""
"feature_cols = [atom_number, distance, dist_mem_i, dist_mem_j, atom_mem]"
estimator = model.make_estimator(
"feature_columns=feature_cols, weight_column=weight_col, metrics=metrics)"
"estimator.train(input_fn=lambda: input_fn(100, 250))"
""
"results = estimator.evaluate(input_fn=lambda: input_fn(n_samples, 1))"
assert results['error'] < 0.1
Construct layers for all nodes.
Create the loss function.
Create inputs for the features.
Create inputs for the children.
Concatenate all inputs together.
Create the output.
"If necessary, download the file defining the ontology."
Parse the ontology definition and create a list of terms.
Create OntologyNode objects for all the terms.
"Assign parent-child relationships between nodes, and identify root nodes."
Create a single root node that combines the three GO roots.
Assign features to nodes.
Count the number of features within each node.  Eliminate nodes with too few
features and set the number of outputs for each one.
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def MultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultitaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
Predict the output and uncertainty.
Predict the output and uncertainty.
Predict the output and uncertainty.
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"frag2_z = np.random.rand(N_atoms, 3)"
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
Check that predicting internal layers works.
Create two models using the same model directory.
Check that they produce different results.
"Save a checkpoint from the first model and load it into the second one,"
and make sure they now match.
Train a model to overfit the dataset.
"Create an identical model, do a single step of fitting with restore=True,"
and make sure it got restored correctly.
Build a model that predicts uncertainty.
Fit the model and see if its predictions are correct.
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Train the model while logging the validation ROC AUC.
Parse the log to pull out the AUC scores.
The last reported score should match the current performance of the model.
Reload the save model and confirm that it matches the best logged score.
Create a dataset and an input function for processing it.
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
Featurize qm9 dataset
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
#### TIMING ######
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
No tasks since no labels provided.
For now images are loaded directly by ImageLoader
Load Sweetlead dataset
Featurize SWEET dataset
Initialize transformers
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Featurize bbb dataset
Initialize transformers
Initialize transformers
Load nci dataset
Featurize nci dataset
Featurize HOPV dataset
Featurize PPB dataset
Load MUV dataset
Featurize MUV dataset
Featurize clearance dataset
Initialize transformers
Featurize BBBC001 dataset
Featurize Images into NumpyArrays
Load text file with labels
Strip the first line which holds field labels
Format is: Image_name count1 count2
This is kludgy way to add y to dataset. Can be done better?
Featurize BBBC002 dataset
Featurize Images into NumpyArrays
Load text file with labels
Strip the first line which holds field labels
Format is: Image_name count1 count2
This is kludgy way to add y to dataset. Can be done better?
Featurize TOXCAST dataset
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
#### TIMING ###########
Featurizer thermosol dataset
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
######### TIMING ################
Most reaction dataset ML tasks train the prediction of products from
"ractants. Both of these are contained in the rxn object that is output,"
"so there is no ""tasks"" field."
Download USPTO dataset
Unzip
Unzipped file is a tap seperated values file (despite the .txt)
The first element in the row is the reaction smarts
"Sometimes smarts have extraneous information at end of form """
"|f:0"" that causes parsing to fail. Not sure what this information"
"is, but just ignoring for now."
Make up dummy labels since DiskDataset.from_numpy doesn't allow
creation from just features for now.
TODO: This dataset isn't saved to disk so reload doesn't happen.
Featurizer hppb dataset
Featurize hiv dataset
Extract locations of data
Extract labels
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
Featurize Data
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
Delete labels and ids for failing elements
No transformations of data
Split dataset
TODO(rbharath): This should be modified to contain a cluster split so
structures of the same protein aren't in both train/test
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Featurize Delaney dataset
Featurize PCBA dataset
Featurize Lipophilicity dataset
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Batch size of models
Fit models
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
graph_model = dc.nn.SequentialGraph(n_feat)
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating postives and negatives so each batch will have
equal numbers of both.
Create the model to train.  We use a simple fully connected network with
one hidden layer.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load SAMPL(FreeSolv) dataset
Define metric
Batch size of models
Fit trained model
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
Atom features with padding
A_tilda_k computation
Final feed_dict setup
"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
"assert atom_features.shape == (self.batch_size, self.max_nodes,"
self.num_node_features)
Fit models
Args
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Args
TODO (VIGS25): Account for the reload option
Downloading train files
Parsing training data
"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
Test Files loading
One Hot Featurization
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/env python2
-*- coding: utf-8 -*-
inputs placeholder
data preprocessing and augmentation
first conv layer
downsample by max pooling
each module is a residual convolutional block
followed by a convolutional downsample layer
max pooling over the final outcome
fully connected layers
dropout for dense layers
"in_layer = Dropout(0.25, in_layers=[in_layer])"
weight decay regularizer
"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
sample cut ratio from a clipped gaussian
train/valid differences
!/usr/bin/env python2
-*- coding: utf-8 -*-
Define and build model
model.restore()
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
plt.show()
"run_benchmark(FILE, DEEPCHEM_DIR)"
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Create duplicate placeholders for meta-optimization.
Create the loss function for meta-optimization.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
"Shape (N_sequences, num_tasks)"
"Shape (N_sequences, num_tasks, 1, 1, 1)"
Mutates every position of the sequence to every letter
"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
Breakdown:
"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
remove wild-type
len(arange) = N_letters * sequence_length
len(horizontal cycle) = N_letters * sequence_length
add mutant
make mutant predictions
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
Best score case
Worst score case
Encode motif
"sequences now has shape (3, 4, 5, 1)"
"sequences now has shape (3, 4, 5, 1)"
Construct and train SequenceDNN model
"X = np.random.rand(10, 1, 4, 50)"
"y = np.random.randint(0, 2, size=(10, 1))"
"dataset = dc.data.NumpyDataset(X, y)"
Call in-silico mutagenesis
Construct and train SequenceDNN model
"X = np.random.rand(10, 1, 4, 50)"
"y = np.random.randint(0, 2, size=(10, 1))"
"dataset = dc.data.NumpyDataset(X, y)"
Call in-silico mutagenesis
Check nonzero elements exist
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
"(X, y, w, ids)"
Sometimes zip files contain directories within. Traverse directories
TODO(rbharath): Add support for more extensions
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset and have it create an Iterator.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
(ytz): this skips everything except possibly the last shard
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
First try using images for X.
Now try using images for y.
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Generate data
Transform it
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Create image file
Create zip of image file
"self.zip_path = ""/home/rbharath/misc/cells.zip"""
Create zip of multiple image files
"Create zip of multiple image files, multiple_types"
Create image directory
These are the known dimensions of face.png
TODO(rbharath): Where are the color channels?
"Since the different files have different shapes, makes an object array"
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
extracting validation set of MNIST for testing the DataTransforms
extract only the images (no need of the labels)
reshaping the vector to image
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check Blurring
Check rotation
Some more test cases for flip
Check flip
Check Scales
Check shift
check gaussian noise
check salt and pepper noise
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are either in .sdf.csv file or in the .sdf file itself
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
TODO: mol should be always sanitized when charges are calculated
can't change it now because it would break a lot of examples
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
TODO(rbharath): Is this squeeze OK?
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
"Results should be sorted by (atom_degree, mol_index)"
"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
Currently handles loading failures by returning None
TODO: Is there a better handling procedure?
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
TODO (VIGS25): Complete the description
Handle loading failures which return None
Fit atomic conv model
Add the Atomic Convolution layers to fetches
Extract the atomic convolution features
Handle loading failures which return None
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other
splits (or potentially refactor those to match this).
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
calculate percent split for valid (out of test and valid)
"split remaining data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Test singletask case.
Should have made an 80/10/10 train/valid/test split of actives.
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
Calculate what the new shape will be.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
Generate the nb_affine weights and biases
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, W_list,"
b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
## Performs computations
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
self.build()
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
Create the variables.
Compute the distances and radial symmetry functions.
check that there isnt just one or zero inputs
create subspaces
create the alpha learnable parameters
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
We do not need the mask because every graph has self.num_vertices vertices now
So the Tensor has known dimensions
Add in features
Add in labels
Add in all layers
The last layer is the output of the model
TODO(rbharath): Add in support for additional
losses.
TODO(rbharath): The TensorGraph can't be built until
fit is called since the shapes of features/labels
not specified. Need to figure out a good restoration
method for this use case.
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the dense layers
Compute the loss function for each label.
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Extract atom_features
Add trainable weights
Extract atom_features
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
Arguments
Aliases.
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
In eager mode we want an optimizer and a function to compute the
gradient of the loss.
In graph mode we want a training operation.
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
instead of using the **kwargs hack.
Gather results for each output
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"If only one output, just return array"
Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
"The next release of Tensorflow will add a proper jacobian() function, so"
we can remove this then.
"Remove extra dimensions, because I couldn't figure out how to get the"
jacobian() function to not produce them.
"In eager mode, we need to execute every layer once to ensure its variables"
have been created.
"We can't execute Input layers in eager mode, since they would try"
to create placeholders.  Instead create a tensor of the correct
size and type.
Build the layers.
Initialize variables.
In graph mode we need to create the computation graph.
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Check the inputs.
Define a function that recursively creates tensors from layers.
Define the model function.
Define the inputs.
"Create the correct outputs, based on the mode."
Create the Estimator.
Add or remove dimensions of size 1 to match the shape of the layer.
Should we keep a separate global step count for each submodel?
Add the input features.
Weight decay not activated
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Predict the output and uncertainty.
Predict the output and uncertainty.
Predict the output and uncertainty.
Fit trained model
Eval model on train
Prepare Training Data
Train the model
Prepare the Testing data
predict
check output shape
new object of UNet to test if loading the model results in same predictions
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
Create a dataset and an input function for processing it.
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"frag2_z = np.random.rand(N_atoms, 3)"
"For simplicity, let's assume both molecules have same number of"
atoms.
Creates a set of dummy features that contain the coordinate and
neighbor-list features required by the AtomicConvModel.
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
arbitrary label
Run a fitting operation
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
"If we don't specify the initial state, it should default to zeros."
Explicitly specifying the zero state should give the same result.
Specifying a different initial state should produce a different result.
We should get the same result with either predict_on_batch() or __call__().
Take a tiny step in the direction of s and see if the output changes by
the expected amount.
Test for correct value return (normal mode)
Test for shapes (normal mode)
Test for correct value return (eager mode)
Test for shape (eager mode)
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Layer is wrapper around embedding lookup, tested that then"
Expected output
Create a dataset with three tasks.  The first two tasks each depend only
on half the features.  The third task depends on all of them.
Create an OntologyModel.  Two leaf nodes contain half the features.
Train the model on the datase.
It should have learned to predict all of the tasks accurately.
"In addition, it should be able to predict the first task based only on the"
"first leaf node, and the second task based only on the second leaf node."
Create a dataset with three tasks.  The first two tasks each depend only
on half the features.  The third task depends on all of them.
Create an OntologyModel.  Two leaf nodes contain half the features.
Train the model on the datase.
It should have learned to predict all of the tasks accurately.
"In addition, it should be able to predict the first task based only on the"
"first leaf node, and the second task based only on the second leaf node."
Here are mappings for just a few yeast genes.
"Build the ontology, then see if it looks correct."
Should be able to call fit twice without failure.
# TODO(rbharath): Transform these into useful weights.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
# TODO(rbharath): Add a test with per-class weighting.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
Prepare Training Data
Train the model
Prepare the Testing data
predict
check output shape
new object of ResNet to test if loading the model results in same predictions
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create the model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate results
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Construct layers for all nodes.
Create the loss function.
Create inputs for the features.
Create inputs for the children.
Concatenate all inputs together.
Create the output.
"If necessary, download the file defining the ontology."
Parse the ontology definition and create a list of terms.
Create OntologyNode objects for all the terms.
"Assign parent-child relationships between nodes, and identify root nodes."
Create a single root node that combines the three GO roots.
Assign features to nodes.
Count the number of features within each node.  Eliminate nodes with too few
features and set the number of outputs for each one.
Create the inputs.
Create the generators.
Create the discriminators.
Make a copy of the discriminator that takes each generator's output as
its input.
Make a list of all layers in the generators and discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
Compute the weighted errors
Add an entropy term to the loss.
Create submodels for training the generators and discriminators.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Returns:
Build placeholders
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def MultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultitaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
Transform SMILES sequence to integers
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
TODO: Turning off queue for now. Safe to re-activate?
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
Featurize qm9 dataset
TODO: Check for this
Download files if they don't exist
Featurize the KINASE dataset
Shuffle the training data
Apply transformations
#### TIMING ######
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
No tasks since no labels provided.
For now images are loaded directly by ImageLoader
Load Sweetlead dataset
Featurize SWEET dataset
Initialize transformers
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize BBBC001 dataset
Featurize Images into NumpyArrays
Load text file with labels
Strip the first line which holds field labels
Format is: Image_name count1 count2
This is kludgy way to add y to dataset. Can be done better?
Featurize BBBC002 dataset
Featurize Images into NumpyArrays
Load text file with labels
Strip the first line which holds field labels
Format is: Image_name count1 count2
This is kludgy way to add y to dataset. Can be done better?
Featurize TOXCAST dataset
Initialize transformers
Download files if they don't exist
Featurizing datasets
Missing entry removal
Shuffle the training data
Apply transformations
#### TIMING ###########
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
TODO: Check if anything needs to be added
Featurize the FACTORS dataset
Shuffle the training data
Apply transformations
######### TIMING ################
Most reaction dataset ML tasks train the prediction of products from
"ractants. Both of these are contained in the rxn object that is output,"
"so there is no ""tasks"" field."
DeepChem currently has no transformers for reaction data
Download USPTO dataset
Unzip
Unzipped file is a tap seperated values file (despite the .txt)
The first element in the row is the reaction smarts
"Sometimes smarts have extraneous information at end of form """
"|f:0"" that causes parsing to fail. Not sure what this information"
"is, but just ignoring for now."
Make up dummy labels since DiskDataset.from_numpy doesn't allow
creation from just features for now.
TODO: This dataset isn't saved to disk so reload doesn't happen.
Featurize hiv dataset
Initialize transformers
Extract locations of data
Extract labels
Skip comment lines
Lines have format
"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
"The base-10 logarithm, -log kd/pk"
Featurize Data
"Pulled from PDB files. For larger datasets with more PDBs, would use"
max num atoms instead of exact.
Cutoff in angstroms
Cutoff in angstroms
Delete labels for failing elements
No transformations of data
TODO(rbharath): This should be modified to contain a cluster split so
structures of the same protein aren't in both train/test
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load sider models now
Load sweetlead dataset now. Pass in dataset object and appropriate
transformers to predict functions
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load QM7 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Batch size of models
Fit models
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating postives and negatives so each batch will have
equal numbers of both.
Create the model to train.  We use a simple fully connected network with
one hidden layer.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load SAMPL(FreeSolv) dataset
Define metric
Batch size of models
Fit trained model
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
plt.show()
"run_benchmark(FILE, DEEPCHEM_DIR)"
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Create duplicate placeholders for meta-optimization.
Create the loss function for meta-optimization.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
"(X, y, w, ids)"
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset and have it create an Iterator.
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
(ytz): this skips everything except possibly the last shard
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
"There is one ""image channel""."
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Generate data
Transform it
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
extracting validation set of MNIST for testing the DataTransforms
extract only the images (no need of the labels)
reshaping the vector to image
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check Blurring
Check rotation
Some more test cases for flip
Check flip
Check Scales
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
The label encoder is given characters for ACGTN
Peak at the first sequence to get the length of the sequence.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
TODO: mol should be always sanitized when charges are calculated
can't change it now because it would break a lot of examples
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
One sequence has length longer than others. This should throw a
ValueError.
Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
define methods to calculate available flat features
all methods (flat and voxel) must have the same API:
"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
define methods to calculate available voxel features
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
some users might try to read smiles with this function
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
Calculate what the new shape will be.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
Generate the nb_affine weights and biases
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, W_list,"
b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
## Performs computations
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
self.build()
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
check that there isnt just one or zero inputs
create subspaces
create the alpha learnable parameters
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
We do not need the mask because every graph has self.num_vertices vertices now
So the Tensor has known dimensions
Add in features
Add in labels
Add in all layers
The last layer is the output of the model
TODO(rbharath): Add in support for additional
losses.
TODO(rbharath): The TensorGraph can't be built until
fit is called since the shapes of features/labels
not specified. Need to figure out a good restoration
method for this use case.
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Tensorflow expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the dense layers
Compute the loss function for each label.
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Extract atom_features
Add trainable weights
Extract atom_features
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
Arguments
Aliases.
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
In eager mode we want an optimizer and a function to compute the
gradient of the loss.
In graph mode we want a training operation.
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
instead of using the **kwargs hack.
Gather results for each output
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"If only one output, just return array"
"In eager mode, we need to execute every layer once to ensure its variables"
have been created.
"We can't execute Input layers in eager mode, since they would try"
to create placeholders.  Instead create a tensor of the correct
size and type.
Build the layers.
Initialize variables.
In graph mode we need to create the computation graph.
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Check the inputs.
Define a function that recursively creates tensors from layers.
Define the model function.
Define the inputs.
"Create the correct outputs, based on the mode."
Create the Estimator.
Add or remove dimensions of size 1 to match the shape of the layer.
Should we keep a separate global step count for each submodel?
Add the input features.
Weight decay not activated
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"But if we specify a different starting state, that should produce a"
different result.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
TODO What should shape[1] be?  It's not documented.
TODO(rbharath): Why is it 2*n_features instead of n_features?
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"TODO What should the output shape be?  It's not documented, and there"
are no other test cases for it.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
"Creating a second layer should produce different results, since it has"
different random weights.
But evaluating the first layer again should produce the same result as before.
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
"If we don't specify the initial state, it should default to zeros."
Explicitly specifying the zero state should give the same result.
Specifying a different initial state should produce a different result.
We should get the same result with either predict_on_batch() or __call__().
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
Should be able to call fit twice without failure.
# TODO(rbharath): Transform these into useful weights.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
# TODO(rbharath): Add a test with per-class weighting.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create the model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
Evaluate the model.
Create a dataset and an input function for processing it.
Create a TensorGraph model.
Create an estimator from it.
Train the model.
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Predict the output and uncertainty.
Predict the output and uncertainty.
Predict the output and uncertainty.
Fit trained model
Eval model on train
Create the inputs.
Create the generators.
Create the discriminators.
Make a copy of the discriminator that takes each generator's output as
its input.
Make a list of all layers in the generators and discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
Compute the weighted errors
Add an entropy term to the loss.
Create submodels for training the generators and discriminators.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
Create a dataset and an input function for processing it.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Returns:
Build placeholders
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
################### Deprecation warnings for renamed TensorGraph models ####################
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def MultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultitaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
Transform SMILES string to integer vectors
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
################### Deprecation warnings for renamed TensorGraph models ####################
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Predict the output and uncertainty.
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultitaskDNN model
Building tensorflow robust MultitaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Load Sweetlead dataset
Featurize SWEET dataset
Initialize transformers
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load SWEET dataset
Featurize SWEET dataset
Initialize transformers
Set some global variables up top
removes directory if present -- warning
default split is 80-10-10 train-valid-test split
Fit Logistic Regression models
Fit Logistic Regression models
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating postives and negatives so each batch will have
equal numbers of both.
Create the model to train.  We use a simple fully connected network with
one hidden layer.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load SAMPL(FreeSolv) dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
References
Arguments
Aliases.
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveLayer(WeaveLayer):
""""""" Alternate implementation of weave module"
"same variables, different graph structures"
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
""
Parameters
----------
x: list
list of Tensors of form described above.
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
A: Tensor
Tensor of atom_features
P: Tensor
Tensor of pair_features
""""""""
# Add trainable weights
self.build()
""
atom_features = x[0]
pair_features = x[1]
""
pair_split = x[2]
atom_to_pair = x[4]
""
"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
AA = self.activation(AA)
"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
PA = self.activation(PA)
"PA = tf.segment_sum(PA, pair_split)"
""
"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
A = self.activation(A)
""
if self.update_pair:
AP_ij = tf.matmul(
tf.reshape(
"tf.gather(atom_features, atom_to_pair),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ij = self.activation(AP_ij)
AP_ji = tf.matmul(
tf.reshape(
"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
AP_ji = self.activation(AP_ji)
""
"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
PP = self.activation(PP)
"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
P = self.activation(P)
else:
P = pair_features
""
"return A, P"
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class WeaveConcat(Layer):
""""""""" Concat a batch of molecules into a batch of atoms"
""""""""
""
"def __init__(self,"
"batch_size,"
"n_atom_input_feat=50,"
"n_output=128,"
"init='glorot_uniform',"
"activation='tanh',"
**kwargs):
""""""""
Parameters
----------
batch_size: int
number of molecules in a batch
"n_atom_input_feat: int, optional"
Number of features for each atom in input.
"n_output: int, optional"
Number of output features for each atom(concatenated)
"init: str, optional"
Weight initialization for filters.
"activation: str, optional"
Activation function applied
""
""""""""
self.batch_size = batch_size
self.n_atom_input_feat = n_atom_input_feat
self.n_output = n_output
self.init = initializations.get(init)  # Set weight initialization
self.activation = activations.get(activation)  # Get activations
"super(WeaveConcat, self).__init__(**kwargs)"
""
def build(self):
"""""""""Construct internal trainable weights."
""""""""
""
"self.W = self.init([self.n_atom_input_feat, self.n_output])"
self.b = model_ops.zeros(shape=[
"self.n_output,"
])
""
self.trainable_weights = self.W + self.b
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_mask]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of concatenated atom features
""""""""
self.build()
atom_features = x[0]
atom_masks = x[1]
"A = tf.split(atom_features, self.batch_size, axis=0)"
A_mask = tf.split(
"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
outputs = tf.concat(
"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
"outputs = tf.matmul(outputs, self.W) + self.b"
outputs = self.activation(outputs)
return outputs
TODO(rbharath): This class does not yet have a
"TensorGraph equivalent, but one may not be required."
"Commented out for now, remove if OK."
class AlternateWeaveGather(WeaveGather):
"""""""Alternate implementation of weave gather layer"
corresponding to AlternateWeaveLayer
""""""""
""
"def call(self, x, mask=None):"
"""""""Execute this layer on input tensors."
""
"x = [atom_features, atom_split]"
""
Parameters
----------
x: list
Tensors as listed above
"mask: bool, optional"
Ignored. Present only to shadow superclass call() method.
""
Returns
-------
outputs: Tensor
Tensor of molecular features
""""""""
# Add trainable weights
self.build()
outputs = x[0]
atom_split = x[1]
""
if self.gaussian_expand:
outputs = self.gaussian_histogram(outputs)
""
"output_molecules = tf.segment_sum(outputs, atom_split)"
""
if self.gaussian_expand:
"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
output_molecules = self.activation(output_molecules)
return output_molecules
Each directory holds a range of assay results
Just write NA
"Now, write out the results csv, going line by line through all molecule results"
printing the mol_id
printing the SMILES
Now gzip it
Now remove the intermediate csv
First download all SDF files. We need these to get smiles
Next download all Bioassays
RDKit consistently hangs when trying to read this file
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
from simdna import simulations
define layer out functions
get layer outputs for a positive simulation example
plot layer outputs
highlight motif sites
get a positive and a negative example from the simulation data
"get motif scores, ISM scores, and DeepLIFT scores"
get motif site locations
organize legends
plot scores and highlight motif site locations
initialize fwd and reverse scores to -infinity
"cross-correlate separately for each base,"
for both the PSSM and its reverse complement
sum over the bases
take max of fwd and reverse scores at each position
return 1D view of sequence characters
class SequenceDNN(Model):
""""""""
Sequence DNN models.
""
Parameters
----------
"seq_length : int, optional"
length of input sequence.
"keras_model : instance of keras.models.Sequential, optional"
seq_length or keras_model must be specified.
"num_tasks : int, optional"
number of tasks. Default: 1.
num_filters : list[int] | tuple[int]
"number of convolutional filters in each layer. Default: (15,)."
conv_width : list[int] | tuple[int]
"width of each layer's convolutional filters. Default: (15,)."
pool_width : int
width of max pooling after the last layer. Default: 35.
L1 : float
strength of L1 penalty.
dropout : float
dropout probability in every convolutional layer. Default: 0.
verbose: int
"Verbosity level during training. Valida values: 0, 1, 2."
""
Returns
-------
Compiled DNN model.
""""""""
""
"def __init__(self,"
"seq_length=None,"
"keras_model=None,"
"use_RNN=False,"
"num_tasks=1,"
"num_filters=(15, 15, 15),"
"conv_width=(15, 15, 15),"
"pool_width=35,"
"GRU_size=35,"
"TDD_size=15,"
"L1=0,"
"dropout=0.0,"
"num_epochs=100,"
verbose=1):
self.num_tasks = num_tasks
self.num_epochs = num_epochs
self.verbose = verbose
self.train_metrics = []
self.valid_metrics = []
if keras_model is not None and seq_length is None:
self.model = keras_model
self.num_tasks = keras_model.layers[-1].output_shape[-1]
elif seq_length is not None and keras_model is None:
self.model = Sequential()
assert len(num_filters) == len(conv_width)
"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
conv_height = 4 if i == 0 else 1
self.model.add(
Convolution2D(
"nb_filter=nb_filter,"
"nb_row=conv_height,"
"nb_col=nb_col,"
"activation='linear',"
"init='he_normal',"
"input_shape=(1, 4, seq_length),"
"W_regularizer=l1(L1),"
b_regularizer=l1(L1)))
self.model.add(Activation('relu'))
self.model.add(Dropout(dropout))
"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
if use_RNN:
num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
"self.model.add(Permute((2, 1)))"
"self.model.add(GRU(GRU_size, return_sequences=True))"
"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
self.model.add(Flatten())
self.model.add(Dense(output_dim=self.num_tasks))
self.model.add(Activation('sigmoid'))
"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
else:
raise ValueError(
"""Exactly one of seq_length or keras_model must be specified!"")"
""
"def train(self,"
"X,"
"y,"
"validation_data,"
"early_stopping_metric='Loss',"
"early_stopping_patience=5,"
save_best_model_to_prefix=None):
if y.dtype != bool:
"assert set(np.unique(y)) == {0, 1}"
y = y.astype(bool)
multitask = y.shape[1] > 1
if not multitask:
num_positives = y.sum()
num_sequences = len(y)
num_negatives = num_sequences - num_positives
if self.verbose >= 1:
print('Training model (* indicates new best result)...')
"X_valid, y_valid = validation_data"
early_stopping_wait = 0
best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
"for epoch in range(1, self.num_epochs + 1):"
self.model.fit(
"X,"
"y,"
"batch_size=128,"
"nb_epoch=1,"
class_weight={
"True: num_sequences / num_positives,"
False: num_sequences / num_negatives
"} if not multitask else None,"
verbose=self.verbose >= 2)
"epoch_train_metrics = self.test(X, y)"
"epoch_valid_metrics = self.test(X_valid, y_valid)"
self.train_metrics.append(epoch_train_metrics)
self.valid_metrics.append(epoch_valid_metrics)
if self.verbose >= 1:
print('Epoch {}:'.format(epoch))
print('Train {}'.format(epoch_train_metrics))
"print('Valid {}'.format(epoch_valid_metrics), end='')"
current_metric = epoch_valid_metrics[early_stopping_metric].mean()
if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
if self.verbose >= 1:
print(' *')
best_metric = current_metric
best_epoch = epoch
early_stopping_wait = 0
if save_best_model_to_prefix is not None:
self.save(save_best_model_to_prefix)
else:
if self.verbose >= 1:
print()
if early_stopping_wait >= early_stopping_patience:
break
early_stopping_wait += 1
if self.verbose >= 1:
print('Finished training after {} epochs.'.format(epoch))
if save_best_model_to_prefix is not None:
"print(""The best model's architecture and weights (from epoch {0}) """
'were saved to {1}.arch.json and {1}.weights.h5'.format(
"best_epoch, save_best_model_to_prefix))"
""
"def predict(self, X):"
"return self.model.predict(X, batch_size=128, verbose=False)"
""
def get_sequence_filters(self):
""""""""
Returns 3D array of 2D sequence filters.
""""""""
return self.model.layers[0].get_weights()[0].squeeze(axis=1)
""
"def deeplift(self, X, batch_size=200):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
""""""""
assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
from deeplift.conversion import keras_conversion as kc
""
# convert to deeplift model and get scoring function
"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
score_func = deeplift_model.get_target_contribs_func(
find_scores_layer_idx=0)
# use a 40% GC reference
"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
# get deeplift scores
"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
for i in range(self.num_tasks):
deeplift_scores[i] = score_func(
"task_idx=i,"
"input_data_list=[X],"
"batch_size=batch_size,"
"progress_update=None,"
input_references_list=input_references)
return deeplift_scores
""
"def in_silico_mutagenesis(self, X):"
""""""""
"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
""""""""
"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
wild_type_predictions = self.predict(X)
"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
np.newaxis]
"for sequence_index, (sequence, wild_type_prediction) in enumerate("
"zip(X, wild_type_predictions)):"
mutated_sequences = np.repeat(
"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
# remove wild-type
arange = np.arange(len(mutated_sequences))
horizontal_cycle = np.tile(
"np.arange(sequence.shape[-1]), sequence.shape[-2])"
"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
# add mutant
vertical_repeat = np.repeat(
"np.arange(sequence.shape[-2]), sequence.shape[-1])"
"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
# make mutant predictions
mutated_predictions = self.predict(mutated_sequences)
mutated_predictions = mutated_predictions.reshape(sequence.shape +
"(self.num_tasks,))"
mutagenesis_scores[
sequence_index] = wild_type_prediction - mutated_predictions
"return np.rollaxis(mutagenesis_scores, -1)"
""
@staticmethod
"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
from dragonn.plot import plot_bases_on_ax
scores = score_func(X).squeeze(
"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
try:
os.makedirs(output_directory)
except OSError:
pass
num_tasks = len(scores)
"for task_index, task_scores in enumerate(scores):"
"for sequence_index, sequence_scores in enumerate(task_scores):"
# sequence_scores is num_bases x sequence_length
basewise_max_sequence_scores = sequence_scores.max(axis=0)
plt.clf()
"figure, (top_axis, bottom_axis) = plt.subplots(2)"
top_axis.plot(
"range(1,"
"len(basewise_max_sequence_scores) + 1),"
basewise_max_sequence_scores)
top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
peak_position = basewise_max_sequence_scores.argmax()
top_axis.axvspan(
"peak_position - peak_width,"
"peak_position + peak_width,"
"color='grey',"
alpha=0.1)
"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
peak_position + peak_width].T
# Set non-max letter_heights to zero
letter_heights = np.zeros_like(peak_sequence_scores)
"letter_heights[np.arange(len(letter_heights)),"
peak_sequence_scores.argmax(axis=1)] = \
basewise_max_sequence_scores[peak_position - peak_width :
peak_position + peak_width]
"plot_bases_on_ax(letter_heights, bottom_axis)"
bottom_axis.set_xticklabels(
tuple(
"map(str,"
"np.arange(peak_position - peak_width,"
peak_position + peak_width + 1))))
"bottom_axis.tick_params(axis='x', labelsize='small')"
plt.xlabel('Position')
plt.ylabel('Score')
plt.savefig(
"os.path.join(output_directory, 'sequence_{}{}'.format("
"sequence_index, '_task_{}'.format(task_index)"
if num_tasks > 1 else '')))
plt.close()
""
"def plot_deeplift(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.deeplift,"
score_name='DeepLift')
""
"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
self._plot_scores(
"X,"
"output_directory,"
"peak_width,"
"score_func=self.in_silico_mutagenesis,"
score_name='ISM')
""
"def plot_architecture(self, output_file):"
from dragonn.visualize_util import plot as plot_keras_model
"plot_keras_model(self.model, output_file, show_shape=True)"
""
"def save(self, save_best_model_to_prefix):"
arch_fname = save_best_model_to_prefix + '.arch.json'
weights_fname = save_best_model_to_prefix + '.weights.h5'
"open(arch_fname, 'w').write(self.model.to_json())"
"self.model.save_weights(weights_fname, overwrite=True)"
""
@staticmethod
"def load(arch_fname, weights_fname=None):"
model_json_string = open(arch_fname).read()
sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
if weights_fname is not None:
sequence_dnn.model.load_weights(weights_fname)
return sequence_dnn
create temporary fasta files
run command
remove fasta files
write test fasta file
test gkmsvm
get classification results
This SDF file fails to parse with RDKit on Ubuntu 16.04
"Using canonical smiles for glycine, as in original research paper"
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
Add the input features.
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
plt.show()
"run_benchmark(FILE, DEEPCHEM_DIR)"
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Create duplicate placeholders for meta-optimization.
Create the loss function for meta-optimization.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
TODO(rbharath): This has been a major source of bugs. Is there a more
robust characterization of which metrics require class-probs and which
don't?
Reshape to handle 1-d edge cases
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
"(X, y, w, ids)"
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
Retrieve the first sample so we can determine the dtypes.
Create a Tensorflow Dataset and have it create an Iterator.
The -1 indicates that y will be reshaped to have length -1
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
(ytz): this skips everything except possibly the last shard
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Protect against generator exhaustion
This ensures tasks are consistent for all datasets
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
example.fasta contains 3 sequences each of length 58.
The one-hot encoding turns base-pairs into vectors of length 4.
"There is one ""image channel"")"
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
approx 1/15! chance of equality
Generate data
Generate data
Generate data
Transform it
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Check that we have all the data in
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
depends on Python version
The label encoder is given characters for ACGTN
"These are transformed in 0, 1, 2, 3, 4 in input sequence"
TODO(rbharath): Unlike the DRAGONN implementation from which this
"was ported, I couldn't transform the ""ACGT..."" strings into"
integers all at once. Had to do one at a time. Might be worth
figuring out what's going on under the hood.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
TODO: mol should be always sanitized when charges are calculated
can't change it now because it would break a lot of examples
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
One sequence has length longer than others. This should throw a
value error.
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
define methods to calculate available flat features
all methods (flat and voxel) must have the same API:
"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
define methods to calculate available voxel features
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
If uses use_chirality
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
some users might try to read smiles with this function
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"H(x), with same number of input and output channels"
"T(x), with same number of input and output channels"
Calculate what the new shape will be.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, self.W_list,"
self.b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
self.build()
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
check that there isnt just one or zero inputs
create subspaces
create the alpha learnable parameters
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
We do not need the mask because every graph has self.num_vertices vertices now
So the Tensor has known dimensions
Calling fit() for first time
Add in features
Add in labels
Add in all layers
The last layer is the output of the model
TODO(rbharath): Add in support for additional
losses.
TODO(rbharath): The TensorGraph can't be built until
fit is called since the shapes of features/labels
not specified. Need to figure out a good restoration
method for this use case.
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
"Results is of shape (n_samples, n_tasks, n_classes)"
"Results is of shape (n_samples, n_tasks)"
Add the input features.
Add the shared dense layers
Add task-specific bypass layers
Add the input features.
Add the dense layers
Compute the loss function for each label.
"Results is of shape (n_samples, n_tasks, n_classes)"
"retval is of shape (n_samples, n_tasks)"
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Similarity values
Labels for all top K similar samples
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Extract atom_features
Add trainable weights
Extract atom_features
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
Arguments
Aliases.
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
Gather results for each output
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
"If only one output, just return array"
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Should we keep a separate global step count for each submodel?
Add the input features.
Weight decay not activated
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
"Results is of shape (n_samples, n_tasks, 1)"
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
See if it has done a plausible job of learning the distribution.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
Should be able to call fit twice without failure.
# TODO(rbharath): Transform these into useful weights.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
# TODO(rbharath): Add a test with per-class weighting.
#class_weight={
"#    True: num_sequences / num_positives,"
#    False: num_sequences / num_negatives
"#} if not multitask else None,"
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Test it out.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Create the inputs.
Create the generators.
Create the discriminators.
Make a copy of the discriminator that takes each generator's output as
its input.
Make a list of all layers in the generators and discriminators.
Compute the loss functions.
Create learnable weights for the generators and discriminators.
Compute the weighted errors
Add an entropy term to the loss.
Create submodels for training the generators and discriminators.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Gather results for each output
Recording the number of samples in the input batch
GraphConvTensorGraph constantly outputs batch_size number of
"results, only valid samples should be appended to final results"
"If only one output, just return array"
Returns:
Concatenates along 0-th dimension
Returns:
Build placeholders
w_b act as the indicator of unique samples in the batch
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
MPNN only accept padded input
MPNN only accept padded input
Extract number of unique samples in the batch from w_b
Only fetch the first set of unique samples
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def MultiTaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultiTaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
TODO(rbharath): This model only supports one-conv layer. Extend
so that conv layers of greater depth can be implemented.
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
Transform SMILES string to integer vectors
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Building tensorflow MultiTaskDNN model
Building tensorflow robust MultiTaskDNN model
Building scikit logistic regression model
Transform fingerprints to IRV features
Building tensorflow IRV model
Building scikit random forest model
Building scikit learn Kernel SVM model
Building xgboost classification model
Remove token for paddings
Building scikit random forest model
Building scikit learn Kernel Ridge Regression model
Building scikit learn Kernel Ridge Regression model
Building xgboost regression model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
"Record the actions, converting to one-hot if necessary."
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Run the algorithm.
Save a file checkpoint.
Build the tree.
Compute the final probabilities and expected reward.
Mark this node as terminal
Expand this node.
Select the next action to perform.
Recursively build the tree.
Update statistics for this node.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
The state consists of two numbers: a current value and a target value.
The policy just needs to learn to output the target value (or at least
move toward it).
A simple policy with no hidden layers.
Optimize it.
Try running it and see if it reaches the target
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Evaluate performances with different training set fraction
Datasets and models used in the benchmark test
Uncomment the two lines below if hyper_parameters are provided
"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
hyper_parameters = pickle.load(f)
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load SWEET dataset
Featurize SWEET dataset
Initialize transformers
Set some global variables up top
removes directory if present -- warning
default split is 80-10-10 train-valid-test split
Fit Logistic Regression models
Fit Logistic Regression models
##Load data###
##Create model###
Use R2 classification metric
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Featurize qm9 dataset
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Featurize Tox21 dataset
Initialize transformers
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm7 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm8 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Load dataset
Featurize ChEMBL dataset
Initialize transformers
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Featurize Delaney dataset
Initialize transformers
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating postives and negatives so each batch will have
equal numbers of both.
Create the model to train.  We use a simple fully connected network with
one hidden layer.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
Note sensitivity = recall
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Featurize hiv dataset
Initialize transformers
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Featurize SAMPL dataset
Initialize transformers
Load clintox dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Load PDBBind dataset
Define featurizers
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
Featurize Dataset
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Featurize HOPV dataset
Initialize transformers
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Create duplicate placeholders for meta-optimization.
Create the loss function for meta-optimization.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
References
Arguments
Aliases.
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Store the summed atoms by degree
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
TODO(rbharath): It's not clear where nb_affine comes from.
Is there a solid explanation here?
Generate the nb_affine weights and biases
Add trainable weights
Extract atom_features
Extract graph topology
Perform the mol conv
Extract nodes and membership
Extract atom_features
Extract graph topology
Perform the mol gather
Extract nodes
Extract atom_features
Extract graph topology
Perform the mol gather
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
r = self.r_init
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new aattention states
"def build(self, input_shape):"
"_, support_input_shape = input_shape  #Unpack"
n_feat = support_input_shape[1]
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Not sure if it helps to place the update here or later yet.  Will
decide
z = r
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
"return [x+p, z+q]"
No other forget biases supported right now.
"def build(self, input_shape):"
Taken from Keras code [citation needed]
###################################################### DEBUG
"return o, [h, c]"
###################################################### DEBUG
"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
distance_hidden = self.activation(distance_hidden)
atom_features_hidden = self.activation(atom_features_hidden)
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
Extract atom_features
Basic features of every atom: (batch_size*max_atoms) * n_atom_features
calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
"step i calculates the graph features for atoms of index `parents[:,i,0]`"
target atoms for each step: (batch_size*max_atoms) * max_atoms
"represent the same atoms of `parents[:, :, 0]`,"
different in that these index are positions in `atom_features`
"number of atoms in total, should equal `batch_size*max_atoms`"
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
last step generates graph features for all target atom
Add trainable weights
Extract atom_features
sum all graph outputs
Aliases.
TODO(rbharath): What does this line do?
TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
This dictionary holds a mapping {graph: learning_phase}.
A learning phase is a bool tensor used to run Keras models in
either train mode (learning_phase == 1) or test mode (learning_phase == 0).
else: assume learning phase is a placeholder tensor.
need broadcasting
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
These properties should have been set
"by the child class, as appropriate."
These properties should be set by the user via keyword arguments.
"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
are only applicable to input layers: do not pass these keywords
to non-input layers.
In this case we will create an input layer
to insert before the current layer
Update self.losses
In case self.losses isn't settable
(i.e. it's a getter method).
In that case the `losses` property is
auto-computed and shouldn't be set.
Update self._per_input_updates
Updates indexed by None are unconditional
rather than input-dependent
outputs = to_list(self.call(x))
return outputs
TODO(rbharath): Keras uses a global var here to maintain
unique counts. This seems dangerous. How does tensorflow handle?
TODO(rbharath): Support this type of functional API.
If batch size not specified
Input shape
Output shape
References
Not Trainable
Not Trainable
need broadcasting
pick the normalized form of x corresponding to the training phase
sample-wise normalization
from deepchem.nn.model_ops import variable
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Arguments
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Add trainable weights
Add trainable weights
Add trainable weights
Add trainable weights
"Output should be of shape (?, nb_filter)"
"Output should be of shape (batch_size, n_feat)"
Try concatenating the two lists of placeholders
Try concatenating the two lists of placeholders
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
TODO: add pi_stack and cation_pi to feature_types (it's not trivial
because they require sanitized molecules)
"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""],"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
TODO(rbharath): This has been a major source of bugs. Is there a more
robust characterization of which metrics require class-probs and which
don't?
Reshape to handle 1-d edge cases
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
The -1 indicates that y will be reshaped to have length -1
"Set labels to be zero, with zero weights"
Load obsolete format -> save in new format
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"(ytz): Depending on the application, thread-based pools may be faster"
"than process based pools, since process based pools need to pickle/serialize"
"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
we're actually protected by the GIL.
(ytz): this skips everything except possibly the last shard
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
Generate data
Generate data
Generate data
Transform it
Transform it
special case to test
deterministic
non-deterministic
we don't know the order in which the shards are iterated in.
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
TODO: mol should be always sanitized when charges are calculated
can't change it now because it would break a lot of examples
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
if ring is aromatic
"save its indices, center, and normal"
remember protein-ligand pairs we already counted
"if this pair is new, count atoms forming a contact"
"if this pair is new, count atoms forming a contact"
if ring from mol1 is aromatic
...and atom from mol2 is a cation
if angle and distance are correct
count atoms forming a contact
find interacting rings from protein and cations from ligand
find interacting cations from protein and rings from ligand
merge counters
TODO(LESWING)
check if user tries to set removed arguments
list of features that require sanitized molecules
not implemented featurization types
default values
update with cutoffs specified by the user
define methods to calculate available flat features
all methods (flat and voxel) must have the same API:
"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
define methods to calculate available voxel features
"each entry is a tuple (is_flat, feature_name)"
list of features that cannot be calculated with specified parameters
this list is used to define <flat/voxel/all>_combined subset
parse provided feature types
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
"Since ConvMol is an object and not a numpy array, need to set dtype to"
object.
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
some users might try to read smiles with this function
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
check if correct distance metric was used
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
simple flat ring
load and sanitize two real molecules
FIXME might break with different version of rdkit
FIXME might break with different version of rdkit
parallel normals
perpendicular normals
too far away
perpendicular normals
parallel normals
too far away
order of the molecules shouldn't matter
with this criteria we should find both types of stacking
parallel normals
perpendicular normals
too far away
"TODO find better example, currently dicts are empty"
"TODO find better example, currently dicts are empty"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
test if default parameters work
check if use-case from examples works
test if input is flattened when flat features are used
test voxel features
test flat features
check if aromatic features are ignores if sanitize=False
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
Path to save checkpoint files
first layer in model: check that it is an input layer
Add losses to graph
Loss for each batch element
Loss should be a float
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
TODO(rbharath): Don't support example weighting yet.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Arguments
Returns
Arguments
Returns
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"H(x), with same number of input and output channels"
"T(x), with same number of input and output channels"
Calculate what the new shape will be.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, self.W_list,"
self.b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
self.build()
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
check that there isnt just one or zero inputs
create subspaces
create the alpha learnable parameters
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
We do not need the mask because every graph has self.num_vertices vertices now
So the Tensor has known dimensions
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Extract atom_features
Add trainable weights
Extract atom_features
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
Gather results for each output
"If only one output, just return array"
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Create the inputs.
Create the generator.
Create the discriminator.
Make a copy of the discriminator that takes the generator's output as
its input.
Make a list of all layers in the generator and discriminator.
Create submodels for training the generator and discriminator.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Gather results for each output
Recording the number of samples in the input batch
GraphConvTensorGraph constantly outputs batch_size number of
"results, only valid samples should be appended to final results"
"If only one output, just return array"
Returns:
Concatenates along 0-th dimension
Returns:
Build placeholders
w_b act as the indicator of unique samples in the batch
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
MPNN only accept padded input
MPNN only accept padded input
Extract number of unique samples in the batch from w_b
Only fetch the first set of unique samples
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def tensorGraphMultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultiTaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
Transform SMILES string to integer vectors
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply an attention lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .85
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply a residual lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Generate dummy dataset
Fit trained model
Eval model on train
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
-*- coding: utf-8 -*-
"due to the different shape of weight(ndims=2) and bias(ndims=1),"
will using this version for logreg
exclude bias variables
setting up n_tasks nodes(output nodes)
label placeholders with size batch_size * 1
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
using self-defined regularization
adding output nodes of sigmoid function
"fix the size to be [?,1]"
Dummy placeholders
Dummy placeholders
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Add the input features.
Add the dense layers
Compute the loss function for each label.
"Results is of shape (n_samples, n_tasks, n_classes)"
"retval is of shape (n_samples, n_tasks)"
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Dummy placeholders
Dummy placeholders
Dummy placeholders
Dummy placeholders
Run fit transformers on dummy dataset to determine n_features after transformation
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building scikit learn Kernel SVM model
Loading hyper parameters
Building xgboost classification model
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Initialize model folder
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Remove token for paddings
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building scikit learn Kernel Ridge Regression model
Loading hyper parameters
Building scikit learn Kernel Ridge Regression model
Loading hyper parameters
Building xgboost classification model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
Will raise a CalledProcessError if fails.
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load SWEET dataset
Featurize SWEET dataset
Initialize transformers
Set some global variables up top
removes directory if present -- warning
default split is 80-10-10 train-valid-test split
Fit Logistic Regression models
Fit Logistic Regression models
##Load data###
##Create model###
Use R2 classification metric
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
Featurize qm9 dataset
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load tox21 dataset
Fit models
Fit trained model
Featurize Tox21 dataset
Initialize transformers
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm7 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Load QM8 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm8 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Load dataset
Featurize ChEMBL dataset
Initialize transformers
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
Set to higher values to get better numbers
======================================================================
"Run Benchmarks {GC-DNN, SVR, RF}"
!/usr/bin/env python2
-*- coding: utf-8 -*-
Only for debug!
Load Delaney dataset
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
!/usr/bin/env python2
-*- coding: utf-8 -*-
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Featurize Delaney dataset
Initialize transformers
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Load the data.
"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
sparse: most tasks do not include data for most molecules.  It also is very
"unbalanced: there are many more negatives than positives.  For each task,"
create a list of alternating postives and negatives so each batch will have
equal numbers of both.
Create the model to train.  We use a simple fully connected network with
one hidden layer.
Define a MetaLearner describing the learning problem.
Run meta-learning on 80% of the tasks.
Validate on the remaining tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
replace with your own scratch directory
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
"'ani_gdb_s03.h5',"
"'ani_gdb_s04.h5',"
"'ani_gdb_s05.h5',"
"'ani_gdb_s06.h5',"
"'ani_gdb_s07.h5',"
'ani_gdb_s08.h5'
Extract the data
Print the data
self-interaction energies taken from
https://github.com/isayev/ANI1_dataset README
flush once more at the end
"# For production, set nb_epoch to 100+"
"print(""Train scores"")"
print(train_scores)
"print(""Minimization of a single test set structure:"")"
"print(model.minimize_structure(coords, atomic_nums))"
Written by Roman Zubatyuk and Justin S. Smith
Modified by Yutong Zhao to make python2 compatible
opening file
print(store_loc)
print(type(v[0]))
print(k)
print(path)
Number of conformations in each file increases exponentially.
Start with a smaller dataset before continuing. Use all of them
for production
Extract the data
Note sensitivity = recall
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Featurize hiv dataset
Initialize transformers
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Featurize SAMPL dataset
Initialize transformers
Load clintox dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Load PDBBind dataset
Define featurizers
"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
reduce validation performance
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
Featurize Dataset
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Featurize HOPV dataset
Initialize transformers
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
"Copied from the yt_project, commit e8fb57e"
yt/doc/extensions/notebook_sphinxext.py
https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
Almost completely re-written by Matthew Harrigan to use nbconvert v4
1. Uneval notebook
2. Python
3. HTML (execute first)
Set per-cell timeout to 60 seconds
4. Eval'd notebook
Create link to notebook and script files
create notebook node
add dependency
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
Perform recursive modification to set css mime types.
Perform recursive modification to set js mime types.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
Record inputs.
Create the output directory if necessary.
Create duplicate placeholders for meta-optimization.
Create the loss function for meta-optimization.
"In the final loss, use different placeholders for all inputs so the loss will be"
computed from a different batch.
Create variables for accumulating the gradients.
Create the optimizers for meta-optimization and task optimization.
Main optimization loop.
Do checkpointing.
This is a MetaLearner that learns to generate sine functions with variable
amplitude and phase.
Optimize it.
Test it out on some new tasks and see how it works.
Initially the model should do a bad job of fitting the sine function.
After one step of optimization it should do much better.
"Verify that we can create a new MAML object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
References
Arguments
Aliases.
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Store the summed atoms by degree
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
TODO(rbharath): It's not clear where nb_affine comes from.
Is there a solid explanation here?
Generate the nb_affine weights and biases
Add trainable weights
Extract atom_features
Extract graph topology
Perform the mol conv
Extract nodes and membership
Extract atom_features
Extract graph topology
Perform the mol gather
Extract nodes
Extract atom_features
Extract graph topology
Perform the mol gather
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
r = self.r_init
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new aattention states
"def build(self, input_shape):"
"_, support_input_shape = input_shape  #Unpack"
n_feat = support_input_shape[1]
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Not sure if it helps to place the update here or later yet.  Will
decide
z = r
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
"return [x+p, z+q]"
No other forget biases supported right now.
"def build(self, input_shape):"
Taken from Keras code [citation needed]
###################################################### DEBUG
"return o, [h, c]"
###################################################### DEBUG
"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
distance_hidden = self.activation(distance_hidden)
atom_features_hidden = self.activation(atom_features_hidden)
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
Extract atom_features
Basic features of every atom: (batch_size*max_atoms) * n_atom_features
calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
"step i calculates the graph features for atoms of index `parents[:,i,0]`"
target atoms for each step: (batch_size*max_atoms) * max_atoms
"represent the same atoms of `parents[:, :, 0]`,"
different in that these index are positions in `atom_features`
"number of atoms in total, should equal `batch_size*max_atoms`"
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
last step generates graph features for all target atom
Add trainable weights
Extract atom_features
sum all graph outputs
Aliases.
TODO(rbharath): What does this line do?
TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
This dictionary holds a mapping {graph: learning_phase}.
A learning phase is a bool tensor used to run Keras models in
either train mode (learning_phase == 1) or test mode (learning_phase == 0).
else: assume learning phase is a placeholder tensor.
need broadcasting
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
These properties should have been set
"by the child class, as appropriate."
These properties should be set by the user via keyword arguments.
"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
are only applicable to input layers: do not pass these keywords
to non-input layers.
In this case we will create an input layer
to insert before the current layer
Update self.losses
In case self.losses isn't settable
(i.e. it's a getter method).
In that case the `losses` property is
auto-computed and shouldn't be set.
Update self._per_input_updates
Updates indexed by None are unconditional
rather than input-dependent
outputs = to_list(self.call(x))
return outputs
TODO(rbharath): Keras uses a global var here to maintain
unique counts. This seems dangerous. How does tensorflow handle?
TODO(rbharath): Support this type of functional API.
If batch size not specified
Input shape
Output shape
References
Not Trainable
Not Trainable
need broadcasting
pick the normalized form of x corresponding to the training phase
sample-wise normalization
from deepchem.nn.model_ops import variable
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Arguments
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Add trainable weights
Add trainable weights
Add trainable weights
Add trainable weights
"Output should be of shape (?, nb_filter)"
"Output should be of shape (batch_size, n_feat)"
Try concatenating the two lists of placeholders
Try concatenating the two lists of placeholders
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
causes segfaults.
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
TODO(rbharath): This has been a major source of bugs. Is there a more
robust characterization of which metrics require class-probs and which
don't?
Reshape to handle 1-d edge cases
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
Only the first set of copy will be counted in training loss
The -1 indicates that y will be reshaped to have length -1
"Set labels to be zero, with zero weights"
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
Handle edge case.
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
Generate data
Generate data
Generate data
Transform it
Transform it
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
"Get the reversed shape of z: (..., n_tasks, batch_size)"
Find the task dimension of z
Prevent broadcasting on wrong dimension
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
TODO(LESWING)
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
Euclidean distance between atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
"Since ConvMol is an object and not a numpy array, need to set dtype to"
object.
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
"Distance is either graph distance(True) or Euclidean distance(False,"
only support datasets providing Cartesian coordinates)
Set dtype
If includes explicit hydrogens
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
TODO test more formats for ligand
some users might try to read smiles with this function
adding hydrogens and charges is tested in dc.utils
3D vector with unit length
"very basic test, we check if rotations actually work in test_rotate_molecules"
check if distances do not change
check if it works for molecules with different numbers of atoms
"random coords between 0 and 1, so the max possible distance in sqrt(2)"
TODO test if dict contains smiles
check if results are the same if we provide precomputed distances
...but first check if we actually got two dicts
check if we get less features with smaller distance cutoff
ligands are typically small so all atoms might be present
check if using different ecfp_degree changes anything
TODO upperbound?
"20 points with coords between -5 and 5, centered at 0"
indices are positive
coordinates were properly translated and scaled
for coordinates outside of the box function should properly transform them
to indices and warn the user
"TODO check if function warns. There is assertWarns method in unittest,"
but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
"20 points with coords between -5 and 5, centered at 0"
3 pairs of indices
"protein is too big for the box, some features should be missing"
whole ligand should fit in the box
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
dict is needed in case groups aren't strictly flattened or
hashed by something non-integer like
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
0 1 2 3 4 5 6 7 8 9
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
Path to save checkpoint files
first layer in model: check that it is an input layer
Add losses to graph
Loss for each batch element
Loss should be a float
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
TODO(rbharath): Don't support example weighting yet.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Arguments
Returns
Arguments
Returns
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
optimization to allow for tensorcontraction/broadcasted mmul
using a reshape trick. Note that the np and tf matmul behavior
differs when dealing with broadcasts
-*- coding: UTF-8 -*-
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"H(x), with same number of input and output channels"
"T(x), with same number of input and output channels"
Calculate what the new shape will be.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, self.W_list,"
self.b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
No other forget biases supported right now.
Taken from Keras code [citation needed]
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new attention states
Support set lstm
Test lstm
self.build()
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
check that there isnt just one or zero inputs
create subspaces
create the alpha learnable parameters
"concatenate subspaces, reshape to size of original input, then stack"
"such that out_tensor has shape (2,?,original_cols)"
creates subspaces the same way it was done in AlphaShare
calculate squared Frobenius norm
"(TODO YTZ:) faster, less memory intensive way"
"r = tf.reduce_sum(tf.square(coordinates), 2)"
"r = tf.expand_dims(r, -1)"
"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
"d = r - inner + tf.transpose(r, perm=[0,2,1])"
d = tf.nn.relu(d) # fix numerical instabilities about diagonal
d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
Calculate pairwise distance
Cutoff with threshold Rc
return d
tf.stack issues again...
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
We do not need the mask because every graph has self.num_vertices vertices now
So the Tensor has known dimensions
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
"Default message function: edge network, update function: GRU"
more options to be implemented
Extract atom_features
Add trainable weights
Extract atom_features
Add another value(~-Inf) to prevent error in softmax
Model using this layer must set pad_batches=True
Perform one step of LSTM
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
"Don't let this thread get ahead of the enqueue thread, since if"
"we try to read more batches than the total number that get queued,"
this thread will hang indefinitely.
Gather results for each output
"If only one output, just return array"
Ensure all training operators have been created.
Initialize variables.
"As a sanity check, make sure all tensors have the correct shape."
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
The loss doesn't depend on any variables.
Set by variable constructor.
Set by set_variable_initial_values().
Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
Optimize the main loss.  This should send both variables toward 1.
Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
See if it has done a plausible job of learning the distribution.
We have to set the gradient penalty very small because the generator's
"output is only a single number, so the default penalty would constrain"
it far too much.
See if it has done a plausible job of learning the distribution.
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
Train the model on random sequences.  We aren't training long enough to
"really make it reliable, but I want to keep this test fast, and it should"
still be able to reproduce a reasonable fraction of input sequences.
Test it out.
Check that it got at least a quarter of them correct.
Actually training a VAE takes far too long for a unit test.  Just run a
"few steps of training to make sure nothing crashes, then check that the"
results are at least internally consistent.
use central difference since forward difference has a pretty high
approximation error
assert min_coords[1][0] != new_x[3]
assert min_coords[1][1] != new_x[4]
assert min_coords[1][2] != new_x[5]
Create the inputs.
Create the generator.
Create the discriminator.
Make a copy of the discriminator that takes the generator's output as
its input.
Make a list of all layers in the generator and discriminator.
Create submodels for training the generator and discriminator.
"Every call to fit_generator() will increment global_step, but we only"
"want it to get incremented once for the entire batch, so record the"
value and keep resetting it.
Train the discriminator.
Train the generator.
Write checkpoints and report progress.
Write out final results.
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
Gather results for each output
Recording the number of samples in the input batch
GraphConvTensorGraph constantly outputs batch_size number of
"results, only valid samples should be appended to final results"
"If only one output, just return array"
Returns:
Concatenates along 0-th dimension
Returns:
Build placeholders
w_b act as the indicator of unique samples in the batch
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
MPNN only accept padded input
MPNN only accept padded input
Extract number of unique samples in the batch from w_b
Only fetch the first set of unique samples
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def tensorGraphMultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultiTaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
(ytz): this is really dirty but needed for restoring models
"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
SMILES strings
Maximum length is expanded to allow length variation during train and inference
'_' served as delimiter and padding
Initialize common characters as keys
Include space to avoid extra keys
"For 'Cl', 'Br', etc."
"Character not recognized, add to extra_keys"
Add all extra_keys to char_dict
Character embedding
Multiple convolutional layers with different filter widths
Max-over-time pooling
Concat features from all filters(one feature per filter)
Highway layer from https://arxiv.org/pdf/1505.00387.pdf
Transform SMILES string to integer vectors
Skip all spaces
"For 'Cl', 'Br', etc."
Padding with '_'
Do a simple greedy search.
Do a beam search with length normalization.
"Represent each candidate as (normalized prob, raw prob, sequence)"
This candidate sequence has already been terminated
Consider all possible tokens we could add to this candidate sequence.
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply an attention lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .85
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply a residual lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Generate dummy dataset
Fit trained model
Eval model on train
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
-*- coding: utf-8 -*-
"due to the different shape of weight(ndims=2) and bias(ndims=1),"
will using this version for logreg
exclude bias variables
setting up n_tasks nodes(output nodes)
label placeholders with size batch_size * 1
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
using self-defined regularization
adding output nodes of sigmoid function
"fix the size to be [?,1]"
Dummy placeholders
Dummy placeholders
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Add the input features.
Add the dense layers
Compute the loss function for each label.
"Results is of shape (n_samples, n_tasks, n_classes)"
"retval is of shape (n_samples, n_tasks)"
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Dummy placeholders
Dummy placeholders
Dummy placeholders
Dummy placeholders
Run fit transformers on dummy dataset to determine n_features after transformation
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
gpu memory growth option
gpu memory growth option
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
Keep track of the layers
"For graphical layers, add connectivity placeholders"
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building scikit learn Kernel SVM model
Loading hyper parameters
Building xgboost classification model
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Initialize model folder
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Remove token for paddings
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building scikit learn Kernel Ridge Regression model
Loading hyper parameters
Building scikit learn Kernel Ridge Regression model
Loading hyper parameters
Building xgboost classification model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
"Float or int hyper parameters(ex. batch_size, learning_rate)"
List of float or int hyper parameters(ex. layer_sizes)
Number of parameters
Range of optimization
Dummy names
Input hyper parameters
Run benchmark
Record hyperparameters
Record performances
"GPGO maximize performance by default, set performance to its negative value for minimization"
Readout best hyper parameters
Compare best model to default hyperparameters
Record hyperparameters
Record performances
"Optimized model is better, return hyperparameters"
Return default hyperparameters
!/usr/bin/env python2
-*- coding: utf-8 -*-
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Have the worker threads generate the rollouts for this iteration.
Perform optimization.
Build the feed dict and run the optimizer.
Update the number of steps taken so far and perform checkpointing.
Merge all the rollouts into a single set of arrays.
Iterate slices.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Return the processed arrays.
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the discounted rewards and advantages.
Convert the actions to one-hot.
Rearrange the states into the proper set of arrays.
Build the feed dict and apply gradients.
Assume all arrays are float32.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new PPO object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
The environment is a plane in which the agent moves by steps until it reaches a randomly
positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
"to learn by standard methods, since it may take a very long time to receive any feedback"
at all.  Using hindsight makes it much easier.
A simple policy with two hidden layers.
Optimize it.
Try running it a few times and see if it succeeds.
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Assigning featurizer
Some exceptions in datasets
loading datasets
time_finish_loading-time_start is the time(s) used for dataset loading
dataset has customized features
running model
Initialize metrics
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Evaluating tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Evaluating tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Evaluating tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Evaluating tensorflow IRV model
Initialize model folder
Loading hyper parameters
Building graph convolution model
Gather Projection
Fit trained model
Evaluating graph convolution model
Loading hyper parameters
Building scikit random forest model
Evaluating scikit random forest model
Loading hyper parameters
Building xgboost classification model
Evaluating xgboost classification model
Initialize metrics
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Evaluating tensorflow MultiTaskDNN model
Initialize model folder
Loading hyper parameters
Building graph convoluwtion model
Gather Projection
Fit trained model
Evaluating graph convolution model
Loading hyper parameters
Building scikit random forest model
Evaluating scikit random forest model
Loading hyper parameters
Building xgboost classification model
Evaluating xgboost classification model
Global variables
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
input hyperparameters
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
-*- coding: utf-8 -*-
main layer
bypass layer
penalty
general figure
learning rate
for graph-conv and random forest
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load SWEET dataset
Featurize SWEET dataset
Initialize transformers
Set some global variables up top
removes directory if present -- warning
default split is 80-10-10 train-valid-test split
Fit Logistic Regression models
Fit Logistic Regression models
##Load data###
##Create model###
Use R2 classification metric
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
"Images are square 28x28 (batch, height, width, channel)"
Featurize qm9 dataset
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Fit trained model
Featurize Tox21 dataset
Initialize transformers
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm7 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Load Tox21 dataset
Batch size of models
Fit models
Fit trained model
Featurize qm8 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Load dataset
Featurize ChEMBL dataset
Initialize transformers
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
======================================================================
"Run Benchmarks {GC-DNN, P-DNN, SVR, RF}"
we cant re-open the closed session...
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Featurize Delaney dataset
Initialize transformers
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Note sensitivity = recall
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Featurize hiv dataset
Initialize transformers
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Featurize permeability dataset
Load Tox21 dataset
Fit trained model
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Featurize SAMPL dataset
Initialize transformers
Load clintox dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Load PDBBind dataset
Define featurizers
"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
reduce validation performance
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
Featurize Dataset
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Featurize HOPV dataset
Initialize transformers
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
setup optimizer
setup optimizer
"print(""tasK: %d"" %task)"
"cores = torch.cat([scores, 1.-scores], dim=1)"
"print(""scores"")"
print(scores.size())
"print(""task_label"")"
print(task_label.size())
"task_loss =  self.criterion(scores, task_label)"
"print(""task_loss"")"
print(task_loss.size())
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
x = F.selu( fc(x) )
x = F.selu( fc(x) )
2017 DeepCrystal Technologies - Patrick Hop
""
Data loading a splitting file
""
MIT License - have fun!!
===========================================================
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
########################################################### DEBUG
########################################################### DEBUG
s3cmd -M -H sync docs/_build/ s3://deepchem.io/
########################################################### DEBUG
########################################################### DEBUG
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
References
Arguments
Aliases.
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Store the summed atoms by degree
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
TODO(rbharath): It's not clear where nb_affine comes from.
Is there a solid explanation here?
Generate the nb_affine weights and biases
Add trainable weights
Extract atom_features
Extract graph topology
Perform the mol conv
Extract nodes and membership
Extract atom_features
Extract graph topology
Perform the mol gather
Extract nodes
Extract atom_features
Extract graph topology
Perform the mol gather
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
r = self.r_init
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new aattention states
"def build(self, input_shape):"
"_, support_input_shape = input_shape  #Unpack"
n_feat = support_input_shape[1]
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Not sure if it helps to place the update here or later yet.  Will
decide
z = r
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
"return [x+p, z+q]"
No other forget biases supported right now.
"def build(self, input_shape):"
Taken from Keras code [citation needed]
###################################################### DEBUG
"return o, [h, c]"
###################################################### DEBUG
"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
distance_hidden = self.activation(distance_hidden)
atom_features_hidden = self.activation(atom_features_hidden)
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
Extract atom_features
Basic features of every atom: (batch_size*max_atoms) * n_atom_features
calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
"step i calculates the graph features for atoms of index `parents[:,i,0]`"
target atoms for each step: (batch_size*max_atoms) * max_atoms
"represent the same atoms of `parents[:, :, 0]`,"
different in that these index are positions in `atom_features`
"number of atoms in total, should equal `batch_size*max_atoms`"
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
last step generates graph features for all target atom
Add trainable weights
Extract atom_features
sum all graph outputs
Aliases.
TODO(rbharath): What does this line do?
TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
This dictionary holds a mapping {graph: learning_phase}.
A learning phase is a bool tensor used to run Keras models in
either train mode (learning_phase == 1) or test mode (learning_phase == 0).
else: assume learning phase is a placeholder tensor.
need broadcasting
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
These properties should have been set
"by the child class, as appropriate."
These properties should be set by the user via keyword arguments.
"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
are only applicable to input layers: do not pass these keywords
to non-input layers.
In this case we will create an input layer
to insert before the current layer
Update self.losses
In case self.losses isn't settable
(i.e. it's a getter method).
In that case the `losses` property is
auto-computed and shouldn't be set.
Update self._per_input_updates
Updates indexed by None are unconditional
rather than input-dependent
outputs = to_list(self.call(x))
return outputs
TODO(rbharath): Keras uses a global var here to maintain
unique counts. This seems dangerous. How does tensorflow handle?
TODO(rbharath): Support this type of functional API.
If batch size not specified
Input shape
Output shape
References
Not Trainable
Not Trainable
need broadcasting
pick the normalized form of x corresponding to the training phase
sample-wise normalization
from deepchem.nn.model_ops import variable
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Arguments
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Add trainable weights
Add trainable weights
Add trainable weights
Add trainable weights
def test_batch_normalization(self):
"""""""Tests that batch normalization layers can be created."""""""
"Output should be of shape (?, nb_filter)"
"Output should be of shape (batch_size, n_feat)"
Try concatenating the two lists of placeholders
Try concatenating the two lists of placeholders
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
causes segfaults.
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
TODO(rbharath): This has been a major source of bugs. Is there a more
robust characterization of which metrics require class-probs and which
don't?
Reshape to handle 1-d edge cases
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
The -1 indicates that y will be reshaped to have length -1
"Set labels to be zero, with zero weights"
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
Handle edge case.
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
Generate data
Generate data
Generate data
Transform it
Transform it
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
TODO(LESWING)
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
TODO(enf): make array index checking not a try-catch statement.
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
"Since ConvMol is an object and not a numpy array, need to set dtype to"
object.
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
Set dtype
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
"Note there is a central nitrogen of degree 4, with 4 carbons"
of degree 1 (connected only to central nitrogen).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central nitrogen
Central nitrogen connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Pick the mol closest to everything as the first element of training
Pick the closest mol from what is left
Test is everything else
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
Path to save checkpoint files
first layer in model: check that it is an input layer
Add losses to graph
Loss for each batch element
Loss should be a float
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
TODO(rbharath): Don't support example weighting yet.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Arguments
Returns
Arguments
Returns
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Calculate pairwise distance
Masking for valid atom index
Cutoff with threshold Rc
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
Reshape everything to match the input with the most dimensions.
"This probably means the variable hasn't been created yet, so try again"
with reuse set to false.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, self.W_list,"
self.b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def tensorGraphMultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultiTaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
!/usr/bin/env python2
-*- coding: utf-8 -*-
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply an attention lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .85
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply a residual lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Generate dummy dataset
Fit trained model
Eval model on train
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
"""""""Ops for graph construction."""""""
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals
""
import sys
import traceback
import tensorflow as tf
from keras import backend as K
""
"def cosine_distances(test, support):"
"""""""Computes pairwise cosine distances between provided tensors"
""
Parameters
----------
test: tf.Tensor
"Of shape (n_test, n_feat)"
support: tf.Tensor
"Of shape (n_support, n_feat)"
""
Returns
-------
tf.Tensor:
"Of shape (n_test, n_support)"
""""""""
"rnorm_test = tf.rsqrt(tf.reduce_sum(tf.square(test), 1,"
keep_dims=True)) + K.epsilon()
"rnorm_support = tf.rsqrt(tf.reduce_sum(tf.square(support), 1,"
keep_dims=True)) + K.epsilon()
test_normalized = test * rnorm_test
support_normalized = support * rnorm_support
""
# Transpose for mul
"support_normalized_t = tf.transpose(support_normalized, perm=[1,0])"
"g = tf.matmul(test_normalized, support_normalized_t)  # Gram matrix"
return g
""
"def euclidean_distance(test, support, max_dist_sq=20):"
"""""""Computes pairwise euclidean distances between provided tensors"
""
TODO(rbharath): BROKEN! THIS DOESN'T WORK!
""
Parameters
----------
test: tf.Tensor
"Of shape (n_test, n_feat)"
support: tf.Tensor
"Of shape (n_support, n_feat)"
"max_dist_sq: float, optional"
Maximum pairwise distance allowed.
""
Returns
-------
tf.Tensor:
"Of shape (n_test, n_support)"
""""""""
"test = tf.expand_dims(test, 1)"
"support = tf.expand_dims(support, 0)"
"g = -tf.maximum(tf.reduce_sum(tf.square(test - support), 2), max_dist_sq)"
return g
""
"def add_bias(tensor, init=None, name=None):"
"""""""Add a bias term to a tensor."
""
Parameters
----------
tensor: tf.Tensor
Variable tensor.
init: float
Bias initializer. Defaults to zero.
name: str
Name for this op. Defaults to tensor.op.name.
""
Returns
-------
tf.Tensor
A biased tensor with the same shape as the input tensor.
""""""""
if init is None:
init = tf.zeros([tensor.get_shape()[-1].value])
"with tf.name_scope(name, tensor.op.name, [tensor]):"
"b = tf.Variable(init, name='b')"
"return tf.nn.bias_add(tensor, b)"
""
""
"def dropout(tensor, dropout_prob, training=True, training_only=True):"
"""""""Random dropout."
""
"This implementation supports ""always-on"" dropout (training_only=False), which"
"can be used to calculate model uncertainty. See Gal and Ghahramani,"
http://arxiv.org/abs/1506.02142.
""
"NOTE(user): To simplify the implementation, I have chosen not to reverse"
the scaling that occurs in tf.nn.dropout when using dropout during
inference. This shouldn't be an issue since the activations will be scaled
by the same constant in both training and inference. This means that there
are no training-time differences between networks that use dropout during
inference and those that do not.
""
Parameters
----------
tensor: tf.Tensor
Input tensor.
dropout_prob: float
Float giving dropout probability for weights (NOT keep probability).
training_only: bool
"Boolean. If True (standard dropout), apply dropout only"
"during training. If False, apply dropout during inference as well."
""
Returns
-------
tf.Tensor:
A tensor with the same shape as the input tensor.
""""""""
if not dropout_prob:
return tensor  # do nothing
keep_prob = 1.0 - dropout_prob
if training or not training_only:
"tensor = tf.nn.dropout(tensor, keep_prob)"
return tensor
""
""
"def fully_connected_layer(tensor, size=None, weight_init=None, bias_init=None,"
name=None):
"""""""Fully connected layer."
""
Parameters
----------
tensor: tf.Tensor
Input tensor.
size: int
Number of output nodes for this layer.
weight_init: float
Weight initializer.
bias_init: float
Bias initializer.
name: str
Name for this op. Defaults to 'fully_connected'.
""
Returns
-------
tf.Tensor:
A new tensor representing the output of the fully connected layer.
""
Raises
------
ValueError
If input tensor is not 2D.
""""""""
if len(tensor.get_shape()) != 2:
"raise ValueError('Dense layer input must be 2D, not %dD'"
% len(tensor.get_shape()))
if weight_init is None:
num_features = tensor.get_shape()[-1].value
"weight_init = tf.truncated_normal([num_features, size], stddev=0.01)"
if bias_init is None:
bias_init = tf.zeros([size])
""
"with tf.name_scope(name, 'fully_connected', [tensor]):"
"w = tf.Variable(weight_init, name='w', dtype=tf.float32)"
"b = tf.Variable(bias_init, name='b', dtype=tf.float32)"
"return tf.nn.xw_plus_b(tensor, w, b)"
""
"def weight_decay(penalty_type, penalty):"
"""""""Add weight decay."
""
Args:
model: TensorflowGraph.
""
Returns:
A scalar tensor containing the weight decay cost.
""
Raises:
NotImplementedError: If an unsupported penalty type is requested.
""""""""
variables = []
# exclude bias variables
for v in tf.trainable_variables():
if v.get_shape().ndims == 2:
variables.append(v)
""
with tf.name_scope('weight_decay'):
if penalty_type == 'l1':
cost = tf.add_n([tf.reduce_sum(tf.abs(v)) for v in variables])
elif penalty_type == 'l2':
cost = tf.add_n([tf.nn.l2_loss(v) for v in variables])
else:
raise NotImplementedError('Unsupported penalty_type %s' % penalty_type)
cost *= penalty
"tf.scalar_summary('Weight Decay Cost', cost)"
return cost
""
""
"def multitask_logits(features, num_tasks, num_classes=2, weight_init=None,"
"bias_init=None, dropout_prob=None, name=None):"
"""""""Create a logit tensor for each classification task."
""
Args:
features: A 2D tensor with dimensions batch_size x num_features.
num_tasks: Number of classification tasks.
num_classes: Number of classes for each task.
weight_init: Weight initializer.
bias_init: Bias initializer.
dropout_prob: Float giving dropout probability for weights (NOT keep
probability).
name: Name for this op. Defaults to 'multitask_logits'.
""
Returns:
A list of logit tensors; one for each classification task.
""""""""
logits_list = []
with tf.name_scope('multitask_logits'):
for task_idx in range(num_tasks):
"with tf.name_scope(name,"
"('task' + str(task_idx).zfill(len(str(num_tasks)))), [features]):"
logits_list.append(
"logits(features, num_classes, weight_init=weight_init,"
"bias_init=bias_init, dropout_prob=dropout_prob))"
return logits_list
""
""
"def logits(features, num_classes=2, weight_init=None, bias_init=None,"
"dropout_prob=None, name=None):"
"""""""Create a logits tensor for a single classification task."
""
You almost certainly don't want dropout on there -- it's like randomly setting
the (unscaled) probability of a target class to 0.5.
""
Args:
features: A 2D tensor with dimensions batch_size x num_features.
num_classes: Number of classes for each task.
weight_init: Weight initializer.
bias_init: Bias initializer.
dropout_prob: Float giving dropout probability for weights (NOT keep
probability).
name: Name for this op.
""
Returns:
A logits tensor with shape batch_size x num_classes.
""""""""
"with tf.name_scope(name, 'logits', [features]) as name:"
return dropout(
"fully_connected_layer(features, num_classes, weight_init=weight_init,"
"bias_init=bias_init, name=name),"
dropout_prob)
""
""
"def softmax_N(tensor, name=None):"
"""""""Apply softmax across last dimension of a tensor."
""
Args:
tensor: Input tensor.
"name: Name for this op. If None, defaults to 'softmax_N'."
""
Returns:
A tensor with softmax-normalized values on the last dimension.
""""""""
"with tf.name_scope(name, 'softmax_N', [tensor]):"
exp_tensor = tf.exp(tensor)
reduction_indices = [tensor.get_shape().ndims - 1]
"return tf.div(exp_tensor,"
"tf.reduce_sum(exp_tensor,"
"reduction_indices=reduction_indices,"
keep_dims=True))
""
"def optimizer(optimizer=""adam"", learning_rate=.001, momentum=.9):"
"""""""Create model optimizer."
""
Parameters
----------
"optimizer: str, optional"
Name of optimizer
"learning_rate: float, optional"
Learning rate for algorithm
"momentum: float, optional"
Momentum rate
""
Returns
-------
A training Optimizer.
""
Raises:
NotImplementedError: If an unsupported optimizer is requested.
""""""""
# TODO(user): gradient clipping (see Minimize)
if optimizer == 'adagrad':
train_op = tf.train.AdagradOptimizer(learning_rate)
elif optimizer == 'adam':
train_op = tf.train.AdamOptimizer(learning_rate)
elif optimizer == 'momentum':
"train_op = tf.train.MomentumOptimizer(learning_rate,"
momentum)
elif optimizer == 'rmsprop':
"train_op = tf.train.RMSPropOptimizer(learning_rate,"
momentum)
elif optimizer == 'sgd':
train_op = tf.train.GradientDescentOptimizer(learning_rate)
else:
raise NotImplementedError('Unsupported optimizer %s' % optimizer)
return train_op
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
-*- coding: utf-8 -*-
"due to the different shape of weight(ndims=2) and bias(ndims=1),"
will using this version for logreg
exclude bias variables
setting up n_tasks nodes(output nodes)
label placeholders with size batch_size * 1
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
using self-defined regularization
adding output nodes of sigmoid function
"fix the size to be [?,1]"
Dummy placeholders
Dummy placeholders
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Add the input features.
Add the dense layers
Compute the loss function for each label.
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Dummy placeholders
Dummy placeholders
Dummy placeholders
Dummy placeholders
Run fit transformers on dummy dataset to determine n_features after transformation
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like model_dir/model.ckpt-N
"self._save_path is ""model_dir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
Keep track of the layers
############################################ DEBUG
"print(""start - add()"")"
"print(""self.output"")"
print(self.output)
############################################ DEBUG
"For graphical layers, add connectivity placeholders"
############################################ DEBUG
"print(""end- add()"")"
"print(""self.output"")"
print(self.output)
############################################ DEBUG
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
Create target inputs
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
############################################################ DEBUG
self.feat_dim = self.model.get_num_output_features()
############################################################ DEBUG
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building xgboost classification model
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Initialize model folder
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building xgboost classification model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
Generate the rollout.
Compute an estimate of the reward for the rest of the episode.
Compute the output arrays.
This is modeled after the Roulette-v0 environment from OpenAI Gym.
"The player can bet on any number from 0 to 36, or walk away (which ends the"
"game).  The average reward for any bet is slightly negative, so the best"
strategy is to walk away.
"This policy just learns a constant probability for each action, and a constant for the value."
Optimize it.
"It should have learned that the expected value is very close to zero, and that the best"
action is to walk away.
"Verify that we can create a new A3C object, reload the parameters from the first one, and"
get the same result.
"Do the same thing, only using the ""restore"" argument to fit()."
The environment just has a constant state.
The policy includes a single recurrent layer.
"We don't care about actually optimizing it, so just run a few rollouts to make"
"sure fit() doesn't crash, then check the behavior of the GRU state."
"On the first call, the initial state should be all zeros."
It should still be zeros since we didn't save it last time.
It should be different now.
This should be the same as the previous one.
"Now we reset it, so we should get the same result as initially."
Randomize who goes first
Illegal move -- the square is not empty
Move X
Did X Win
Did O Win
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
!/usr/bin/env python3
-*- coding: utf-8 -*-
Assigning featurizer
Some exceptions in datasets
loading datasets
time_finish_loading-time_start is the time(s) used for dataset loading
dataset has customized features
running model
Initialize metrics
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Evaluating tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Evaluating tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Evaluating tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Evaluating tensorflow IRV model
Initialize model folder
Loading hyper parameters
Building graph convolution model
Gather Projection
Fit trained model
Evaluating graph convolution model
Loading hyper parameters
Building scikit random forest model
Evaluating scikit random forest model
Loading hyper parameters
Building xgboost classification model
Evaluating xgboost classification model
Initialize metrics
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Evaluating tensorflow MultiTaskDNN model
Initialize model folder
Loading hyper parameters
Building graph convoluwtion model
Gather Projection
Fit trained model
Evaluating graph convolution model
Loading hyper parameters
Building scikit random forest model
Evaluating scikit random forest model
Loading hyper parameters
Building xgboost classification model
Evaluating xgboost classification model
Global variables
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
input hyperparameters
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
!/usr/bin/env python3
-*- coding: utf-8 -*-
Datasets and models used in the benchmark test
"irv, rf, rf_regression should be assigned manually"
-*- coding: utf-8 -*-
main layer
bypass layer
penalty
general figure
learning rate
for graph-conv and random forest
Set numpy seed
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Featurize Kinase dataset
##Load data###
num_trials = 5
##Create model###
Use R2 classification metric
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
##Load data###
num_trials = 5
Set some global variables up top
Fit trained model
Featurize PCBA dataset
Initialize transformers
Fit trained model
Load SWEET dataset
Featurize SWEET dataset
Initialize transformers
Set some global variables up top
removes directory if present -- warning
default split is 80-10-10 train-valid-test split
Fit Logistic Regression models
Fit Logistic Regression models
##Load data###
##Create model###
Use R2 classification metric
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
##Load data###
Use R2 classification metric
##Load data###
##Create model###
##Load data###
"n_estimators=100, max_features=int(num_features/3),"
##Load data###
##Create model###
Use R2 classification metric
"Images are square 28x28 (batch, height, width, channel)"
Featurize qm9 dataset
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load tox21 dataset
Fit models
Fit trained model
Featurize Tox21 dataset
Initialize transformers
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Number of features on conv-mols
Batch size of models
Fit trained model
Load tox21 dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Tox21 dataset
Fit models
Fit trained model
Featurize FACTORS dataset
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
##Create model###
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Featurize qm7 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
Fit trained model
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Featurize qm8 dataset
Load Tox21 dataset
Fit models
Batch size of models
Fit trained model
Fit trained model
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Load dataset
Featurize ChEMBL dataset
Initialize transformers
Load ChEMBL dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
DeepCrystal Technologies 2017 - Patrick Hop
MIT License - have fun!!
======================================================================
"Run Benchmarks {GC-DNN, P-DNN, SVR, RF}"
we cant re-open the closed session...
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Fit trained model
Load Delaney dataset
Fit models
Batch size of models
"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load Delaney dataset
Fit models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load Delaney dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Featurize Delaney dataset
Initialize transformers
Load Delaney dataset
Fit models
Batch size of models
Fit trained model
Only for debug!
Load Delaney dataset
Fit models
Fit trained model
Load MUV dataset
Fit models
Fit trained model
Evaluate train/test scores
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV data
Build model
Fit trained model
Evaluate train/test scores
Extract active site
Featurize ligand
Default for CircularFingerprint
Featurize pocket
Note broadcast operation
Compute labels for pockets
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Filter if missing PDB files
Load PDBBind dataset
Define featurizers
Featurize Dataset
########################################################## DEBUG
########################################################## DEBUG
For stable runs
Fit trained model
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Number of features on conv-mols
Batch size of models
Fit trained model
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
4-fold splits
num positive/negative ligands
10 trials on test-set
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Number of folds for split
Depth of attention module
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply a residual lstm layer
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Set some global variables up top
Featurize Tox21 dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Number of folds for split
Depth of attention module
number positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
Apply an attention lstm layer
4-fold splits
10 positive/negative ligands
10 trials on test-set
Sample supports without replacement (all pos/neg should be different)
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
"print(""Score on task %s is %s"" % (str(task), str(score)))"
Join information for all tasks.
Number of folds for split
num positive/negative ligands
Set batch sizes for network
Number of features on conv-mols
Define metric
Train support model on train
Add layers
4-fold splits
num positive/negative ligands
Define metric
Get supports on test-set
Compute accuracies
Train model on support
Test model
Join information for all tasks.
Note sensitivity = recall
NOTE THE RENAMING:
Note sensitivity = recall
Load nci dataset
Featurize nci dataset
Initialize transformers
Set some global variables up top
Fit trained model
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Featurize hiv dataset
Initialize transformers
Only for debug!
Load hiv dataset
Fit models
Fit trained model
Fit trained model
Fit models
Batch size of models
"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
Fit trained model
Load SIDER dataset
Featurize SIDER dataset
Initialize transformers
Only for debug!
Load SAMPL dataset
Fit models
Fit trained model
Load Tox21 dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Dense post-processing layer
Fit trained model
Featurize SAMPL dataset
Initialize transformers
Load clintox dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Only for debug!
Load clintox dataset
Fit models
Fit trained model
Load clintox dataset
Fit models
Do setup required for tf/keras models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
-*- coding: utf-8 -*-
#############################################################################
## save dataset
#############################################################################
## load datasets
load sweetfda
load aact
## fixup smiles for matching
return smiles
map original smiles to converted smiles
"## join dataframes, index on smiles"
map original smiles back
## fill all nan with 0
## construct datasets
store in new datasets
## save datasets
"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_singletask.csv"""
"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
"fout = ""aacttox_sweetfda_phase_multitask.csv"""
"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
For stable runs
Fit trained model
For stable runs
Fit trained model
Some complexes have labels but no PDB files. Filter these manually
Some of the ligand-names are of form (FMN ox). Use regex
to merge into form (FMN-ox)
Load PDBBind dataset
Define featurizers
"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
reduce validation performance
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
Featurize Dataset
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
transformers = [
"dc.trans.LogTransformer(transform_X=True),"
"dc.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Featurize UV dataset
##Load data###
Use R2 classification metric
##Load data###
##Create model###
Use R2 classification metric
##Load data###
##Create model###
Set numpy seed
##Load data###
##Create model###
Use R2 classification metric
"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
Only use for final evaluation
Force matplotlib to not use any Xwindows backend.
##Load data###
the histogram of the data
##Load data###
###################################################### DEBUG
###################################################### DEBUG
Load HOPV dataset
Fit models
Number of features on conv-mols
Batch size of models
Gather Projection
Fit trained model
Featurize HOPV dataset
Initialize transformers
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Only for debug!
Load HOPV dataset
Fit models
Fit trained model
Load TOXCAST dataset
Featurize TOXCAST dataset
Initialize transformers
Fit trained model
Processing of ToxCast data
Author - Aneesh Pappu
Loading dataframes and editing indices
Loop through rows of hitc matrix and replace codes with smiles strings
get corresponding casn
get corresponding smiles
write to cell
Tidy up and write to csv
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
-*- coding: utf-8 -*-
Save hyperparameters
weight decay
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
############################################################# TIMING
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
2017 DeepCrystal Technologies - Patrick Hop
""
Message Passing Neural Network for Chemical Multigraphs
""
MIT License - have fun!!
===========================================================
A = {}
"valid_bonds = {'SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC'}"
for valid_bond in valid_bonds:
"A[valid_bond] = nn.Linear(75, 75)"
"GRU = nn.GRU(150, 75, 1)"
"flow_delta = Variable(torch.zeros(1, 1))"
"h_t = Variable(torch.zeros(1, 1, 75))"
bond_type = e_vw.GetBondType()
A_vw = A[str(e_vw.GetBondType())]
"gru_act, h_t = GRU(catted.view(1, 1, 150), h_t)"
measure convergence
pdist = nn.PairwiseDistance(2)
"flow_delta = flow_delta + torch.sum(pdist(gru_act.view(1, 75), h[v]))"
"h[v] = gru_act.view(1, 75)"
"print '    flow delta [%i] [%f]' % (k, flow_delta.data.numpy()[0])"
training loop
"{'params': A['DOUBLE'].parameters()},"
"{'params': A['TRIPLE'].parameters()},"
"{'params': A['AROMATIC'].parameters()},"
"{'params': GRU.parameters()},"
"rf.fit(np_fps_train, train_labels)"
labels = rf.predict(val_fps)
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
Set random seeds
Setup directories
Model constants
Load and transform datasets
convert -logKi to dG = +RTlogKi [kJ/mol]
Atomic convolution variables
at = atomic numbers (atom types)
"radial basis function parameters [cutoff, mean, width]"
Model hyperparameters
Initialize model
Fit model
Evaluate model
test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
test_scores = test_evaluator.compute_model_performance(metric)
"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
param.update(test_scores)
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
for transformer in transformers:
train_dataset = transformer.transform(train_dataset)
test_dataset = transformer.transform(test_dataset)
"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
Create some directories for analysis
The base_dir holds the results of all analysis
Make directories to store the raw and featurized datasets.
Load PDBBind dataset
Define featurizers
Currently featurizes with shard_size=1
Dataset can be reshard: dataset = dataset.reshard(48) for example
This could be done with openbabel in python
Compute cells for this molecule. O(constant)
min == max if molecule is planar in some direction
we should still create a bin
TODO(JSG): Implement non-PBC version.  For now this seems fine ..
Note neighbors contains self!
Associate each atom with cell it belongs to. O(N)
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"For each atom, loop through all atoms in its cell and neighboring cells."
Accept as neighbors only those within threshold. This computation should be
"O(Nm), where m is the number of atoms within a set of neighboring-cells."
Sort neighbors by distance
Pick up to max_num_neighbors
Type of data created by this featurizer
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
Sort from largest to smallest scaffold sets
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Dummy placeholders
Dummy placeholders
## AtomicNet fully-connected layer ops ###
## Atomicnet coordinate transform ops ###
## Atomicnet symmetry function kernel ops ###
## Atomicnet symmetry function ops ###
## Atomcnet symmetry function layer ops ###
We apply the radial pooling filter before atom type conv
to reduce computation
## Misc convenience ops ###
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
Higher is Better
The secret key is available as a secure environment variable
on travis-ci to push the build documentation to Amazon S3.
########################################################### DEBUG
########################################################### DEBUG
s3cmd -M -H sync docs/_build/ s3://deepchem.io/
########################################################### DEBUG
########################################################### DEBUG
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
References
Arguments
Aliases.
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Store the summed atoms by degree
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
TODO(rbharath): It's not clear where nb_affine comes from.
Is there a solid explanation here?
Generate the nb_affine weights and biases
Add trainable weights
Extract atom_features
Extract graph topology
Perform the mol conv
Extract nodes and membership
Extract atom_features
Extract graph topology
Perform the mol gather
Extract nodes
Extract atom_features
Extract graph topology
Perform the mol gather
"x is test set, xp is support set."
# Initializes trainable weights.
## Performs computations
Get initializations
r = self.r_init
Process using attention
"Eqn (4), appendix A.1 of Matching Networks paper"
Generate new aattention states
"def build(self, input_shape):"
"_, support_input_shape = input_shape  #Unpack"
n_feat = support_input_shape[1]
Support set lstm
Test lstm
Get initializations
Rename support
Process support xp using attention
Get linear combination of support set
Not sure if it helps to place the update here or later yet.  Will
decide
z = r
Process test x using attention
Generate new support attention states
Generate new test attention states
Redefine
"return [x+p, z+q]"
No other forget biases supported right now.
"def build(self, input_shape):"
Taken from Keras code [citation needed]
###################################################### DEBUG
"return o, [h, c]"
###################################################### DEBUG
"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
distance_hidden = self.activation(distance_hidden)
atom_features_hidden = self.activation(atom_features_hidden)
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
Extract atom_features
Basic features of every atom: (batch_size*max_atoms) * n_atom_features
calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
"step i calculates the graph features for atoms of index `parents[:,i,0]`"
target atoms for each step: (batch_size*max_atoms) * max_atoms
"represent the same atoms of `parents[:, :, 0]`,"
different in that these index are positions in `atom_features`
"number of atoms in total, should equal `batch_size*max_atoms`"
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
last step generates graph features for all target atom
Add trainable weights
Extract atom_features
sum all graph outputs
Aliases.
TODO(rbharath): What does this line do?
TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
This dictionary holds a mapping {graph: learning_phase}.
A learning phase is a bool tensor used to run Keras models in
either train mode (learning_phase == 1) or test mode (learning_phase == 0).
else: assume learning phase is a placeholder tensor.
need broadcasting
ensure that randomness is conditioned by the Numpy RNG
ensure that randomness is conditioned by the Numpy RNG
TODO(rbharath): Should probably swap this over to tf mode.
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
scale preds so that the class probas of each sample sum to 1
manual computation of crossentropy
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
if our output includes timesteps we need to reshape
Arguments
Returns
Note: tf.nn.softmax_cross_entropy_with_logits
"expects logits, Keras expects probabilities."
transform back to logits
"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
a tensor. Confusing with tf.zeros...
Transpose for mul
exclude bias variables
"tf.scalar_summary('Weight Decay Cost', cost)"
TODO(user): gradient clipping (see Minimize)
These properties should have been set
"by the child class, as appropriate."
These properties should be set by the user via keyword arguments.
"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
are only applicable to input layers: do not pass these keywords
to non-input layers.
In this case we will create an input layer
to insert before the current layer
Update self.losses
In case self.losses isn't settable
(i.e. it's a getter method).
In that case the `losses` property is
auto-computed and shouldn't be set.
Update self._per_input_updates
Updates indexed by None are unconditional
rather than input-dependent
outputs = to_list(self.call(x))
return outputs
TODO(rbharath): Keras uses a global var here to maintain
unique counts. This seems dangerous. How does tensorflow handle?
TODO(rbharath): Support this type of functional API.
If batch size not specified
Input shape
Output shape
References
Not Trainable
Not Trainable
need broadcasting
pick the normalized form of x corresponding to the training phase
sample-wise normalization
from deepchem.nn.model_ops import variable
Assuming convolution kernels (2D or 3D).
"TF kernel shape: (..., input_depth, depth)"
No specific assumptions.
References
References
References
References
Pick the one with the correct shape.
Arguments
Aliases.
!/usr/bin/env python2
-*- coding: utf-8 -*-
Add trainable weights
Add trainable weights
Add trainable weights
Add trainable weights
def test_batch_normalization(self):
"""""""Tests that batch normalization layers can be created."""""""
"Output should be of shape (?, nb_filter)"
"Output should be of shape (batch_size, n_feat)"
Try concatenating the two lists of placeholders
Try concatenating the two lists of placeholders
Fit model on dataset
Fit model on dataset
"Should be an array of size (n_pocket_atoms, 3)"
"coords[triangle, 0] gives the x-dimension of all triangle points"
Take transpose to make sure rows correspond to atoms.
We voxelize so all grids have integral coordinates (convenience)
"If overlap of box with previously generated output boxes, return"
Carry forward mappings
We know that box has at least one atom not in outputs
Current box has been merged into box further down list.
No need to output current box
"protein_coords is (N, 3) tensor"
Load binding pocket model
TODO(rbharath): Shift refined to full once trained.
Fit model on dataset
Create featurizers
"if not ligand_file.endswith("".sdf""):"
"raise ValueError(""Only .sdf ligand files can be featurized."")"
"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
ligand_mol2 = os.path.join(
"self.base_dir, ligand_basename + "".mol2"")"
""
# Write mol2 file for ligand
obConversion = ob.OBConversion()
"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
ob_mol = ob.OBMol()
"obConversion.ReadFile(ob_mol, str(ligand_file))"
"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
""
# Featurize ligand
"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
if mol is None:
"return None, None"
# Default for CircularFingerprint
n_ligand_features = 1024
ligand_features = self.ligand_featurizer.featurize([mol])
""
# Featurize pocket
"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
"protein_file, ligand_file)"
n_pockets = len(pockets)
n_pocket_features = BindingPocketFeaturizer.n_features
""
"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
pocket_features = self.pocket_featurizer.featurize(
"protein_file, pockets, pocket_atoms_map, pocket_coords)"
# Note broadcast operation
"features[:, :n_pocket_features] = pocket_features"
"features[:, n_pocket_features:] = ligand_features"
dataset = NumpyDataset(X=features)
pocket_preds = self.model.predict(dataset)
pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
""
# Find pockets which are active
active_pockets = []
active_pocket_atoms_map = {}
active_pocket_coords = []
for pocket_ind in range(len(pockets)):
#################################################### DEBUG
"# TODO(rbharath): For now, using a weak cutoff. Fix later."
#if pocket_preds[pocket_ind] == 1:
if pocket_pred_proba[pocket_ind][1] > .15:
#################################################### DEBUG
pocket = pockets[pocket_ind]
active_pockets.append(pocket)
active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
active_pocket_coords.append(pocket_coords[pocket_ind])
"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
# TODO(LESWING)
"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
causes segfaults.
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
TODO(rbharath): May want to move this file to S3 so we can ensure it's
always available.
Prepare receptor
Get protein centroid and range
"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
going to be extremely slow!
TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
first pocket.
Prepare receptor
TODO(rbharath): Generalize this so can support mol2 files as well.
Write Vina conf file
Define locations of log and output files
TODO(rbharath): Let user specify the number of poses required.
TODO(rbharath): Convert the output pdbqt to a pdb file.
Return docked files
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Check returned files exist
Note this may download autodock Vina...
Note this may download autodock Vina...
Note this may download autodock Vina...
Check returned files exist
Note this may download autodock Vina...
Check returned files exist
"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
box1 contained in box2
"box1 in box2, so complete overlap"
"4/5 atoms in box2 in box1, so 80 % overlap"
box2 contains box1
box1 contains box2
"box1 contains box2, box3"
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Test that every atom in pocket maps exists
Check that the atoms is actually in protein
Add active site to dict
The convention used is that the first task is the metric.
"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
"an option in the Metric class. Instead, this should be possible to move into"
user-space as a custom task_averager function.
"TODO(rbharath, joegomes): What is this magic number?"
"If there are no nonzero examples, metric is ill-defined."
TODO(rbharath): This has been a major source of bugs. Is there a more
robust characterization of which metrics require class-probs and which
don't?
Reshape to handle 1-d edge cases
ids = df[id_field].values
Set missing data to have weight zero
TODO (ytz) this is a bandage solution to reorder the atoms so
that they're always in the same canonical order. Presumably this
should be correctly implemented in the future for graph mols.
Featurize task results iff they exist.
Filter out examples where featurization failed.
"For prospective data where results are unknown, it makes"
no sense to have y values or weights.
Remove support indices
Remove support indices
Remove support indices
Get task specific entries
Now just get weights for this task
Get task specific entries
Now just get weights for this task
Now just get weights for this task
Now just get weights for this task
Split data into pos and neg lists.
No replacement allowed for supports
Handle one-d vs. non one-d feature matrices
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
Init the iterator
Set initial iterator state
support = self.supports[task][self.trial_num]
Increment and update logic
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
"By invariant of when this is called, can assume num_samples > 0"
and num_samples < batch_size
Fill in batch arrays
The -1 indicates that y will be reshaped to have length -1
"Set labels to be zero, with zero weights"
note that this corresponds to the _construct_metadata column order
if not len(self.metadata_df):
"raise ValueError(""No data in dataset."")"
return next(self.metadata_df.iterrows())[1]['task_names']
Create temp directory to store resharded version
Write data in new shards
Handle spillover from last shard
These columns may be missing is the dataset is unlabelled.
"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
Handle edge case.
if data_dir is None:
data_dir = tempfile.mkdtemp()
The -1 indicates that y will be reshaped to have length -1
"raw_data = (X, y, w, ids)"
Get full dataset in memory
Shuffle in memory
Write shuffled shards out to disk
Shuffle the arrays corresponding to each row in metadata_df
TODO (ytz): Under what condition does this exist but the file itself doesn't?
Handle edge case with empty indices
Find indices which rest in this shard
Need to offset indices to fit within shard_size
Handle the case of datasets with y/w missing
Updating counts
Break when all indices have been used up already
TODO(rbharath): Get rid of * import
Load MUV dataset
Do an approximate comparison since splits are sometimes slightly off from
the exact fraction.
"TODO(rbharath): Transformers don't play nice with reload! Namely,"
reloading will cause the transform to be reapplied. This is undesirable in
almost all cases. Need to understand a method to fix this.
def test_shuffle(self):
"""""""Test that datasets can be merged."""""""
current_dir = os.path.dirname(os.path.realpath(__file__))
dataset_file = os.path.join(
"current_dir, ""../../models/tests/example.csv"")"
featurizer = dc.feat.CircularFingerprint(size=1024)
"tasks = [""log-solubility""]"
loader = dc.data.CSVLoader(
"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
"dataset = loader.featurize(dataset_file, shard_size=2)"
"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
orig_len = len(dataset)
dataset.shuffle(iterations=5)
"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
dataset.ids)
""
assert len(dataset) == orig_len
# The shuffling should have switched up the ordering
"assert not np.array_equal(orig_ids, new_ids)"
# But all the same entries should still be present
assert sorted(orig_ids) == sorted(new_ids)
# All the data should have same shape
assert X_orig.shape == X_new.shape
assert y_orig.shape == y_new.shape
assert w_orig.shape == w_new.shape
The shuffling should have switched up the ordering
But all the same entries should still be present
All the data should have same shape
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
The ids should now store the performed permutation. Check that the
original dataset is recoverable.
Set some global variables up top
Featurize emols dataset
Generate dummy dataset
Generate dummy dataset
Generate dummy dataset
Set last n_samples/2 weights to 0
Check that no support elements are sample from zero-weight samples
Generate dummy dataset
Generate dummy dataset
Create support generator
Generate dummy dataset
Create support generator
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Assert all support elements have been removed
Generate dummy dataset
Assert all remove elements have been removed
Generate dummy dataset
Set last n_samples/2 weights to 0
Sample from first n_samples/2 elements for support
Should lie within first n_samples/2 samples only
Generate dummy dataset
Create support generator
Generate dummy dataset
Test on identity matrix
Generate random sparse features dataset
Test edge case with array of all zeros
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test cases where n_samples < 2*n_samples < batch_size
Test cases where n_samples < batch_size
Test case where n_samples == batch_size
Test case for object featurization.
Test case for more complicated object featurization
Test case with multidimensional data
Test first resharding worked
Test second resharding worked
Generate data
Generate data
Generate data
Transform it
Transform it
Splits featurized samples into train/test
Splits featurized samples into train/test
Splits featurized samples into train/test
"splittype = ""random"""
Splits featurized samples into train/test
Now perform move
Only for debug!
#Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
###### Do featurization
Do train/valid split.
###### Do singletask load
################# Do comparison
Only for debug!
Set some global variables up top
Make directories to store the raw and featurized datasets.
Load dataset
Featurize tox21 dataset
For debugging purposes
###### Do multitask load
Do train/valid split.
###### Do singletask load
################# Do comparison
"task_type = ""regression"""
coding=utf-8
Note that transformers have to be undone in reversed order
Hack to allow for easy unpickling:
http://stefaanlippens.net/pickleproblem
"One, but not both, transform_X or tranform_y is true"
Use fact that bools add as ints in python
Control for pathological case with no variance.
BalancingTransformer can only transform weights.
Compute weighting factors from dataset.
Ensure dataset is binary
Remove labels with zero weights
self.w = dataset.w
"TODO (flee2): for transform_y, figure out weights"
"print(""y will not be transformed by CDFTransformer, for now."")"
"print(""Cannot undo CDF Transformer, for now."")"
Need this for transform_y
array = np.transpose(array)
"print(""y will not be transformed by PowerTransformer, for now."")"
"print(""Cannot undo Power Transformer, for now."")"
the tf graph here pick up the (K+1) highest similarity values
and their indices
map the indices to labels
generating batch of data by slicing similarity matrix
into 100*reference_dataset_length
concatenate batches of data together
highest similarity is 1: target is in the reference
use the following K points
"highest less than 1: target not in the reference, use top K points"
calculate matrix multiplicatin on slices
concatenate the slices together
list of calculation orders for DAGs
stemming from one specific atom in the molecule
starting from the adjacency list derived by graphconv featurizer
"number of atoms, also number of DAGs"
"DAG on a molecule with k atoms includes k steps of calculation,"
each step calculating graph features for one atom.
`max_atoms` is the maximum number of steps
each iteration generates the DAG starting from atom with index `count`
"list of lists, elements represent the calculation orders"
for atoms in the current graph
starting from the target atom with index `count`
flags of whether the atom is already included in the DAG
atom `count` is in the DAG
recording number of radial propagation steps
"in the fisrt loop, atoms directly connected to `count` will be added"
"into the DAG(radial=0), then atoms two-bond away from `count`"
will be added in the second loop(radial=1).
atoms i-bond away will be added in i-th loop
"when molecules have separate parts, starting from one part,"
it is not possible to include all atoms.
this break quit the loop when going into such condition
reinitialize targets for next iteration
atoms connected to current_atom
generate the dependency map of current DAG
atoms connected to `current_atoms`(and not included in the DAG)
"are added, and will be the `current_atoms` for next iteration."
"into next iteration, finding atoms connected one more bond away"
"DAG starts from the target atom, calculation should go in reverse"
`edge[1]` is the parent of `edge[0]`
all the parents of `edge[1]` is also the parents of `edge[0]`
"after this loop, `parents[i]` includes all parents of atom i"
manually adding the atom index into its parents list
"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
atoms with less parents(farther from the target atom) come first.
"graph features of atoms without parents will be first calculated,"
then atoms with more parents can be calculated in order
based on previously calculated graph features.
target atom of this DAG will be calculated in the last step
padding with `max_atoms`
padding
"`parents[i]` is the calculation order for the DAG stemming from atom i,"
which is a max_atoms * max_atoms numpy array after padding
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
transforming y should raise an exception
transforming w should raise an exception
transforming X should be okay
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Tests logarithmic data transformer with selection.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
Check y is now a logarithmic version of itself
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
"Check that y_t has zero mean, unit std."
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is a X transformer
Check w is unchanged since this is a y transformer
"Check that X_t has zero mean, unit std."
np.set_printoptions(threshold='nan')
Entries with zero std are not normalized
TODO(rbharath): Untransform doesn't work properly for binary feature
vectors. Need to figure out what's wrong here. (low priority)
# Check that untransform does the right thing.
"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Test CDF transformer on Gaussian normal dataset.
Check ids are unchanged.
Check X is unchanged since this is an y transformer
Check w is unchanged since this is an y transformer
Check y is now holding the proper values when sorted.
Check that untransform does the right thing.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values when sorted.
Check ids are unchanged.
Check X is unchanged since this is a y transformer
Check w is unchanged since this is a y transformer
Check y is now holding the proper values when sorted.
Check ids are unchanged.
Check y is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check X is now holding the proper values in each column.
Check ids are unchanged.
Check X is unchanged since this is an X transformer
Check w is unchanged since this is an X transformer
Check y is now holding the proper values in each column.
Check that untransform does the right thing.
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
Check ids are unchanged.
Check X is unchanged since this is a w transformer
Check y is unchanged since this is a w transformer
Assert that entries with zero weight retain zero weight
Check that sum of 0s equals sum of 1s in transformed for each task
TODO(rbharath): Use standard joblib once old-data has been regenerated.
"If gzipped, need to compute extension again"
Tasks are stored in .sdf.csv file
Structures are stored in .sdf file
First line of user-specified CSV *must* be header.
Try older joblib version for legacy files.
First line of user-specified CSV *must* be header.
First line of user-specified CSV *must* be header.
combine dataframes
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
import nglview
import tempfile
import os
import mdtraj as md
import numpy as np
import tempfile
from rdkit import Chem
from rdkit.Chem import Draw
from itertools import islice
"from IPython.display import Image, HTML, display"
""
"def combine_mdtraj(protein, ligand):"
chain = protein.topology.add_chain()
"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
for atom in ligand.topology.atoms:
"protein.topology.add_atom(atom.name, atom.element, residue)"
"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
protein.topology.create_standard_bonds()
return protein
""
def visualize_complex(complex_mdtraj):
"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
binding_pocket_residues = [str(r) for r in binding_pocket_residues]
"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
""
traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""cartoon"", ""params"": {"
"""sele"": ""protein"", ""color"": ""residueindex"""
"} },"
"{ ""type"": ""licorice"", ""params"": {"
"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
"} },"
"{ ""type"": ""ball+stick"", ""params"": {"
"""sele"": ""LIG"""
} }
]
return ngltraj
""
def visualize_ligand(ligand_mdtraj):
traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
ngltraj = nglview.NGLWidget( traj )
ngltraj.representations = [
"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
return ngltraj
""
def convert_lines_to_mdtraj(molecule_lines):
tempdir = tempfile.mkdtemp()
"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
"with open(molecule_file, ""wb"") as f:"
f.writelines(molecule_lines)
molecule_mdtraj = md.load(molecule_file)
return molecule_mdtraj
""
def display_images(filenames):
"""""""Helper to pretty-print images."""""""
imagesList=''.join(
"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
% str(s) for s in sorted(filenames)])
display(HTML(imagesList))
""
"def mols_to_pngs(mols, basename=""test""):"
"""""""Helper to write RDKit mols to png files."""""""
filenames = []
"for i, mol in enumerate(mols):"
"filename = ""%s%d.png"" % (basename, i)"
"Draw.MolToFile(mol, filename)"
filenames.append(filename)
return filenames
TODO(rbharath): This is now simple enough that we should probably get rid of
Evaluator object to avoid clutter.
Compute multitask metrics
Compute multitask metrics
Loosening atol to see if tests stop failing sporadically
!/usr/bin/env python2
-*- coding: utf-8 -*-
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
TODO(LESWING)
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
############################################################# TIMING
TODO(enf): make array index checking not a try-catch statement.
Get the degree id list (which corrects for min_deg)
Get the size of each degree block
Get the the start indices for items in each block
Get the node indices when they are reset when the degree changes
Convert to numpy array
Reorder old atom_features
Reorder old deg lists
Sort membership
Create old to new dictionary. not exactly intuitive
Reorder adjacency lists
Get numpy version of degree list for indexing
"Initialize adj_lists, which supports min_deg = 1 only"
Parse as deg separated
Get indices corresponding to the current degree
Extract and save adjacency list for the current degree
Construct the slice information
Get the cumulative indices after the first index
Set indices with zero sized slices to zero to avoid indexing errors
TODO(rbharath): Can this be removed?
Use random insted of zeros to prevent weird issues with summing to zero
Get atoms by degree
stack the atoms
Sort all atoms by degree.
"Get the size of each atom list separated by molecule id, then by degree"
Get the final size of each degree block
"Get the index at which each degree starts, not resetting after each degree"
And not stopping at any speciic molecule
"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
first column telling the start indices of each degree block and the
second colum telling the size of each degree block
Input for tensorflow
Determines the membership (atom i belongs to membership[i] molecule)
"Get the index at which each deg starts, resetting after each degree"
(deg x num_mols) matrix describing the start indices when you count up the atoms
"in the final representation, stopping at each molecule,"
resetting every time the degree changes
Gets the degree resetting block indices for the atoms in each molecule
"Here, the indices reset when the molecules change, and reset when the"
degree changes
Get the degree id lookup list. It allows us to search for the degree of a
molecule mol_id with corresponding atom mol_atom_id using
"deg_id_lists[mol_id,mol_atom_id]"
This is used for convience in the following function (explained below)
Get the degree id (corrected for min_deg) of the considered atom
Return the final index of atom mol_atom_id in molecule mol_id.  Using
"the degree of this atom, must find the index in the molecule's original"
"degree block corresponding to degree id deg_id (second term), and then"
calculate which index this degree block ends up in the final
representation (first term). The sum of the two is the final indexn
Initialize the new degree separated adjacency lists
Update the old adjcency lists with the new atom indices and then combine
all together
Iterate through all the molecules
Get the adjacency lists for this molecule and current degree id
"Correct all atom indices to the final indices, and then save the"
results into the new adjacency lists
Increment once row is done
Get the final aggregated molecule
RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
consistent with most QM software packages.
Type of data created by this featurizer
TODO(rbharath): Should this return a list?
Type of data created by this featurizer
generate SMILES for fragments
Initalize with 1
Allow 0 index to correspond to null molecule 1
Correct for null
"print(6-k-1, id)"
Correct for last one
first `bt_len` features are bond features(if applicable)
`bt_len`-th feature is if the pair of atoms are in the same ring
graph distance between two atoms
atoms `radial` bonds away from `a1`
atoms less than `radial` bonds away
find atoms `radial`+1 bonds away
"Since ConvMol is an object and not a numpy array, need to set dtype to"
object.
Get the node features
Stack nodes into an array
Get bond lists with reverse edges included
Get canonical adjacency list
Set dtype
Atom features
Stack nodes into an array
Get bond lists
Get canonical adjacency list
Calculate pair features
atom_name is of format RESX-ATOMTYPE
where X is a 1 to 4 digit number
list-of-available-descriptors.
(ytz): This is done to avoid future compatibility issues like inclusion of
the 3D descriptors or changing the feature size.
check for separate count and SMILES entries for each fragment
"Note there is a central carbon of degree 4, with 3 carbons and"
one nitrogen of degree 1 (connected only to central carbon).
5 atoms in compound
Get the adjacency lists grouped by degree
The 4 outer atoms connected to central carbon
Central carbon connected to everything else.
Only one carbon
"No bonds, so degree adjacency lists are empty"
3 carbonds in alkane
Outer two carbonds are connected to central carbon
Central carbon connected to outer two
"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
this expected behavior? Need to think about API.
Do a manual distance computation and make
Test with cutoff 0 angstroms. There should be no neighbors in this case.
Test with cutoff 100 angstroms. Everything should be neighbors now.
Do a manual distance computation and ensure that selected neighbor is
closest since we set max_num_neighbors = 1
Splits featurized samples into train/test
Artificial feature array.
0 atoms of degree 0
0 atoms of degree 1
4 atoms of degree 2
0 atoms of degree 3
0 atoms of degree 4
0 atoms of degree 5
0 atoms of degree 6
0 atoms of degree 7
0 atoms of degree 8
0 atoms of degree 9
0 atoms of degree 10
atom 4 has 0 neighbors
atom 0 has 2 neighbors
atom 1 has 2 neighbors
atom 2 has 2 neighbors
atom 3 has 3 neighbors.
Verify that atom features have been sorted by atom degree.
Sorting is done by atom degree as before. So the ordering goes
"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
from new position to old position is
"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
list respects this reordering and returns correct adjacency list.
### First example molecule
Artificial feature array.
### Second example molecule
## Third example molecule
Test agglomerate molecule method
No atoms of degree 0
3 atoms of degree 1
8 atoms of degree 2
1 atom of degree 3
0 atoms of degree 4
0 atoms of degree 5
Check that atoms are only connected to themselves.
Check that there's one atom of each degree.
assumes that every array is of the same dimension
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
returns list of per column sum of non zero elements
Compute number of actives needed per task.
loop through each column and obtain index required to splice out for
required fraction of hits
Find the first index where the cumulative number of actives equals
the actives_count
Note that np.where tells us last index required to exceed
"actives_count, so we actually want the following location"
TODO(rbharath): Refactor this split method to match API of other splits (or
potentially refactor those to match this.
Handle edge case where frac_split is 1
Create weight matrices fpor two haves.
copy over up to required index for weight first_split
check out if any rows in either w_1 or w_2 are just zeros
"Obtain original x, y, and w arrays and shuffle"
calculate percent split for valid (out of test and valid)
"split test data into valid and test, treating sub test set also as sparse"
rem_dataset is remaining portion of dataset
Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
to k-1.
JSG Assert that split fractions can be written as proper fractions over 10.
This can be generalized in the future with some common demoninator determination.
This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
Append remaining examples to train
Sort by increasing MW
(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
for m_idx in cluster:
"continue until we find an active in all the tasks, otherwise we can't"
compute a meaningful AUC
"TODO (ytz): really, we want at least one active and inactive in both scenarios."
TODO (Ytz): for regression tasks we'd stop after only one cluster.
Sort from largest to smallest scaffold sets
All datasets share features and identifiers by assumption.
TODO(rbharath): Get rid of * import
Note that the extra task goes to test
Number tasks per fold
Find the tasks that correspond to this test fold
Assert that all arrays look like they should
TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
data. Make a test for properly splitting of sharded data. Perhaps using
reshard() to handle this?
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Verify lengths is 10/k == 2
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
Test singletask case.
The split index should partition dataset in half.
Test singletask case.
Test case where some weights are zero (i.e. masked)
Set half the positives to have zero weight
There are 10 nonzero actives.
"The split index should partition this into half, so expect 5"
The split index should partition dataset in half.
Mask half the examples
The split index should partition dataset in half.
Test singletask case.
Should have split cleanly in half (picked random seed to ensure this)
Check positives are correctly distributed
Verify lengths is 100/k == 20
Note: This wouldn't work for multitask str
assert len(fold_dataset) == n_samples/K
Verify that each fold has n_positives/K = 4 positive examples.
Verify that compounds in this fold are subset of original compounds
Verify that no two folds have overlapping compounds.
sparsity is determined by number of w weights that are 0 for a given
task structure of w np array is such that each row corresponds to a
sample. The loaded sparse dataset has many rows with only zeros
verify that there are no rows (samples) in weights matrix w
that have no hits.
Path to save checkpoint files
first layer in model: check that it is an input layer
Add losses to graph
Loss for each batch element
Loss should be a float
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
TODO(rbharath): Don't support example weighting yet.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Arguments
Returns
Arguments
Returns
task_metadata_rows = {task: [] for task in tasks}
Extract those datapoints which are present for this task
Loading is done on-the-fly
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs, ndim)"
"Shape (N_atoms, M_nbrs)"
TODO(rbharath): Note sure if this layer can be called with __call__
"meaningfully, so not going to support that functionality for now."
"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Generate the nb_affine weights and biases
Extract atom_features
Extract graph topology
Perform the mol conv
"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
"self.max_deg, self.min_deg, self.W_list,"
self.b_list)
Sum all neighbors using adjacency matrix
Get collection of modified atom features
Obtain relevant atoms for this degree
Get self atoms
Apply hidden affine to relevant atoms and append
Determine the min_deg=0 case
Only use the self layer
Combine all atoms back into the list
Tensorflow correctly processes empty lists when using concat
"Sum along neighbors as well as self, and store"
Perform the mol gather
"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
"self.max_degree, self.min_degree)"
Tensorflow correctly processes empty lists when using concat
Get self atoms
Expand dims
always deg-1 for deg_adj_lists
"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
Extract graph topology
Perform the mol gather
Obtain the partitions for each of the molecules
Sum over atoms for each molecule
Get the final sparse representations
Number of rotatable bonds
TODO(rbharath): Vina actually sets this per-molecule. See if makes
a difference.
TODO(rbharath): This layer shouldn't be neighbor-listing. Make
neighbors lists an argument instead of a part of this layer.
"Shape (N, M)"
"Shape (N, M)"
"Shape (N, M)"
Number of grid cells
TODO(rbharath): Support batching
"Shape (n_cells, ndim)"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each element of different length uniques_i"
"List of length N_atoms, each a tensor of shape"
"(uniques_i, ndim)"
Add phantom atoms that exist far outside the box
"List of length N_atoms, each of shape (1, ndim)"
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
List of length N_atoms each of shape (M_nbrs)
"N_atoms elts of size (M_nbrs,) each"
"Shape (N_atoms, 1)"
Find M_nbrs atoms closest to each cell
"Shape (n_cells, M_nbrs)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells)"
"Shape (N_atoms, n_nbr_cells, M_nbrs)"
"Shape (N_atoms, n_nbr_cells*M_nbrs)"
"List of length N_atoms, each element length uniques_i"
TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
element removed to remove self from list of neighbors. Need to verify
this holds more broadly or come up with robust alternative.
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, ndim) after tile"
Shape (N_atoms*n_cells)
"Shape (n_cells, N_atoms)"
Find k atoms closest to this cell. Notice negative sign since
tf.nn.top_k returns *largest* not smallest.
"Tensor of shape (n_cells, M_nbrs)"
"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
"Shape (N_atoms*n_cells, 1) after tile"
9 neighbors in 2-space
TODO(rbharath): Shoddy handling of higher dimensions...
Number of cells for cube in 3-space is
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
the cube.
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
N: Maximum number of atoms
M: Maximum number of neighbors
d: Number of coordinates/features/filters
B: Batch Size
We apply the radial pooling filter before atom type conv
to reduce computation
!/usr/bin/env python2
-*- coding: utf-8 -*-
"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
and embeddings of atom j(both gone through a hidden layer)
"for atom i, sum the influence from all other atom j in the molecule"
number of inputs each step
Add trainable weights
"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
target atoms for each step: (batch_size*max_atoms) * max_atoms
initialize graph features for each graph
initialize graph features for each graph
another row of zeros is generated for padded dummy atoms
`count`-th step
extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
generating index for graph features used in the inputs
"extracting graph features for parents of the target atoms, then flatten"
shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
concat into the input tensor: (batch_size*max_atoms) * n_inputs
DAGgraph_step maps from batch_inputs to a batch of graph_features
of shape: (batch_size*max_atoms) * n_graph_features
representing the graph features of target atoms in each graph
index for targe atoms
update the graph features for target atoms
Add trainable weights
Extract atom_features
Extract atom_features
sum all graph outputs
Layer Management
Singular place to hold Tensor objects which don't serialize
These have to be reconstructed on restoring from pickle
See TensorGraph._get_tf() for more details on lazy construction
############################################################# TIMING
############################################################# TIMING
Arguments
Returns
Arguments
Returns
Remove out_tensor from the object to be pickled
Pickle itself
add out_tensor back to everyone
"This isn't a meaningful loss, but just for test"
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
1 and 2 are nbrs. 8 and 9 are nbrs
"Now an (N, M) shape"
TODO(rbharath): Move this into a model directly
def test_vina(self):
"""""""Test that vina graph can be constructed in TensorGraph."""""""
N_protein = 4
N_ligand = 1
N_atoms = 5
M_nbrs = 2
ndim = 3
start = 0
stop = 4
nbr_cutoff = 1
"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
start))
"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
start))
y = NumpyDataset(np.random.rand(
"1,))"
"# TODO(rbharath): Mysteriously, the actual atom types aren't"
"# used in the current implementation. This is obviously wrong, but need"
# to dig out why this is happening.
"prot_coords = Feature(shape=(N_protein, ndim))"
"ligand_coords = Feature(shape=(N_ligand, ndim))"
"labels = Label(shape=(1,))"
"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
"# Now an (N, M) shape"
nbr_list = NeighborList(
"N_protein + N_ligand,"
"M_nbrs,"
"ndim,"
"nbr_cutoff,"
"start,"
"stop,"
in_layers=[coords])
"# Shape (N, M)"
dists = InteratomicL2Distances(
"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
repulsion = VinaRepulsion(in_layers=[dists])
hydrophobic = VinaHydrophobic(in_layers=[dists])
hbond = VinaHydrogenBond(in_layers=[dists])
gauss_1 = VinaGaussianFirst(in_layers=[dists])
gauss_2 = VinaGaussianSecond(in_layers=[dists])
"# Shape (N, M)"
interactions = WeightedLinearCombo(
"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
"# Shape (N, M)"
"thresholded = Cutoff(in_layers=[dists, interactions])"
"# Shape (N, M)"
free_energies = VinaNonlinearity(in_layers=[thresholded])
free_energy = ReduceSum(in_layers=[free_energies])
"loss = L2Loss(in_layers=[free_energy, labels])"
"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
tg.set_loss(loss)
tg.fit_generator(databag.iterbatches(epochs=1))
TODO(rbharath): This test should pass. Fix it!
def test_graph_pool(self):
"""""""Test that GraphPool can be invoked."""""""
out_channels = 2
"n_atoms = 4 # In CCC and C, there are 4 atoms"
"raw_smiles = ['CCC', 'C']"
mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
featurizer = ConvMolFeaturizer()
mols = featurizer.featurize(mols)
multi_mol = ConvMol.agglomerate_mols(mols)
atom_features = multi_mol.get_atom_features()
degree_slice = multi_mol.deg_slice
membership = multi_mol.membership
deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
with self.test_session() as sess:
"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
deg_adjs_tf = []
for deg_adj in deg_adjs:
"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
out_tensor = GraphPool(out_channels)(*args)
sess.run(tf.global_variables_initializer())
out_tensor = out_tensor.eval()
"assert out_tensor.shape == (n_atoms, out_channels)"
TODO(rbharath): Why is it 2*n_features instead of n_features?
############################################################# TIMING
############################################################# TIMING
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
import tensorflow as tf
from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
""
""
class WeightedError(Layer):
""
"def __call__(self, *parents):"
"entropy, weights = parents[0], parents[1]"
self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
return self.out_tensor
""
""
"def tensorGraphMultitaskClassifier(n_tasks,"
"n_features,"
"layer_sizes=[500],"
"bypass_layer_sizes=[100],"
model_dir=None):
""""""""
TODO(LESWING) Add Dropout and regularization
""
Parameters
----------
n_tasks
n_features
layer_sizes
bypass_layer_sizes
model_dir
""
Returns
-------
""
""""""""
g = MultiTaskTensorGraph(model_dir=model_dir)
"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
g.add_layer(in_layer)
g.add_feature(in_layer)
""
# Shared Dense Layers
prev_layer = in_layer
dense_layers = []
for i in range(len(layer_sizes)):
dense = Dense(
"out_channels=layer_sizes[i],"
"name=""SDENSE%s"" % i,"
activation_fn=tf.nn.relu)
"g.add_layer(dense, parents=[prev_layer])"
dense_layers.append(dense)
prev_layer = dense
""
# Individual Bypass Layers
costs = []
for task in range(n_tasks):
prev_layer = in_layer
for i in range(len(bypass_layer_sizes)):
dense = Dense(
"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
"g.add_layer(dense, parents=[prev_layer])"
prev_layer = dense
"joined_layer = Concat(name=""JOIN%s"" % task)"
"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
""
"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
"g.add_layer(classification, parents=[joined_layer])"
""
"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
"g.add_layer(softmax, parents=[classification])"
g.add_output(softmax)
""
"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
g.add_layer(label)
g.add_label(label)
""
"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
"g.add_layer(cost, parents=[label, classification])"
costs.append(cost)
""
"entropy = Concat(name=""ENT"")"
"g.add_layer(entropy, parents=costs)"
""
"task_weights = Input(shape=(None, n_tasks), name=""W"")"
g.add_layer(task_weights)
g.set_task_weights(task_weights)
""
"loss = WeightedError(name=""ERROR"")"
"g.add_layer(loss, parents=[entropy, task_weights])"
g.set_loss(loss)
""
return g
update model with best param
Find optimal n_estimators based on original learning_rate
and early_stopping_rounds
"Since test size is 20%, when retrain model to whole data, expect"
n_estimator increased to 1/0.8 = 1.25 time.
Make sure user specified params are in the grid.
Change params back original params
TODO (LESWING) Lazy Load
TODO (LESWING) Lazy Load
Generate dummy dataset
Fit trained model
Check same predictions are made.
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Generate dummy dataset
Fit trained model
Load trained model
Eval model on train
Fit trained model
Eval model on train
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Fit trained model
Eval model on train/test
Test Parameter getting and setting
Fit trained model
Eval model on train/test
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
n_samples = 100
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Generate dummy dataset
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Gather Projection
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Fit trained model
Eval model on train
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply an attention lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .85
#################################################### DEBUG
Load mini log-solubility dataset.
Add layers
"output will be (n_atoms, 64)"
Need to add batch-norm separately to test/support due to differing
shapes.
"output will be (n_atoms, 64)"
"output will be (n_atoms, 64)"
Apply a residual lstm layer
"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly.
"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
n_pos/n_neg accordingly. Note that support is *not* excluded (so we
can measure model has memorized support).  Replacement is turned off to
ensure that support contains full training set. This checks that the
model has mastered memorization of provided support.
Measure performance on 0-th task.
#################################################### DEBUG
TODO(rbharath): Check if something went wrong here...
Measure performance on 0-th task.
assert scores[0] > .9
#################################################### DEBUG
Generate dummy dataset
Fit trained model
Eval model on train
def test_singletask_to_multitask_classification(self):
n_features = 10
n_tasks = 17
tasks = range(n_tasks)
# Define train dataset
n_train = 100
"X_train = np.random.rand(n_train, n_features)"
"y_train = np.random.randint(2, size=(n_train, n_tasks))"
w_train = np.ones_like(y_train)
"ids_train = [""C""] * n_train"
train_dataset = dc.data.DiskDataset.from_numpy(
"X_train, y_train, w_train, ids_train)"
# Define test dataset
n_test = 10
"X_test = np.random.rand(n_test, n_features)"
"y_test = np.random.randint(2, size=(n_test, n_tasks))"
w_test = np.ones_like(y_test)
"ids_test = [""C""] * n_test"
test_dataset = dc.data.DiskDataset.from_numpy(
"X_test, y_test, w_test, ids_test)"
classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
multitask_model = dc.models.SingletaskToMultitask(
"tasks, model_builder)"
# Fit trained model
multitask_model.fit(train_dataset)
multitask_model.save()
# Eval multitask_model on train/test
"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
Generate data
Cleanup
Generate dummy dataset
Fit trained model
Eval model on test
Eval model on train
Fit trained model
Eval model on test
Fit trained model
Eval model on test
def test_sklearn_classification(self):
"""""""Test that sklearn models can learn on simple classification datasets."""""""
np.random.seed(123)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
sklearn_model = LogisticRegression()
model = dc.models.SklearnModel(sklearn_model)
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
assert scores[classification_metric.name] > .5
def test_sklearn_multitask_classification(self):
"""""""Test that sklearn models can learn on simple multitask classification."""""""
np.random.seed(123)
n_tasks = 4
tasks = range(n_tasks)
dataset = sklearn.datasets.load_digits(n_class=2)
"X, y = dataset.data, dataset.target"
"y = np.reshape(y, (len(y), 1))"
y = np.hstack([y] * n_tasks)
""
frac_train = .7
n_samples = len(X)
n_train = int(frac_train*n_samples)
"X_train, y_train = X[:n_train], y[:n_train]"
"X_test, y_test = X[n_train:], y[n_train:]"
"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
def model_builder(model_dir):
sklearn_model = LogisticRegression()
"return dc.models.SklearnModel(sklearn_model, model_dir)"
"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
# Fit trained model
model.fit(train_dataset)
model.save()
# Eval model on test
"scores = model.evaluate(test_dataset, [classification_metric])"
for score in scores[classification_metric.name]:
assert score > .5
Set early stopping round = n_estimators so that esr won't work
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Fit trained model
Eval model on test
Logistic regression doesn't support weights
Consistency check
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
Run training op.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
"""""""Ops for graph construction."""""""
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals
""
import sys
import traceback
import tensorflow as tf
from keras import backend as K
""
"def cosine_distances(test, support):"
"""""""Computes pairwise cosine distances between provided tensors"
""
Parameters
----------
test: tf.Tensor
"Of shape (n_test, n_feat)"
support: tf.Tensor
"Of shape (n_support, n_feat)"
""
Returns
-------
tf.Tensor:
"Of shape (n_test, n_support)"
""""""""
"rnorm_test = tf.rsqrt(tf.reduce_sum(tf.square(test), 1,"
keep_dims=True)) + K.epsilon()
"rnorm_support = tf.rsqrt(tf.reduce_sum(tf.square(support), 1,"
keep_dims=True)) + K.epsilon()
test_normalized = test * rnorm_test
support_normalized = support * rnorm_support
""
# Transpose for mul
"support_normalized_t = tf.transpose(support_normalized, perm=[1,0])"
"g = tf.matmul(test_normalized, support_normalized_t)  # Gram matrix"
return g
""
"def euclidean_distance(test, support, max_dist_sq=20):"
"""""""Computes pairwise euclidean distances between provided tensors"
""
TODO(rbharath): BROKEN! THIS DOESN'T WORK!
""
Parameters
----------
test: tf.Tensor
"Of shape (n_test, n_feat)"
support: tf.Tensor
"Of shape (n_support, n_feat)"
"max_dist_sq: float, optional"
Maximum pairwise distance allowed.
""
Returns
-------
tf.Tensor:
"Of shape (n_test, n_support)"
""""""""
"test = tf.expand_dims(test, 1)"
"support = tf.expand_dims(support, 0)"
"g = -tf.maximum(tf.reduce_sum(tf.square(test - support), 2), max_dist_sq)"
return g
""
"def add_bias(tensor, init=None, name=None):"
"""""""Add a bias term to a tensor."
""
Parameters
----------
tensor: tf.Tensor
Variable tensor.
init: float
Bias initializer. Defaults to zero.
name: str
Name for this op. Defaults to tensor.op.name.
""
Returns
-------
tf.Tensor
A biased tensor with the same shape as the input tensor.
""""""""
if init is None:
init = tf.zeros([tensor.get_shape()[-1].value])
"with tf.name_scope(name, tensor.op.name, [tensor]):"
"b = tf.Variable(init, name='b')"
"return tf.nn.bias_add(tensor, b)"
""
""
"def dropout(tensor, dropout_prob, training=True, training_only=True):"
"""""""Random dropout."
""
"This implementation supports ""always-on"" dropout (training_only=False), which"
"can be used to calculate model uncertainty. See Gal and Ghahramani,"
http://arxiv.org/abs/1506.02142.
""
"NOTE(user): To simplify the implementation, I have chosen not to reverse"
the scaling that occurs in tf.nn.dropout when using dropout during
inference. This shouldn't be an issue since the activations will be scaled
by the same constant in both training and inference. This means that there
are no training-time differences between networks that use dropout during
inference and those that do not.
""
Parameters
----------
tensor: tf.Tensor
Input tensor.
dropout_prob: float
Float giving dropout probability for weights (NOT keep probability).
training_only: bool
"Boolean. If True (standard dropout), apply dropout only"
"during training. If False, apply dropout during inference as well."
""
Returns
-------
tf.Tensor:
A tensor with the same shape as the input tensor.
""""""""
if not dropout_prob:
return tensor  # do nothing
keep_prob = 1.0 - dropout_prob
if training or not training_only:
"tensor = tf.nn.dropout(tensor, keep_prob)"
return tensor
""
""
"def fully_connected_layer(tensor, size=None, weight_init=None, bias_init=None,"
name=None):
"""""""Fully connected layer."
""
Parameters
----------
tensor: tf.Tensor
Input tensor.
size: int
Number of output nodes for this layer.
weight_init: float
Weight initializer.
bias_init: float
Bias initializer.
name: str
Name for this op. Defaults to 'fully_connected'.
""
Returns
-------
tf.Tensor:
A new tensor representing the output of the fully connected layer.
""
Raises
------
ValueError
If input tensor is not 2D.
""""""""
if len(tensor.get_shape()) != 2:
"raise ValueError('Dense layer input must be 2D, not %dD'"
% len(tensor.get_shape()))
if weight_init is None:
num_features = tensor.get_shape()[-1].value
"weight_init = tf.truncated_normal([num_features, size], stddev=0.01)"
if bias_init is None:
bias_init = tf.zeros([size])
""
"with tf.name_scope(name, 'fully_connected', [tensor]):"
"w = tf.Variable(weight_init, name='w', dtype=tf.float32)"
"b = tf.Variable(bias_init, name='b', dtype=tf.float32)"
"return tf.nn.xw_plus_b(tensor, w, b)"
""
"def weight_decay(penalty_type, penalty):"
"""""""Add weight decay."
""
Args:
model: TensorflowGraph.
""
Returns:
A scalar tensor containing the weight decay cost.
""
Raises:
NotImplementedError: If an unsupported penalty type is requested.
""""""""
variables = []
# exclude bias variables
for v in tf.trainable_variables():
if v.get_shape().ndims == 2:
variables.append(v)
""
with tf.name_scope('weight_decay'):
if penalty_type == 'l1':
cost = tf.add_n([tf.reduce_sum(tf.abs(v)) for v in variables])
elif penalty_type == 'l2':
cost = tf.add_n([tf.nn.l2_loss(v) for v in variables])
else:
raise NotImplementedError('Unsupported penalty_type %s' % penalty_type)
cost *= penalty
"tf.scalar_summary('Weight Decay Cost', cost)"
return cost
""
""
"def multitask_logits(features, num_tasks, num_classes=2, weight_init=None,"
"bias_init=None, dropout_prob=None, name=None):"
"""""""Create a logit tensor for each classification task."
""
Args:
features: A 2D tensor with dimensions batch_size x num_features.
num_tasks: Number of classification tasks.
num_classes: Number of classes for each task.
weight_init: Weight initializer.
bias_init: Bias initializer.
dropout_prob: Float giving dropout probability for weights (NOT keep
probability).
name: Name for this op. Defaults to 'multitask_logits'.
""
Returns:
A list of logit tensors; one for each classification task.
""""""""
logits_list = []
with tf.name_scope('multitask_logits'):
for task_idx in range(num_tasks):
"with tf.name_scope(name,"
"('task' + str(task_idx).zfill(len(str(num_tasks)))), [features]):"
logits_list.append(
"logits(features, num_classes, weight_init=weight_init,"
"bias_init=bias_init, dropout_prob=dropout_prob))"
return logits_list
""
""
"def logits(features, num_classes=2, weight_init=None, bias_init=None,"
"dropout_prob=None, name=None):"
"""""""Create a logits tensor for a single classification task."
""
You almost certainly don't want dropout on there -- it's like randomly setting
the (unscaled) probability of a target class to 0.5.
""
Args:
features: A 2D tensor with dimensions batch_size x num_features.
num_classes: Number of classes for each task.
weight_init: Weight initializer.
bias_init: Bias initializer.
dropout_prob: Float giving dropout probability for weights (NOT keep
probability).
name: Name for this op.
""
Returns:
A logits tensor with shape batch_size x num_classes.
""""""""
"with tf.name_scope(name, 'logits', [features]) as name:"
return dropout(
"fully_connected_layer(features, num_classes, weight_init=weight_init,"
"bias_init=bias_init, name=name),"
dropout_prob)
""
""
"def softmax_N(tensor, name=None):"
"""""""Apply softmax across last dimension of a tensor."
""
Args:
tensor: Input tensor.
"name: Name for this op. If None, defaults to 'softmax_N'."
""
Returns:
A tensor with softmax-normalized values on the last dimension.
""""""""
"with tf.name_scope(name, 'softmax_N', [tensor]):"
exp_tensor = tf.exp(tensor)
reduction_indices = [tensor.get_shape().ndims - 1]
"return tf.div(exp_tensor,"
"tf.reduce_sum(exp_tensor,"
"reduction_indices=reduction_indices,"
keep_dims=True))
""
"def optimizer(optimizer=""adam"", learning_rate=.001, momentum=.9):"
"""""""Create model optimizer."
""
Parameters
----------
"optimizer: str, optional"
Name of optimizer
"learning_rate: float, optional"
Learning rate for algorithm
"momentum: float, optional"
Momentum rate
""
Returns
-------
A training Optimizer.
""
Raises:
NotImplementedError: If an unsupported optimizer is requested.
""""""""
# TODO(user): gradient clipping (see Minimize)
if optimizer == 'adagrad':
train_op = tf.train.AdagradOptimizer(learning_rate)
elif optimizer == 'adam':
train_op = tf.train.AdamOptimizer(learning_rate)
elif optimizer == 'momentum':
"train_op = tf.train.MomentumOptimizer(learning_rate,"
momentum)
elif optimizer == 'rmsprop':
"train_op = tf.train.RMSPropOptimizer(learning_rate,"
momentum)
elif optimizer == 'sgd':
train_op = tf.train.GradientDescentOptimizer(learning_rate)
else:
raise NotImplementedError('Unsupported optimizer %s' % optimizer)
return train_op
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
get the divisor
compute the requested central moment
"note that mean is a raw moment, not a central moment"
TODO(user): median is not implemented yet in TensorFlow
-*- coding: utf-8 -*-
"due to the different shape of weight(ndims=2) and bias(ndims=1),"
will using this version for logreg
exclude bias variables
setting up n_tasks nodes(output nodes)
label placeholders with size batch_size * 1
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
using self-defined regularization
adding output nodes of sigmoid function
"fix the size to be [?,1]"
Dummy placeholders
Dummy placeholders
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
run eval data through the model
transfer 2D prediction tensor to 2D x n_classes(=2)
reshape to batch_size x n_tasks x ...
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
"layer has shape [None, layer_sizes[i]]"
"top_multitask_layer has shape [None, layer_sizes[-1]]"
TODO(rbharath): Might want to make it feasible to have multiple
bypass layers.
Construct task bypass layer
"bypass_layer has shape [None, bypass_layer_sizes[i]]"
"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
Add the input features.
Add the dense layers
Compute the loss function for each label.
Add the input features.
Add the dense layers
Compute the loss function for each label.
Run fit transformers on dummy dataset to determine n_features after transformation
Dummy placeholders
Dummy placeholders
Dummy placeholders
Dummy placeholders
Run fit transformers on dummy dataset to determine n_features after transformation
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
Consistency check
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Create placeholders
Handle output layer
Iterate over all previous tasks.
prev_layers is a list with elements of size
"(batch_size, layer_sizes[i-1])"
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
Dummy placeholders
Dummy placeholders
run eval data through the model
"Shape (n_tasks, n__samples)"
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
with self._get_shared_session(train=True) as sess:
Save an initial checkpoint.
Always save a final checkpoint when complete.
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
Dummy placeholders
"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
############################################################# TIMING
############################################################# TIMING
Turns out there are valid cases where we don't want pad-batches
on by default.
"dataset.iterbatches(batch_size, pad_batches=True)):"
if epoch%checkpoint_interval == checkpoint_interval-1:
"saver.save(sess, self._save_path, global_step=epoch)"
############################################################# TIMING
############################################################# TIMING
"(n_samples, n_classes)"
"(n_samples, n_tasks, n_classes)"
Save hyperparameters
Guard variable to make sure we don't Restore() this model
from a disk checkpoint more than once.
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Lazily created by _get_shared_session().
"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
when subclass-overridden methods use the same scopes.
Setup graph
Note that we divide by the batch size and not the number of
"non-zero weight examples in the batch.  Also, instead of using"
tf.reduce_mean (which can put ops on the CPU) we explicitly
calculate with div/sum so it stays on the GPU.
aggregated costs
weight decay
############################################################# TIMING
############################################################# TIMING
Save an initial checkpoint.
Define the code that runs on a separate thread to feed data into the queue.
Main training loop.
Run training op.
We have reached the end of an epoch.
We have reached the end of the data.
Always save a final checkpoint when complete.
############################################################# TIMING
############################################################# TIMING
allow_soft_placement=True allows ops without a GPU implementation
to run on the CPU instead.
TODO(rbharath): Is setting train=False right here?
Discard any padded predictions
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
Special case to handle singletasks.
The iterbatches does padding with zero-weight examples on the last batch.
Remove padded examples.
TODO(rbharath): Verify this can be safely removed.
"def evaluate(self, dataset, metrics, transformers=[]):"
""""""""
Evaluates the performance of this model on specified dataset.
""
Parameters
----------
dataset: dc.data.Dataset
Dataset object.
metric: deepchem.metrics.Metric
Evaluation metric
transformers: list
List of deepchem.transformers.Transformer
Returns
-------
dict
Maps tasks to scores under metric.
""""""""
"evaluator = Evaluator(self, dataset, transformers)"
scores = evaluator.compute_model_performance(metrics)
return scores
checkpoints look like logdir/model.ckpt-N
"self._save_path is ""logdir/model.ckpt"""
run eval data through the model
reshape to batch_size x n_tasks x ...
run eval data through the model
reshape to batch_size x n_tasks x ...
Note that softmax is already applied in construct_grpah
run eval data through the model
reshape to batch_size x n_tasks x ...
Handle edge case when batch-size is 1.
Prune away any padding that was added
Handle case of 0-dimensional scalar output
!/usr/bin/python
""
Copyright 2015 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
parse CheckpointState proto
parse path to actual checkpoint
the provided mask has to be the same shape as features
test k = 1..4
central moments
standardized moments
central across one axis
standardized across one axis
Fit just on task zero
Notice that we keep the session open
Fit on task one
The predictions for task zero should not change after training
on task one.
Keep track of the layers
############################################ DEBUG
"print(""start - add()"")"
"print(""self.output"")"
print(self.output)
############################################ DEBUG
"For graphical layers, add connectivity placeholders"
############################################ DEBUG
"print(""end- add()"")"
"print(""self.output"")"
print(self.output)
############################################ DEBUG
Add layer to the layer list
Keep track of the layers
Create graph topology and x
Keep track of the layers
Whether or not we have used the GraphGather layer yet
Update new value of x
Update new value of x
Update new value of x
Get train function
Initialize
################################################################### DEBUG
self.test_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""label_placeholder""))"
self.test_weight_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
"name=""weight_placeholder""))"
TODO(rbharath): Should weights for the support be used?
Support labels
self.support_label_placeholder = Input(
"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
"name=""support_label_placeholder""))"
################################################################### DEBUG
Generate dictionary elements for support
Get graph information for test
Generate dictionary elements for test
Perform the optimization
Create different support sets
Get batch to try it out on
"Train on support set, batch pair"
Get featurization for test
"Shape (n_test, n_feat)"
Get featurization for support
"Shape (n_support, n_feat)"
Computes the inner part c() of the kernel
(the inset equation in section 2.1.1 of Matching networks paper).
Normalize
TODO(rbharath): euclidean kernel is broken!
elif self.similarity == 'euclidean':
"g = model_ops.euclidean_distance(test_feat, support_feat)"
"Note that gram matrix g has shape (n_test, n_support)"
"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
https://arxiv.org/pdf/1606.04080v1.pdf
"Computes softmax across axis 1, (so sums distances to support set for"
each test entry) to get attention vector
"Shape (n_test, n_support)"
Weighted sum of support labels
"Shape (n_support, 1)"
pred is yhat in eqn (1) of Matching Networks.
"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
"Clip softmax probabilities to range [epsilon, 1-epsilon]"
"Shape (n_test,)"
Convert to logit space using inverse sigmoid (logit) function
logit function: log(pred) - log(1-pred)
Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
in Cross Entropy calculation.
"Shape (n_test,)"
Get scores
Remove padded elements
Get scores
pred corresponds to prob(example == 1)
Remove padded elements
Get batches
TODO(rbharath): Add test for get_task_dataset_minus_support for
multitask case with missing data...
Join information for all tasks.
TODO(rbharath): Find a way to get rid of this import?
Extract model info
Get graph topology for x
Building outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
TODO(rbharath): Find a way to get rid of this import?
Obtain appropriate loss function
Extract model info
Get graph topology for x
############################################################ DEBUG
self.feat_dim = self.model.get_num_output_features()
############################################################ DEBUG
Raw logit outputs
Set epsilon
Initialize
"Path to save checkpoint files, which matches the"
replicated supervisor's default path.
Create target inputs
############################################################### DEBUG
"print(""multitask classifier"")"
"print(""feat"")"
print(feat)
############################################################### DEBUG
Get train function
TODO(rbharath): I believe this is total amount of data
Get graph information
"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
the number of labeled data points in target_i. This is to normalize each task
num_dat_dict = {self.num_datapoints_placeholder : self.}
Get other optimizer information
TODO(rbharath): Figure out how to handle phase appropriately
"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
"tensors of shape (batch_size,)"
Convert the labels into one-hot vector encodings.
Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
un-softmaxed logits rather than softmax outputs.
It's ok to divide by just the batch_size rather than the number of nonzero
examples (effect averages out)
Perform the optimization
TODO(rbharath): Disabling saving for now to try to debug.
run eval data through the model
"Shape (n_samples, n_tasks)"
run eval data through the model
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Merge mol conv objects
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
molecule * atom(graph) => step => features
molecule * atom(graph) => step
molecule * atom(graph) => step
Define the list of tensors to be used as topology
calculation orders for a batch of molecules
padding atom features vector of each molecule with 0
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
Generate dicts
self.n_atoms = n_atoms
Define the list of tensors to be used as topology
Extract atom numbers
number of atoms in each molecule
index of pair features
number of pairs for each atom
atom features
pair features
Generate dicts
Associate each atom with cell it belongs to. O(N*n_cells)
"Shape (n_cells, k)"
"Shape (N, 1)"
Associate each cell with its neighbor cells. Assumes periodic boundary
"conditions, so does wrapround. O(constant)"
"Shape (n_cells, 26)"
"Shape (N, 26)"
"coords of shape (N, ndim)"
"Shape (N, 26, k, ndim)"
"Shape (N, 26, k)"
"Shape (N, 26, k)"
"Shape (N, 26, k, ndim)"
"For smaller systems especially, the periodic boundary conditions can"
result in neighboring cells being seen multiple times. Maybe use tf.unique to
make sure duplicate neighbors are ignored?
TODO(rbharath): How does distance need to be modified here to
account for periodic boundary conditions?
"Shape (N, 26, k)"
"Shape (N, 26*k)"
TODO(rbharath): This will cause an issue with duplicates!
"Shape (N, M)"
"N elts of size (M,) each"
"Shape (N, 26*k)"
"N elts of size (26*k,) each"
"N elts of size (M,) each"
"Shape (N, M)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
"N tensors of shape (n_cells, 1)"
"Shape (N*n_cells, 1) after tile"
"List of N tensors of shape (n_cells, 1)"
Lists of length N
Lists of length n_cells
Get indices of k atoms closest to each cell point
TODO(rbharath): tf.stack for tf 1.0
"Tensor of shape (n_cells, k, ndim)"
atoms_in_cells = tf.stack(atoms_in_cells)
"Tensor of shape (26, k, ndim)"
"Reshape to (26*k, ndim)"
"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
"Dists of shape (26*k, 1)"
"Of shape (k, ndim)"
"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
TODO(rbharath): Change this for tf 1.0
"n_cells tensors of shape (N, 1)"
"Shape (N*n_cells, 1) after tile"
"List of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (k, ndim)"
"Tensor of shape (n_cells, k)"
TODO(rbharath):
- Need to find neighbors of the cells (+/- 1 in every dimension).
- Need to group closest atoms amongst cell neighbors
- Need to do another top_k to find indices of closest neighbors.
- Return N lists corresponding to neighbors for every atom.
TODO(rbharath): Do we need to handle periodic boundary conditions
TODO(rbharath): This doesn't handle boundaries well. We hard-code
"looking for 26 neighbors, which isn't right for boundary cells in"
the cube.
Number of neighbors of central cube in 3-space is
3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
TODO(rbharath)
n_cells = int(cells.get_shape()[0])
"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
"Tile (a, a, a, b, b, b, etc.)"
"Tile (a, b, c, a, b, c, ...)"
"Lists of n_cells tensors of shape (N, 1)"
Lists of length n_cells
Lists of length n_cells
Get indices of k atoms closest to each cell point
"n_cells tensors of shape (26,)"
TODO(rbharath): Make this handle minibatches
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, 3)"
"Shape (N_protein+N_ligand,)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M, 3)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
"Shape (N_protein+N_ligand, M)"
"Shape (N_protein+N_ligand, M)"
TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
computing free-energy. This implementation currently uses all interaction
terms. Not sure if this makes a difference.
"Shape (N_protein+N_ligand, M)"
Shape () -- scalar
# Gather Projection
"graph_model.add(dc.nn.Dense(128, activation='relu'))"
There should be 8 layers in graph_model
assert len(graph_model.layers) == 6
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Add layers
Need to add batch-norm separately to test/support due to differing
shapes.
Apply an attention lstm layer
Gather Projection
Degrees from 1 to max_deg inclusive
TODO(rbharath): Should this be 0 to max_deg inclusive?
"Should have shape (?, deg)"
"Shape of atom_features should be (?, n_feat)"
"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
TODO(rbharath): Check that this operation is differentiable.
The number of cells which we should theoretically have
The number of cells which we should theoretically have
"Each atom neighbors tensor should be (k, ndim) shaped."
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
The number of cells which we should theoretically have
TODO(rbharath): The test below only checks that shapes work out.
Need to do a correctness implementation vs. a simple CPU impl.
TODO(rbharath): Commenting this out due to weird segfaults
def test_vina_generate_conformers(self):
"""""""Test that Vina Model can generate conformers"""""""
data_dir = os.path.dirname(os.path.realpath(__file__))
"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
max_protein_atoms = 3500
max_ligand_atoms = 100
"print(""Loading protein file"")"
"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
protein_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
max_protein_atoms)
"print(""Loading ligand file"")"
"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
ligand_Z = pad_array(
"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
max_ligand_atoms)
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
Assembling train and valid datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Building tensorflow robust MultiTaskDNN model
Loading hyper parameters
Building tensorflow logistic regression model
Loading hyper parameters
Transform fingerprints to IRV features
Building tensorflow IRV model
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building xgboost classification model
Loading hyper parameters
Building tensorflow MultiTaskDNN model
Loading hyper parameters
Initialize model folder
Loading hyper parameters
Gather Projection
Loading hyper parameters
Loading hyper parameters
Loading hyper parameters
Building scikit random forest model
Loading hyper parameters
Building xgboost classification model
Loading hyperparameters
num positive/negative ligands
Set batch sizes for network
Model structure
Traning settings
Fit trained model
Evaluating low data model
-*- coding: utf-8 -*-
Assigning featurizer if not user defined
loading datasets
""
Note by @XericZephyr. Reason why I spun off this function:
1. Some model needs dataset information.
2. It offers us possibility to **cache** the dataset
"if the featurizer runs very slow, e.g., GraphConv."
2+. The cache can even happen at Travis CI to accelerate
CI testing.
""
loading datasets
!/usr/bin/env python2
-*- coding: utf-8 -*-
Featurize qm9 dataset
transformers = [
"deepchem.trans.LogTransformer(transform_X=True),"
"deepchem.trans.NormalizationTransformer(transform_y=True,"
dataset=train_dataset)]
Set shard size low to avoid memory problems.
############################################################# TIMING
############################################################# TIMING
Set some global variables up top
Featurize KAGGLE dataset
############################################################# TIMING
############################################################# TIMING
Featurize qm7 dataset
Featurize clintox dataset
Transform clintox dataset
Split clintox dataset
Featurize bbb dataset
Initialize transformers
Load nci dataset
Featurize nci dataset
Initialize transformers
Featurize HOPV dataset
Initialize transformers
Featurize PPB dataset
Initialize transformers
Load MUV dataset
Featurize MUV dataset
Initialize transformers
Featurize clearance dataset
Initialize transformers
Featurize TOXCAST dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize bace dataset
Initialize transformers
Featurize Tox21 dataset
Initialize transformers
Featurize ChEMBL dataset
Initialize transformers
Featurize hiv dataset
Initialize transformers
Featurize SIDER dataset
Initialize transformers
Featurize SAMPL dataset
Initialize transformers
Featurize Delaney dataset
Initialize transformers
Featurize PCBA dataset
Initialize transformers
Featurize Lipophilicity dataset
Initialize transformers
TODO(rbharath): This function is complicated and monolithic. Is there a nice
way to refactor this?
arbitrarily return last model
Define train dataset
Define validation dataset
TODO (Bowen): make this function less memory intensive
set 1st column as the column index of dataframe
merge descriptor and activities dataframe into output dataframe based on
"the molecule name, which is the index for both dataframes (but named"
differently). Default merge is inner merge
need to manually set dataframe indexname after merge based on index
from deepchem.scripts.dock_dude import *
from ipyparallel import Client
rc = Client()
dview = rc[:]
"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
""
"If mol_id is not set, then use isomeric smiles as unique identifier"
iterator = data_df.iterrows()
TODO(rbharath): BROKEN!
Trim unwanted indexing fields
Connect to running ipython server
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Maps from a function name to a dictionary that describes how to
map from an old argument keyword to the new argument keyword.
Mapping from function to the new name of the function
Functions that were reordered should be changed to the new keyword args
"for safety, if positional arguments are used. If you have reversed the"
"positional arguments yourself, this could do the wrong thing."
Specially handled functions.
TODO(aselle): Could check for a literal list of bools and try to convert
them to indices.
all edits are lists of chars
Iterate of each line
sort by column so that edits are processed in order in order to make
indexing adjustments cumulative for changes that change the string
length
"Extract each line to a list of characters, because mutable lists"
"are editable, unlike immutable strings."
Record a description of the change
Make underscore buffers for underlining where in the line the edit was
Iterate for each edit
"Create effective start, end by accounting for change in length due"
to previous edits
Make sure the edit is changing what it should be changing
Make the edit
Create the underline highlighting of the before and after
Keep track of how to generate effective ranges
Finish the report comment
"Strangely, ast.ListComp returns the col_offset of the first token"
after the '[' token which appears to be a bug. Workaround by
explicitly finding the real start of the list comprehension.
loop over lines
Reverse the text to and regular expression search for whitespace
First find if a [ can be found with only whitespace between it and
col.
TODO(aselle):
"this is poor comment detection, but it is good enough for"
cases where the comment does not contain string literal starting/
ending characters. If ast gave us start and end locations of the
"ast nodes rather than just start, we could use string literal"
node ranges to filter out spurious #'s that appear in string
literals.
"Most other nodes return proper locations (with notably does not), but"
it is not possible to use that in an argument.
"Find a simple attribute name path e.g. ""tf.foo.bar"""
Make sure the func is marked as being part of a call
Call special handlers
Examine any non-keyword argument and make it into a keyword argument
if reordering required.
Examine each keyword argument and convert it to the final renamed form
TODO(aselle): We should scan backward to find the start of the
keyword key. Unfortunately ast does not give you the location of
"keyword keys, so we are forced to infer it from the keyword arg"
value.
"Write to a temporary file, just in case we are doing an implace modify."
Broad exceptions are required here because ast throws whatever it wants.
pylint: disable=broad-except
pylint: enable=broad-except
make sure output directory doesn't exist
make sure output directory does not overlap with root_directory
Collect list of files to process (we do this to correctly handle if the
user puts the output directory in some sub directory of the input dir)
import os
"from deepchem.utils.save import load_from_disk, save_to_disk"
from deepchem.featurizers.fingerprints import CircularFingerprint
from deepchem.featurizers.basic import RDKitDescriptors
from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
from deepchem.featurizers.grid_featurizer import GridFeaturizer
from deepchem.featurizers.featurize import DataLoader
""
"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
"print(""About to load dataset form disk."")"
dataset = load_from_disk(dataset_file)
"print(""Loaded dataset."")"
""
grid_featurizer = GridFeaturizer(
"voxel_width=16.0, feature_types=""voxel_combined"","
"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
"""salt_bridge""], ecfp_power=9, splif_power=9,"
"parallel=True, flatten=True)"
featurizers = [CircularFingerprint(size=1024)]
"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
""
#Make a directory in which to store the featurized complexes.
"base_dir = ""../../../grid_nnscore_circular_features"""
if not os.path.exists(base_dir):
os.makedirs(base_dir)
"data_dir = os.path.join(base_dir, ""data"")"
if not os.path.exists(data_dir):
os.makedirs(data_dir)
""
"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
""
"feature_dir = os.path.join(base_dir, ""features"")"
if not os.path.exists(feature_dir):
os.makedirs(feature_dir)
""
"samples_dir = os.path.join(base_dir, ""samples"")"
if not os.path.exists(samples_dir):
os.makedirs(samples_dir)
""
""
""
featurizers = compound_featurizers + complex_featurizers
"featurizer = DataLoader(tasks=[""label""],"
"smiles_field=""smiles"","
"protein_pdb_field=""protein_pdb"","
"ligand_pdb_field=""ligand_pdb"","
"compound_featurizers=compound_featurizers,"
"complex_featurizers=complex_featurizers,"
"id_field=""complex_id"","
verbose=False)
from ipyparallel import Client
c = Client()
"print(""c.ids"")"
print(c.ids)
dview = c[:]
"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
"worker_pool=dview, shard_size=1024)"
""
"save_to_disk(featurized_samples, featurized_samples_file)"
"print(""Preparing ligand %s"" % mol_name)"
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
TODO(rbharath): Use standard joblib once old-data has been regenerated.
import joblib
First line of user-specified CSV *must* be header.
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): The semantics of this class are very difficult to debug.
"Multiple transformations of the data are performed on disk, and computations"
of mean/std are spread across multiple functions for efficiency. Some
refactoring needs to happen here.
TODO(rbharath): Still a bit of information leakage.
TODO(rbharath): FeaturizedSamples should not be responsible for
"X-transform, X_sums, etc. Move that stuff over to Dataset."
"input/output transforms not specified yet, so"
"self.transforms = (input_transforms, output_transforms) =>"
TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
"called without calling transform(), it will explode. Maybe have a separate"
initialization function to avoid this problem.
Store input_transforms/output_transforms so the dataset remembers its state.
TODO(rbharath): These lines are puzzling. Better way to avoid storage
duplication here?
Turns NaNs to zeros
"The following are all associated with Dataset, but are separate functions to"
make it easy to use multiprocessing.
TODO(rbharath): This is a hack. clean up.
TODO(rbharath): Should X be saved to out_X_transformed as well? Since
itershards expects to loop over X-transformed? (Ditto for y/w)
perform common train/test split across all tasks
Set missing data to have weight zero
Note that X_n is a list of floats
"Note y_n is a list of arrays of shape (n_tasks,)"
TODO(rbharath): This is a hack based on fact that multi-tasktype models
aren't supported.
"Sometimes all samples have zero weight. In this case, continue."
We need to import models so they can be created by model_builder
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
TODO(rbharath): There should be some automatic check to ensure that all
required model_params are specified.
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
Moving imports to be local to avoid isnstall issues with
"Convolution3D, which is not yet part of keras proper."
number of convolutional filters to use at each layer
level of pooling to perform at each layer (POOL x POOL)
level of convolution to perform at each layer (CONV x CONV)
"TODO(rbharath): If we change away from axis-size 32, this code will break."
Eventually figure out a more general rule that works for all axis sizes.
Note that keras requires the model architecture and weights to be stored
separately. A json file is generated that specifies the model architecture.
The weights will be stored in an h5 file. The pkl.gz file with store the
target name.
Save architecture
Add eps weight to avoid minibatches with zero weight (causes theano to crash).
"Class probabilities are predicted for classification outputs. Instead,"
output the most likely class.
TODO(rbharath): This does not work with very large datasets! sklearn does
"support partial_fit, but only for some models. Might make sense to make"
PartialSklearnModel subclass at some point to support large data models.
List of registered models
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
TODO(rbharath): The structure of the produced df might be
complicated. Better way to model?
TODO(rbharath): This feels like a total hack. Is there a structured way
to deal with this instead?
# The following notice is copied from the original NNScore file.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
"This is just a scaling factor, so it's set so as to keep the network"
inputs roughly contained in 0-1
"O-H distance is 0.96 A, N-H is 1.01 A. See"
http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
"If atoms are < 2.5 A apart, we count it as a close contact"
"If receptor and ligand atoms are > 4 A apart, we consider them"
unable to interact with simple electrostatics.
"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
"distance of 7.5 A is good cutoff. This seems really big to me,"
except that pi-pi interactions (parallel) are actually usually
off centered. Interesting paper.  Note that adenine and
"tryptophan count as two aromatic rings. So, for example, an"
"interaction between these two, if positioned correctly, could"
count for 4 pi-pi interactions.
Cation-pi interaction cutoff based on
"""Cation-pi interactions in structural biology."""
4  is good cutoff for salt bridges according to
"""Close-Range Electrostatic Interactions in Proteins"","
"but looking at complexes, I decided to go with 5.5 A"
This is perhaps controversial. I noticed that often a pi-cation
"interaction or other pi interaction was only slightly off, but"
"looking at the structure, it was clearly supposed to be a pi-cation"
interaction. I've decided then to artificially expand the radius of
"each pi ring. Think of this as adding in a VDW radius, or"
"accounting for poor crystal-structure resolution, or whatever you"
want to justify it.
note that dictionaries (hashtables) are passed by reference in python
Now see if there's hydrophobic contacts (C-C contacts)
to convert into J/mol; might be nice to double check this
TODO(bramsundar): What are units of
ligand_charge/receptor_charge?
"so they're more or less perpendicular, it's probably a"
"pi-edge interaction having looked at many structures, I"
noticed the algorithm was identifying T-pi reactions
"when the two rings were in fact quite distant, often"
"with other atoms in between. Eye-balling it, requiring"
that at their closest they be at least 5 A apart seems
to separate the good T's from the bad
"so at their closest points, the two rings come within"
5 A of each other.
"okay, is the ligand pi pointing into the receptor"
"pi, or the other way around?  first, project the"
center of the ligand pi onto the plane of the
"receptor pi, and vs. versa"
"This could be directional somehow, like a hydrogen"
bond.
"now, if it's a true pi-T interaction, this projected"
point should fall within the ring whose plane it's
been projected into.
so it is in the ring on the projected plane.
since it could be interacting with a cofactor or something
Now see if there's some sort of hydrogen bond between
"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
H_BOND_ANGLE.
TODO(rbharath): This is a horrible inner-loop search. Can
this be made more efficient?
Make sure to set comment (used below)
Make sure to set comment (used below)
"print ""nearby hydrogens: "" + str(hydrogens)"
now we need to check the angles
"TODO(rbharath): Rather than using this heuristic, it seems like"
it might be better to just report the angle in the feature
vector...
"so there could be some pi-pi interactions.  Now, let's"
check for stacking interactions. Are the two pi's roughly
parallel?
"so they're more or less parallel, it's probably pi-pi"
"stacking now, since pi-pi are not usually right on"
top of each other. They're often staggered. So I don't
want to just look at the centers of the rings and
compare. Let's look at each of the atoms.  do atom of
"the atoms of one ring, when projected onto the plane of"
"the other, fall within that other ring?"
start by assuming it's not a pi-pi stacking interaction
project the ligand atom onto the plane of the receptor ring
TODO(rbharath): This if-else is confusing.
project the ligand atom onto the plane of the receptor ring
since it could be interacting with a cofactor or something
project the charged onto the plane of the aromatic
since it could be interacting with a cofactor or something
now it's the ligand that has the aromatic group
since it could be interacting with a cofactor or something
so they have oppositve charges
TODO(rbharath): What is atom type A here?
Load receptor and ligand from file.
## OPEN TEMPDIR
## CLOSE TEMPDIR
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
"AddHydrogens(polaronly, correctForPH, pH)"
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
generate SMILES for fragments
import all Featurizer subclasses so __subclasses__ will work
these have to be local imports to avoid circular imports
get output from engines
get the maximum number of conformers
construct the new container
- first axis = # mols
- second axis = max # conformers
- remaining axes = determined by feature shape
fill in the container
"If gzipped, need to compute extension again"
"If CSV input, assume that first row contains labels"
Skip labels
"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
raw_df = pd.DataFrame.from_records(processed_rows)
"pandas rows are tuples (row_num, row_data)"
The standard columns for featurized data.
"compounds_df is not altered by any method after initialization, so it's"
safe to keep a copy in memory and on disk.
TODO(rbharath): Might this be inefficient?
Sort from largest to smallest scaffold sets
list-of-available-descriptors.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
Remove rings of length 0
"To remove duplicate entries, we convert rings from a list to set, and"
then back to a list again. There's a snafu since each ring in rings is
itself a list (and lists are unhashable in python). To circumvent this
"issue, we convert each ring into a string (after sorting). For example,"
"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
"original lists, we use ast.literal_eval."
Use dictionary to maintain state about which rings are supersets.
All distances are in Angstroms. Duplicate pairs not specified. For
"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
"This one not from source sited above. Not sure where it's from, but"
"it wouldn't ever be used in the current context (""AutoGrow"")"
estimate based on eye balling Handbook of Chemistry and Physics
Reset internal state
Now load the file into a list
"Load atom data (coordinates, etc.)"
this string unique identifies each atom
so each atom can only be loaded once. No rotamers.
So you're actually reindexing everything here.
### TODO(rbharath): Disabling loading of non
Check that the range is nonempty.
"just so no PDB is empty, VMD will load them all"
write coordinates
first get available index
now add atom
Add to non-protein list
Functions to determine the bond connectivity based on distance
==============================================================
Functions to identify positive charges
======================================
Metallic atoms are assumed to be cations.
Get all the quartenary amines on non-protein residues (these are the
only non-protein groups that will be identified as positively
charged). Note that nitrogen has only 5 valence electrons (out of 8
"for a full shell), so any nitrogen with four bonds must be positively"
charged (think NH4+).
"a quartenary amine, so it's easy"
so the indices stored is just the index of the nitrogen and any
attached atoms
"maybe you only have two hydrogens added, but they're sp3 hybridized."
"Just count this as a quartenary amine, since I think the positive"
charge would be stabilized. This situation can arise with
lone-pair electron nitrogen compounds like pyrrolidine
(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
Test that the angles approximately match the tetrahedral 109
degrees
so indexes added are the nitrogen and any attached atoms.
let's check for a phosphate or anything where a phosphorus is bound
"to two oxygens, where both oxygens are bound to only one heavy atom"
(the phosphorus). I think this will get several phosphorus
substances.
now count the number of oxygens bound only to the phosphorus
"let's check for guanidino-like groups (actually H2N-C-NH2,"
where not CN3.)
if the carbon has only three atoms connected to it
"if true, carbon is connected to at least two nitrogens now,"
so we need to count the number of nitrogens that are only
connected to one heavy atom (the carbon)
Index of atom that connects this charged group to
"the rest of the molecule, ultimately to make sure"
it's sp3 hybridized. Remains -1 if no such atom exists.
TODO(rbharath): Is picking the first non-nitrogen atom
correct here?
Handle case of guanidinium cation
so there are at two nitrogens that are only
connected to the carbon (and probably some
hydrogens)
now you need to make sure connector_ind atom is sp3 hybridized
"There are only two ""guanidino"" nitrogens. Assume the"
negative charge is spread equally between the two.
a carboxylate carbon will have three items connected to it.
a carboxylate will have two oxygens connected to
"it. Now, each of the oxygens should be connected"
to only one heavy atom (so if it's connected to a
"hydrogen, that's okay)"
so it's a carboxylate! Add a negative charge.
Assume negative charge is centered between the two
oxygens.
let's check for a sulfonate or anything where a sulfur is
bound to at least three oxygens and at least three are
bound to only the sulfur (or the sulfur and a hydrogen).
the sulfur is bound to at least three oxygens now
count the number of oxygens that are only bound to the
sulfur
so there are at least three oxygens that are only
bound to the sulfur
Group atoms in the same residue together
Assign each atom a residue key.
Handle edge case of last residue.
Select those atoms which are part of the charged group.
Functions to identify aromatic rings
====================================
first identify the center point
now get the plane that defines this ring. Recall that there are
atleast 3-points in indices_of_ring by ValueError above.
# formula for plane will be ax + by + cz = d
"first, let's see if the last atom in this ring is a carbon"
connected to four atoms. That would be a quick way of
telling this is not an aromatic ring
now check the dihedral between the ring atoms to see if
it's flat
"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
ring[ind+3] range of this function is -pi to pi
now check the dihedral between the ring atoms and an atom
connected to the current atom to see if that's flat too.
"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
"ring[ind+3], range of this function is -pi to pi"
Get all the rings containing each of the atoms in the ligand
Aromatic rings are of length 5 or 6
"Due to data errors in PDB files, there are cases in which"
non-protein atoms are bonded to protein atoms. Manually remove these
"cases, by testing that ring atom indices are a subset of non-protein"
ring indices.
Aromatic rings are flat
Aromatic rings are of length <= 6
At least 3 indices are required to identify the aromatic plane.
if self.get_aromatic_marker(indices_of_ring) is None:
"raise ValueError(""None at %s for %s"" % (key,"
str(indices_of_ring)))
Tryptophan has two aromatic rings.
Functions to assign secondary structure to protein residues
===========================================================
"first, we need to know what residues are available"
print self.get_residues()
TODO(rbharath): Why magic number 8?
now make sure the first four all have the same resid and
the last four all have the same resid
TODO(rbharath): Ugly code right here...
Now give easier to use names to the atoms
Now compute the phi and psi dihedral angles
Now use those angles to determine if it's alpha or beta
A residue of index i is only going to be in an alpha helix
its CA is within 6 A of the CA of the residue i + 3
so it's in an alpha helix
so now compare that CA to all the other CA's
so it's also in an alpha helix
so this CA atom is one of the ones the first atom
might hydrogen bond with
so these two CA atoms are close enough together
that their residues are probably hydrogen bonded
Alpha helices are only alpha helices if they span at least 4
residues (to wrap around and hydrogen bond). I'm going to
"require them to span at least 5 residues, based on"
examination of many structures.
now go through each of the BETA CA atoms. A residue is only
going to be called a beta sheet if CA atom is within 6.0 A
"of another CA beta, same chain, but index difference > 2."
so it's in a beta sheet
so not comparing an atom to itself
so you're comparing it only to other BETA-sheet atoms
so require them to be on the same chain. needed to
indices can be fairly compared
so the two residues are not simply adjacent to each
other on the chain
so these to atoms are close to each other
Now some more post-processing needs to be done. Do this
again to clear up mess that may have just been created
"(single residue beta strand, for example)"
Beta sheets are usually at least 3 residues long
so they are sequential
Now update each of the atoms with this structural information
Use this list to perform sanity checks on alpha-helix and beta-sheet
labels.
check for separate count and SMILES entries for each fragment
## 3zso comes from PDBBind-CN
The ligand is also specified by pdbbind
"Currently, just verifies that nothing crashes."
## 3zp9 comes from PDBBind-CN
The ligand is also specified by pdbbind
## 3bwf comes from PDBBind-CN
The ligand is also specified by pdbbind
"The keys of these dicts are pairs of atomtypes, but the keys are"
"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
"atom types, there are N*(N+1)/2 unique pairs."
TODO(rbharath): Charges are not computed correctly for certain
ligands! (see 2y2h_ligand). Understand why this happens.
assert np.count_nonzero(np.array(electrostatics.values())) > 0
print counts
1zea is the only example that has any pi-stacking.
Lengths:
ligand_receptor_close_contacts: N*(N+1)/2
ligand_receptor_contacts: N*(N+1)/2
ligand_receptor_electrostatics: N*(N+1)/2
ligand_atom_counts: N
hbonds: 12
hydrophobics: 6
stacking: 3
pi_cation: 6
t_shaped: 3
active_site_flexibility: 6
salt_bridges: 3
rotatable_boonds_count: 1
We need to import models so they can be created by model_builder
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
TODO(rbharath): Use standard joblib once old-data has been regenerated.
import joblib
First line of user-specified CSV *must* be header.
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): The semantics of this class are very difficult to debug.
"Multiple transformations of the data are performed on disk, and computations"
of mean/std are spread across multiple functions for efficiency. Some
refactoring needs to happen here.
TODO(rbharath): Still a bit of information leakage.
TODO(rbharath): FeaturizedSamples should not be responsible for
"X-transform, X_sums, etc. Move that stuff over to Dataset."
"input/output transforms not specified yet, so"
"self.transforms = (input_transforms, output_transforms) =>"
TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
"called without calling transform(), it will explode. Maybe have a separate"
initialization function to avoid this problem.
Store input_transforms/output_transforms so the dataset remembers its state.
TODO(rbharath): These lines are puzzling. Better way to avoid storage
duplication here?
Turns NaNs to zeros
"The following are all associated with Dataset, but are separate functions to"
make it easy to use multiprocessing.
TODO(rbharath): This is a hack. clean up.
TODO(rbharath): Should X be saved to out_X_transformed as well? Since
itershards expects to loop over X-transformed? (Ditto for y/w)
perform common train/test split across all tasks
Set missing data to have weight zero
Note that X_n is a list of floats
"Note y_n is a list of arrays of shape (n_tasks,)"
TODO(rbharath): This is a hack based on fact that multi-tasktype models
aren't supported.
"Sometimes all samples have zero weight. In this case, continue."
We need to import models so they can be created by model_builder
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
TODO(rbharath): There should be some automatic check to ensure that all
required model_params are specified.
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
Moving imports to be local to avoid isnstall issues with
"Convolution3D, which is not yet part of keras proper."
number of convolutional filters to use at each layer
level of pooling to perform at each layer (POOL x POOL)
level of convolution to perform at each layer (CONV x CONV)
"TODO(rbharath): If we change away from axis-size 32, this code will break."
Eventually figure out a more general rule that works for all axis sizes.
Note that keras requires the model architecture and weights to be stored
separately. A json file is generated that specifies the model architecture.
The weights will be stored in an h5 file. The pkl.gz file with store the
target name.
Save architecture
Add eps weight to avoid minibatches with zero weight (causes theano to crash).
"Class probabilities are predicted for classification outputs. Instead,"
output the most likely class.
TODO(rbharath): This does not work with very large datasets! sklearn does
"support partial_fit, but only for some models. Might make sense to make"
PartialSklearnModel subclass at some point to support large data models.
List of registered models
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
TODO(rbharath): The structure of the produced df might be
complicated. Better way to model?
TODO(rbharath): This feels like a total hack. Is there a structured way
to deal with this instead?
# The following notice is copied from the original NNScore file.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
"This is just a scaling factor, so it's set so as to keep the network"
inputs roughly contained in 0-1
"O-H distance is 0.96 A, N-H is 1.01 A. See"
http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
"If atoms are < 2.5 A apart, we count it as a close contact"
"If receptor and ligand atoms are > 4 A apart, we consider them"
unable to interact with simple electrostatics.
"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
"distance of 7.5 A is good cutoff. This seems really big to me,"
except that pi-pi interactions (parallel) are actually usually
off centered. Interesting paper.  Note that adenine and
"tryptophan count as two aromatic rings. So, for example, an"
"interaction between these two, if positioned correctly, could"
count for 4 pi-pi interactions.
Cation-pi interaction cutoff based on
"""Cation-pi interactions in structural biology."""
4  is good cutoff for salt bridges according to
"""Close-Range Electrostatic Interactions in Proteins"","
"but looking at complexes, I decided to go with 5.5 A"
This is perhaps controversial. I noticed that often a pi-cation
"interaction or other pi interaction was only slightly off, but"
"looking at the structure, it was clearly supposed to be a pi-cation"
interaction. I've decided then to artificially expand the radius of
"each pi ring. Think of this as adding in a VDW radius, or"
"accounting for poor crystal-structure resolution, or whatever you"
want to justify it.
note that dictionaries (hashtables) are passed by reference in python
Now see if there's hydrophobic contacts (C-C contacts)
to convert into J/mol; might be nice to double check this
TODO(bramsundar): What are units of
ligand_charge/receptor_charge?
"so they're more or less perpendicular, it's probably a"
"pi-edge interaction having looked at many structures, I"
noticed the algorithm was identifying T-pi reactions
"when the two rings were in fact quite distant, often"
"with other atoms in between. Eye-balling it, requiring"
that at their closest they be at least 5 A apart seems
to separate the good T's from the bad
"so at their closest points, the two rings come within"
5 A of each other.
"okay, is the ligand pi pointing into the receptor"
"pi, or the other way around?  first, project the"
center of the ligand pi onto the plane of the
"receptor pi, and vs. versa"
"This could be directional somehow, like a hydrogen"
bond.
"now, if it's a true pi-T interaction, this projected"
point should fall within the ring whose plane it's
been projected into.
so it is in the ring on the projected plane.
since it could be interacting with a cofactor or something
Now see if there's some sort of hydrogen bond between
"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
H_BOND_ANGLE.
TODO(rbharath): This is a horrible inner-loop search. Can
this be made more efficient?
Make sure to set comment (used below)
Make sure to set comment (used below)
"print ""nearby hydrogens: "" + str(hydrogens)"
now we need to check the angles
"TODO(rbharath): Rather than using this heuristic, it seems like"
it might be better to just report the angle in the feature
vector...
"so there could be some pi-pi interactions.  Now, let's"
check for stacking interactions. Are the two pi's roughly
parallel?
"so they're more or less parallel, it's probably pi-pi"
"stacking now, since pi-pi are not usually right on"
top of each other. They're often staggered. So I don't
want to just look at the centers of the rings and
compare. Let's look at each of the atoms.  do atom of
"the atoms of one ring, when projected onto the plane of"
"the other, fall within that other ring?"
start by assuming it's not a pi-pi stacking interaction
project the ligand atom onto the plane of the receptor ring
TODO(rbharath): This if-else is confusing.
project the ligand atom onto the plane of the receptor ring
since it could be interacting with a cofactor or something
project the charged onto the plane of the aromatic
since it could be interacting with a cofactor or something
now it's the ligand that has the aromatic group
since it could be interacting with a cofactor or something
so they have oppositve charges
TODO(rbharath): What is atom type A here?
Load receptor and ligand from file.
## OPEN TEMPDIR
## CLOSE TEMPDIR
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
"AddHydrogens(polaronly, correctForPH, pH)"
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
generate SMILES for fragments
import all Featurizer subclasses so __subclasses__ will work
these have to be local imports to avoid circular imports
get output from engines
get the maximum number of conformers
construct the new container
- first axis = # mols
- second axis = max # conformers
- remaining axes = determined by feature shape
fill in the container
"If gzipped, need to compute extension again"
"If CSV input, assume that first row contains labels"
Skip labels
"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
raw_df = pd.DataFrame.from_records(processed_rows)
"pandas rows are tuples (row_num, row_data)"
The standard columns for featurized data.
"compounds_df is not altered by any method after initialization, so it's"
safe to keep a copy in memory and on disk.
TODO(rbharath): Might this be inefficient?
Sort from largest to smallest scaffold sets
list-of-available-descriptors.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
Remove rings of length 0
"To remove duplicate entries, we convert rings from a list to set, and"
then back to a list again. There's a snafu since each ring in rings is
itself a list (and lists are unhashable in python). To circumvent this
"issue, we convert each ring into a string (after sorting). For example,"
"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
"original lists, we use ast.literal_eval."
Use dictionary to maintain state about which rings are supersets.
All distances are in Angstroms. Duplicate pairs not specified. For
"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
"This one not from source sited above. Not sure where it's from, but"
"it wouldn't ever be used in the current context (""AutoGrow"")"
estimate based on eye balling Handbook of Chemistry and Physics
Reset internal state
Now load the file into a list
"Load atom data (coordinates, etc.)"
this string unique identifies each atom
so each atom can only be loaded once. No rotamers.
So you're actually reindexing everything here.
### TODO(rbharath): Disabling loading of non
Check that the range is nonempty.
"just so no PDB is empty, VMD will load them all"
write coordinates
first get available index
now add atom
Add to non-protein list
Functions to determine the bond connectivity based on distance
==============================================================
Functions to identify positive charges
======================================
Metallic atoms are assumed to be cations.
Get all the quartenary amines on non-protein residues (these are the
only non-protein groups that will be identified as positively
charged). Note that nitrogen has only 5 valence electrons (out of 8
"for a full shell), so any nitrogen with four bonds must be positively"
charged (think NH4+).
"a quartenary amine, so it's easy"
so the indices stored is just the index of the nitrogen and any
attached atoms
"maybe you only have two hydrogens added, but they're sp3 hybridized."
"Just count this as a quartenary amine, since I think the positive"
charge would be stabilized. This situation can arise with
lone-pair electron nitrogen compounds like pyrrolidine
(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
Test that the angles approximately match the tetrahedral 109
degrees
so indexes added are the nitrogen and any attached atoms.
let's check for a phosphate or anything where a phosphorus is bound
"to two oxygens, where both oxygens are bound to only one heavy atom"
(the phosphorus). I think this will get several phosphorus
substances.
now count the number of oxygens bound only to the phosphorus
"let's check for guanidino-like groups (actually H2N-C-NH2,"
where not CN3.)
if the carbon has only three atoms connected to it
"if true, carbon is connected to at least two nitrogens now,"
so we need to count the number of nitrogens that are only
connected to one heavy atom (the carbon)
Index of atom that connects this charged group to
"the rest of the molecule, ultimately to make sure"
it's sp3 hybridized. Remains -1 if no such atom exists.
TODO(rbharath): Is picking the first non-nitrogen atom
correct here?
Handle case of guanidinium cation
so there are at two nitrogens that are only
connected to the carbon (and probably some
hydrogens)
now you need to make sure connector_ind atom is sp3 hybridized
"There are only two ""guanidino"" nitrogens. Assume the"
negative charge is spread equally between the two.
a carboxylate carbon will have three items connected to it.
a carboxylate will have two oxygens connected to
"it. Now, each of the oxygens should be connected"
to only one heavy atom (so if it's connected to a
"hydrogen, that's okay)"
so it's a carboxylate! Add a negative charge.
Assume negative charge is centered between the two
oxygens.
let's check for a sulfonate or anything where a sulfur is
bound to at least three oxygens and at least three are
bound to only the sulfur (or the sulfur and a hydrogen).
the sulfur is bound to at least three oxygens now
count the number of oxygens that are only bound to the
sulfur
so there are at least three oxygens that are only
bound to the sulfur
Group atoms in the same residue together
Assign each atom a residue key.
Handle edge case of last residue.
Select those atoms which are part of the charged group.
Functions to identify aromatic rings
====================================
first identify the center point
now get the plane that defines this ring. Recall that there are
atleast 3-points in indices_of_ring by ValueError above.
# formula for plane will be ax + by + cz = d
"first, let's see if the last atom in this ring is a carbon"
connected to four atoms. That would be a quick way of
telling this is not an aromatic ring
now check the dihedral between the ring atoms to see if
it's flat
"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
ring[ind+3] range of this function is -pi to pi
now check the dihedral between the ring atoms and an atom
connected to the current atom to see if that's flat too.
"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
"ring[ind+3], range of this function is -pi to pi"
Get all the rings containing each of the atoms in the ligand
Aromatic rings are of length 5 or 6
"Due to data errors in PDB files, there are cases in which"
non-protein atoms are bonded to protein atoms. Manually remove these
"cases, by testing that ring atom indices are a subset of non-protein"
ring indices.
Aromatic rings are flat
Aromatic rings are of length <= 6
At least 3 indices are required to identify the aromatic plane.
if self.get_aromatic_marker(indices_of_ring) is None:
"raise ValueError(""None at %s for %s"" % (key,"
str(indices_of_ring)))
Tryptophan has two aromatic rings.
Functions to assign secondary structure to protein residues
===========================================================
"first, we need to know what residues are available"
print self.get_residues()
TODO(rbharath): Why magic number 8?
now make sure the first four all have the same resid and
the last four all have the same resid
TODO(rbharath): Ugly code right here...
Now give easier to use names to the atoms
Now compute the phi and psi dihedral angles
Now use those angles to determine if it's alpha or beta
A residue of index i is only going to be in an alpha helix
its CA is within 6 A of the CA of the residue i + 3
so it's in an alpha helix
so now compare that CA to all the other CA's
so it's also in an alpha helix
so this CA atom is one of the ones the first atom
might hydrogen bond with
so these two CA atoms are close enough together
that their residues are probably hydrogen bonded
Alpha helices are only alpha helices if they span at least 4
residues (to wrap around and hydrogen bond). I'm going to
"require them to span at least 5 residues, based on"
examination of many structures.
now go through each of the BETA CA atoms. A residue is only
going to be called a beta sheet if CA atom is within 6.0 A
"of another CA beta, same chain, but index difference > 2."
so it's in a beta sheet
so not comparing an atom to itself
so you're comparing it only to other BETA-sheet atoms
so require them to be on the same chain. needed to
indices can be fairly compared
so the two residues are not simply adjacent to each
other on the chain
so these to atoms are close to each other
Now some more post-processing needs to be done. Do this
again to clear up mess that may have just been created
"(single residue beta strand, for example)"
Beta sheets are usually at least 3 residues long
so they are sequential
Now update each of the atoms with this structural information
Use this list to perform sanity checks on alpha-helix and beta-sheet
labels.
check for separate count and SMILES entries for each fragment
## 3zso comes from PDBBind-CN
The ligand is also specified by pdbbind
"Currently, just verifies that nothing crashes."
## 3zp9 comes from PDBBind-CN
The ligand is also specified by pdbbind
## 3bwf comes from PDBBind-CN
The ligand is also specified by pdbbind
"The keys of these dicts are pairs of atomtypes, but the keys are"
"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
"atom types, there are N*(N+1)/2 unique pairs."
TODO(rbharath): Charges are not computed correctly for certain
ligands! (see 2y2h_ligand). Understand why this happens.
assert np.count_nonzero(np.array(electrostatics.values())) > 0
print counts
1zea is the only example that has any pi-stacking.
Lengths:
ligand_receptor_close_contacts: N*(N+1)/2
ligand_receptor_contacts: N*(N+1)/2
ligand_receptor_electrostatics: N*(N+1)/2
ligand_atom_counts: N
hbonds: 12
hydrophobics: 6
stacking: 3
pi_cation: 6
t_shaped: 3
active_site_flexibility: 6
salt_bridges: 3
rotatable_boonds_count: 1
We need to import models so they can be created by model_builder
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
TODO(rbharath): Use standard joblib once old-data has been regenerated.
import joblib
First line of user-specified CSV *must* be header.
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): The semantics of this class are very difficult to debug.
"Multiple transformations of the data are performed on disk, and computations"
of mean/std are spread across multiple functions for efficiency. Some
refactoring needs to happen here.
TODO(rbharath): Still a bit of information leakage.
TODO(rbharath): FeaturizedSamples should not be responsible for
"X-transform, X_sums, etc. Move that stuff over to Dataset."
"input/output transforms not specified yet, so"
"self.transforms = (input_transforms, output_transforms) =>"
TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
"called without calling transform(), it will explode. Maybe have a separate"
initialization function to avoid this problem.
Store input_transforms/output_transforms so the dataset remembers its state.
TODO(rbharath): These lines are puzzling. Better way to avoid storage
duplication here?
Turns NaNs to zeros
"The following are all associated with Dataset, but are separate functions to"
make it easy to use multiprocessing.
TODO(rbharath): This is a hack. clean up.
TODO(rbharath): Should X be saved to out_X_transformed as well? Since
itershards expects to loop over X-transformed? (Ditto for y/w)
perform common train/test split across all tasks
Set missing data to have weight zero
Note that X_n is a list of floats
"Note y_n is a list of arrays of shape (n_tasks,)"
TODO(rbharath): This is a hack based on fact that multi-tasktype models
aren't supported.
"Sometimes all samples have zero weight. In this case, continue."
We need to import models so they can be created by model_builder
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
TODO(rbharath): There should be some automatic check to ensure that all
required model_params are specified.
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
Moving imports to be local to avoid isnstall issues with
"Convolution3D, which is not yet part of keras proper."
number of convolutional filters to use at each layer
level of pooling to perform at each layer (POOL x POOL)
level of convolution to perform at each layer (CONV x CONV)
"TODO(rbharath): If we change away from axis-size 32, this code will break."
Eventually figure out a more general rule that works for all axis sizes.
Note that keras requires the model architecture and weights to be stored
separately. A json file is generated that specifies the model architecture.
The weights will be stored in an h5 file. The pkl.gz file with store the
target name.
Save architecture
Add eps weight to avoid minibatches with zero weight (causes theano to crash).
"Class probabilities are predicted for classification outputs. Instead,"
output the most likely class.
TODO(rbharath): This does not work with very large datasets! sklearn does
"support partial_fit, but only for some models. Might make sense to make"
PartialSklearnModel subclass at some point to support large data models.
List of registered models
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
TODO(rbharath): The structure of the produced df might be
complicated. Better way to model?
TODO(rbharath): This feels like a total hack. Is there a structured way
to deal with this instead?
# The following notice is copied from the original NNScore file.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
"This is just a scaling factor, so it's set so as to keep the network"
inputs roughly contained in 0-1
"O-H distance is 0.96 A, N-H is 1.01 A. See"
http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
"If atoms are < 2.5 A apart, we count it as a close contact"
"If receptor and ligand atoms are > 4 A apart, we consider them"
unable to interact with simple electrostatics.
"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
"distance of 7.5 A is good cutoff. This seems really big to me,"
except that pi-pi interactions (parallel) are actually usually
off centered. Interesting paper.  Note that adenine and
"tryptophan count as two aromatic rings. So, for example, an"
"interaction between these two, if positioned correctly, could"
count for 4 pi-pi interactions.
Cation-pi interaction cutoff based on
"""Cation-pi interactions in structural biology."""
4  is good cutoff for salt bridges according to
"""Close-Range Electrostatic Interactions in Proteins"","
"but looking at complexes, I decided to go with 5.5 A"
This is perhaps controversial. I noticed that often a pi-cation
"interaction or other pi interaction was only slightly off, but"
"looking at the structure, it was clearly supposed to be a pi-cation"
interaction. I've decided then to artificially expand the radius of
"each pi ring. Think of this as adding in a VDW radius, or"
"accounting for poor crystal-structure resolution, or whatever you"
want to justify it.
note that dictionaries (hashtables) are passed by reference in python
Now see if there's hydrophobic contacts (C-C contacts)
to convert into J/mol; might be nice to double check this
TODO(bramsundar): What are units of
ligand_charge/receptor_charge?
"so they're more or less perpendicular, it's probably a"
"pi-edge interaction having looked at many structures, I"
noticed the algorithm was identifying T-pi reactions
"when the two rings were in fact quite distant, often"
"with other atoms in between. Eye-balling it, requiring"
that at their closest they be at least 5 A apart seems
to separate the good T's from the bad
"so at their closest points, the two rings come within"
5 A of each other.
"okay, is the ligand pi pointing into the receptor"
"pi, or the other way around?  first, project the"
center of the ligand pi onto the plane of the
"receptor pi, and vs. versa"
"This could be directional somehow, like a hydrogen"
bond.
"now, if it's a true pi-T interaction, this projected"
point should fall within the ring whose plane it's
been projected into.
so it is in the ring on the projected plane.
since it could be interacting with a cofactor or something
Now see if there's some sort of hydrogen bond between
"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
H_BOND_ANGLE.
TODO(rbharath): This is a horrible inner-loop search. Can
this be made more efficient?
Make sure to set comment (used below)
Make sure to set comment (used below)
"print ""nearby hydrogens: "" + str(hydrogens)"
now we need to check the angles
"TODO(rbharath): Rather than using this heuristic, it seems like"
it might be better to just report the angle in the feature
vector...
"so there could be some pi-pi interactions.  Now, let's"
check for stacking interactions. Are the two pi's roughly
parallel?
"so they're more or less parallel, it's probably pi-pi"
"stacking now, since pi-pi are not usually right on"
top of each other. They're often staggered. So I don't
want to just look at the centers of the rings and
compare. Let's look at each of the atoms.  do atom of
"the atoms of one ring, when projected onto the plane of"
"the other, fall within that other ring?"
start by assuming it's not a pi-pi stacking interaction
project the ligand atom onto the plane of the receptor ring
TODO(rbharath): This if-else is confusing.
project the ligand atom onto the plane of the receptor ring
since it could be interacting with a cofactor or something
project the charged onto the plane of the aromatic
since it could be interacting with a cofactor or something
now it's the ligand that has the aromatic group
since it could be interacting with a cofactor or something
so they have oppositve charges
TODO(rbharath): What is atom type A here?
Load receptor and ligand from file.
## OPEN TEMPDIR
## CLOSE TEMPDIR
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
"AddHydrogens(polaronly, correctForPH, pH)"
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
generate SMILES for fragments
import all Featurizer subclasses so __subclasses__ will work
these have to be local imports to avoid circular imports
get output from engines
get the maximum number of conformers
construct the new container
- first axis = # mols
- second axis = max # conformers
- remaining axes = determined by feature shape
fill in the container
"If gzipped, need to compute extension again"
"If CSV input, assume that first row contains labels"
Skip labels
"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
raw_df = pd.DataFrame.from_records(processed_rows)
"pandas rows are tuples (row_num, row_data)"
The standard columns for featurized data.
"compounds_df is not altered by any method after initialization, so it's"
safe to keep a copy in memory and on disk.
TODO(rbharath): Might this be inefficient?
Sort from largest to smallest scaffold sets
list-of-available-descriptors.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
Remove rings of length 0
"To remove duplicate entries, we convert rings from a list to set, and"
then back to a list again. There's a snafu since each ring in rings is
itself a list (and lists are unhashable in python). To circumvent this
"issue, we convert each ring into a string (after sorting). For example,"
"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
"original lists, we use ast.literal_eval."
Use dictionary to maintain state about which rings are supersets.
All distances are in Angstroms. Duplicate pairs not specified. For
"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
"This one not from source sited above. Not sure where it's from, but"
"it wouldn't ever be used in the current context (""AutoGrow"")"
estimate based on eye balling Handbook of Chemistry and Physics
Reset internal state
Now load the file into a list
"Load atom data (coordinates, etc.)"
this string unique identifies each atom
so each atom can only be loaded once. No rotamers.
So you're actually reindexing everything here.
### TODO(rbharath): Disabling loading of non
Check that the range is nonempty.
"just so no PDB is empty, VMD will load them all"
write coordinates
first get available index
now add atom
Add to non-protein list
Functions to determine the bond connectivity based on distance
==============================================================
Functions to identify positive charges
======================================
Metallic atoms are assumed to be cations.
Get all the quartenary amines on non-protein residues (these are the
only non-protein groups that will be identified as positively
charged). Note that nitrogen has only 5 valence electrons (out of 8
"for a full shell), so any nitrogen with four bonds must be positively"
charged (think NH4+).
"a quartenary amine, so it's easy"
so the indices stored is just the index of the nitrogen and any
attached atoms
"maybe you only have two hydrogens added, but they're sp3 hybridized."
"Just count this as a quartenary amine, since I think the positive"
charge would be stabilized. This situation can arise with
lone-pair electron nitrogen compounds like pyrrolidine
(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
Test that the angles approximately match the tetrahedral 109
degrees
so indexes added are the nitrogen and any attached atoms.
let's check for a phosphate or anything where a phosphorus is bound
"to two oxygens, where both oxygens are bound to only one heavy atom"
(the phosphorus). I think this will get several phosphorus
substances.
now count the number of oxygens bound only to the phosphorus
"let's check for guanidino-like groups (actually H2N-C-NH2,"
where not CN3.)
if the carbon has only three atoms connected to it
"if true, carbon is connected to at least two nitrogens now,"
so we need to count the number of nitrogens that are only
connected to one heavy atom (the carbon)
Index of atom that connects this charged group to
"the rest of the molecule, ultimately to make sure"
it's sp3 hybridized. Remains -1 if no such atom exists.
TODO(rbharath): Is picking the first non-nitrogen atom
correct here?
Handle case of guanidinium cation
so there are at two nitrogens that are only
connected to the carbon (and probably some
hydrogens)
now you need to make sure connector_ind atom is sp3 hybridized
"There are only two ""guanidino"" nitrogens. Assume the"
negative charge is spread equally between the two.
a carboxylate carbon will have three items connected to it.
a carboxylate will have two oxygens connected to
"it. Now, each of the oxygens should be connected"
to only one heavy atom (so if it's connected to a
"hydrogen, that's okay)"
so it's a carboxylate! Add a negative charge.
Assume negative charge is centered between the two
oxygens.
let's check for a sulfonate or anything where a sulfur is
bound to at least three oxygens and at least three are
bound to only the sulfur (or the sulfur and a hydrogen).
the sulfur is bound to at least three oxygens now
count the number of oxygens that are only bound to the
sulfur
so there are at least three oxygens that are only
bound to the sulfur
Group atoms in the same residue together
Assign each atom a residue key.
Handle edge case of last residue.
Select those atoms which are part of the charged group.
Functions to identify aromatic rings
====================================
first identify the center point
now get the plane that defines this ring. Recall that there are
atleast 3-points in indices_of_ring by ValueError above.
# formula for plane will be ax + by + cz = d
"first, let's see if the last atom in this ring is a carbon"
connected to four atoms. That would be a quick way of
telling this is not an aromatic ring
now check the dihedral between the ring atoms to see if
it's flat
"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
ring[ind+3] range of this function is -pi to pi
now check the dihedral between the ring atoms and an atom
connected to the current atom to see if that's flat too.
"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
"ring[ind+3], range of this function is -pi to pi"
Get all the rings containing each of the atoms in the ligand
Aromatic rings are of length 5 or 6
"Due to data errors in PDB files, there are cases in which"
non-protein atoms are bonded to protein atoms. Manually remove these
"cases, by testing that ring atom indices are a subset of non-protein"
ring indices.
Aromatic rings are flat
Aromatic rings are of length <= 6
At least 3 indices are required to identify the aromatic plane.
if self.get_aromatic_marker(indices_of_ring) is None:
"raise ValueError(""None at %s for %s"" % (key,"
str(indices_of_ring)))
Tryptophan has two aromatic rings.
Functions to assign secondary structure to protein residues
===========================================================
"first, we need to know what residues are available"
print self.get_residues()
TODO(rbharath): Why magic number 8?
now make sure the first four all have the same resid and
the last four all have the same resid
TODO(rbharath): Ugly code right here...
Now give easier to use names to the atoms
Now compute the phi and psi dihedral angles
Now use those angles to determine if it's alpha or beta
A residue of index i is only going to be in an alpha helix
its CA is within 6 A of the CA of the residue i + 3
so it's in an alpha helix
so now compare that CA to all the other CA's
so it's also in an alpha helix
so this CA atom is one of the ones the first atom
might hydrogen bond with
so these two CA atoms are close enough together
that their residues are probably hydrogen bonded
Alpha helices are only alpha helices if they span at least 4
residues (to wrap around and hydrogen bond). I'm going to
"require them to span at least 5 residues, based on"
examination of many structures.
now go through each of the BETA CA atoms. A residue is only
going to be called a beta sheet if CA atom is within 6.0 A
"of another CA beta, same chain, but index difference > 2."
so it's in a beta sheet
so not comparing an atom to itself
so you're comparing it only to other BETA-sheet atoms
so require them to be on the same chain. needed to
indices can be fairly compared
so the two residues are not simply adjacent to each
other on the chain
so these to atoms are close to each other
Now some more post-processing needs to be done. Do this
again to clear up mess that may have just been created
"(single residue beta strand, for example)"
Beta sheets are usually at least 3 residues long
so they are sequential
Now update each of the atoms with this structural information
Use this list to perform sanity checks on alpha-helix and beta-sheet
labels.
check for separate count and SMILES entries for each fragment
## 3zso comes from PDBBind-CN
The ligand is also specified by pdbbind
"Currently, just verifies that nothing crashes."
## 3zp9 comes from PDBBind-CN
The ligand is also specified by pdbbind
## 3bwf comes from PDBBind-CN
The ligand is also specified by pdbbind
"The keys of these dicts are pairs of atomtypes, but the keys are"
"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
"atom types, there are N*(N+1)/2 unique pairs."
TODO(rbharath): Charges are not computed correctly for certain
ligands! (see 2y2h_ligand). Understand why this happens.
assert np.count_nonzero(np.array(electrostatics.values())) > 0
print counts
1zea is the only example that has any pi-stacking.
Lengths:
ligand_receptor_close_contacts: N*(N+1)/2
ligand_receptor_contacts: N*(N+1)/2
ligand_receptor_electrostatics: N*(N+1)/2
ligand_atom_counts: N
hbonds: 12
hydrophobics: 6
stacking: 3
pi_cation: 6
t_shaped: 3
active_site_flexibility: 6
salt_bridges: 3
rotatable_boonds_count: 1
We need to import models so they can be created by model_builder
-*- coding: utf-8 -*-
""
"deepchem documentation build configuration file, created by"
sphinx-quickstart on Tue Jan 19 17:37:50 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"sys.path.insert(0, os.path.abspath('.'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
Example configuration for intersphinx: refer to the Python standard library.
lines in the label file have format
PDB-code Resolution Release-Year -logKd Kd reference ligand-name
"print line[0], line[3]"
TODO(rbharath): Use standard joblib once old-data has been regenerated.
import joblib
First line of user-specified CSV *must* be header.
working-with-3d-molecules
initial embedding
minimization and pruning
always keep lowest-energy conformer
discard conformers after max_conformers is reached
get RMSD to selected conformers
discard conformers within the RMSD threshold
create a new molecule to hold the chosen conformers
this ensures proper conformer IDs and energy-based ordering
TODO(rbharath): The semantics of this class are very difficult to debug.
"Multiple transformations of the data are performed on disk, and computations"
of mean/std are spread across multiple functions for efficiency. Some
refactoring needs to happen here.
TODO(rbharath): Still a bit of information leakage.
TODO(rbharath): FeaturizedSamples should not be responsible for
"X-transform, X_sums, etc. Move that stuff over to Dataset."
"input/output transforms not specified yet, so"
"self.transforms = (input_transforms, output_transforms) =>"
TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
"called without calling transform(), it will explode. Maybe have a separate"
initialization function to avoid this problem.
Store input_transforms/output_transforms so the dataset remembers its state.
TODO(rbharath): These lines are puzzling. Better way to avoid storage
duplication here?
Turns NaNs to zeros
"The following are all associated with Dataset, but are separate functions to"
make it easy to use multiprocessing.
TODO(rbharath): This is a hack. clean up.
TODO(rbharath): Should X be saved to out_X_transformed as well? Since
itershards expects to loop over X-transformed? (Ditto for y/w)
perform common train/test split across all tasks
Set missing data to have weight zero
Note that X_n is a list of floats
"Note y_n is a list of arrays of shape (n_tasks,)"
TODO(rbharath): This is a hack based on fact that multi-tasktype models
aren't supported.
"Sometimes all samples have zero weight. In this case, continue."
We need to import models so they can be created by model_builder
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
TODO(rbharath): There should be some automatic check to ensure that all
required model_params are specified.
Featurize input
Transform data into arrays for ML
Split into train/test
Transforming train/test data
Fit model
Eval model on train
Moving imports to be local to avoid isnstall issues with
"Convolution3D, which is not yet part of keras proper."
number of convolutional filters to use at each layer
level of pooling to perform at each layer (POOL x POOL)
level of convolution to perform at each layer (CONV x CONV)
"TODO(rbharath): If we change away from axis-size 32, this code will break."
Eventually figure out a more general rule that works for all axis sizes.
Note that keras requires the model architecture and weights to be stored
separately. A json file is generated that specifies the model architecture.
The weights will be stored in an h5 file. The pkl.gz file with store the
target name.
Save architecture
Add eps weight to avoid minibatches with zero weight (causes theano to crash).
"Class probabilities are predicted for classification outputs. Instead,"
output the most likely class.
TODO(rbharath): This does not work with very large datasets! sklearn does
"support partial_fit, but only for some models. Might make sense to make"
PartialSklearnModel subclass at some point to support large data models.
List of registered models
TODO(rbharath/enf): We need a structured way to deal with potential GPU
memory overflows.
TODO(rbharath): The structure of the produced df might be
complicated. Better way to model?
TODO(rbharath): This feels like a total hack. Is there a structured way
to deal with this instead?
# The following notice is copied from the original NNScore file.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
"This is just a scaling factor, so it's set so as to keep the network"
inputs roughly contained in 0-1
"O-H distance is 0.96 A, N-H is 1.01 A. See"
http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
"If atoms are < 2.5 A apart, we count it as a close contact"
"If receptor and ligand atoms are > 4 A apart, we consider them"
unable to interact with simple electrostatics.
"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
"distance of 7.5 A is good cutoff. This seems really big to me,"
except that pi-pi interactions (parallel) are actually usually
off centered. Interesting paper.  Note that adenine and
"tryptophan count as two aromatic rings. So, for example, an"
"interaction between these two, if positioned correctly, could"
count for 4 pi-pi interactions.
Cation-pi interaction cutoff based on
"""Cation-pi interactions in structural biology."""
4  is good cutoff for salt bridges according to
"""Close-Range Electrostatic Interactions in Proteins"","
"but looking at complexes, I decided to go with 5.5 A"
This is perhaps controversial. I noticed that often a pi-cation
"interaction or other pi interaction was only slightly off, but"
"looking at the structure, it was clearly supposed to be a pi-cation"
interaction. I've decided then to artificially expand the radius of
"each pi ring. Think of this as adding in a VDW radius, or"
"accounting for poor crystal-structure resolution, or whatever you"
want to justify it.
note that dictionaries (hashtables) are passed by reference in python
Now see if there's hydrophobic contacts (C-C contacts)
to convert into J/mol; might be nice to double check this
TODO(bramsundar): What are units of
ligand_charge/receptor_charge?
"so they're more or less perpendicular, it's probably a"
"pi-edge interaction having looked at many structures, I"
noticed the algorithm was identifying T-pi reactions
"when the two rings were in fact quite distant, often"
"with other atoms in between. Eye-balling it, requiring"
that at their closest they be at least 5 A apart seems
to separate the good T's from the bad
"so at their closest points, the two rings come within"
5 A of each other.
"okay, is the ligand pi pointing into the receptor"
"pi, or the other way around?  first, project the"
center of the ligand pi onto the plane of the
"receptor pi, and vs. versa"
"This could be directional somehow, like a hydrogen"
bond.
"now, if it's a true pi-T interaction, this projected"
point should fall within the ring whose plane it's
been projected into.
so it is in the ring on the projected plane.
since it could be interacting with a cofactor or something
Now see if there's some sort of hydrogen bond between
"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
H_BOND_ANGLE.
TODO(rbharath): This is a horrible inner-loop search. Can
this be made more efficient?
Make sure to set comment (used below)
Make sure to set comment (used below)
"print ""nearby hydrogens: "" + str(hydrogens)"
now we need to check the angles
"TODO(rbharath): Rather than using this heuristic, it seems like"
it might be better to just report the angle in the feature
vector...
"so there could be some pi-pi interactions.  Now, let's"
check for stacking interactions. Are the two pi's roughly
parallel?
"so they're more or less parallel, it's probably pi-pi"
"stacking now, since pi-pi are not usually right on"
top of each other. They're often staggered. So I don't
want to just look at the centers of the rings and
compare. Let's look at each of the atoms.  do atom of
"the atoms of one ring, when projected onto the plane of"
"the other, fall within that other ring?"
start by assuming it's not a pi-pi stacking interaction
project the ligand atom onto the plane of the receptor ring
TODO(rbharath): This if-else is confusing.
project the ligand atom onto the plane of the receptor ring
since it could be interacting with a cofactor or something
project the charged onto the plane of the aromatic
since it could be interacting with a cofactor or something
now it's the ligand that has the aromatic group
since it could be interacting with a cofactor or something
so they have oppositve charges
TODO(rbharath): What is atom type A here?
Load receptor and ligand from file.
## OPEN TEMPDIR
## CLOSE TEMPDIR
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
"AddHydrogens(polaronly, correctForPH, pH)"
a*x + b*y + c*z = dI think that
"self.x, self.y, self.z = x, y, z"
"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
TODO(bramsundar): Should this be __copy__?
"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
"return np.array([self.x, self.y, self.z])"
TODO(rbharath): Should this be an atom function?
"This line is necessary for babel to work, though many PDBs in"
the PDB would have this line commented out
now atom type (for pdbqt)
"If atomtype is not specified, but atomname is, set atomtype to the"
"first letter of atomname. This heuristic suffices for proteins,"
since no two-letter elements appear in standard amino acids.
Any number needs to be removed from the element name
"this only uses the rightmost three characters, essentially"
removing unique rotamer identification
"The normal vector to plane is n = [a, b, c]"
We first shift by basepoint (a point on given plane) to make math
simpler. basepoint is given by d/||n||^2 * n
The perpendicular component of diff to plane is
(n^T diff / ||n||^2) * n
generate SMILES for fragments
import all Featurizer subclasses so __subclasses__ will work
these have to be local imports to avoid circular imports
get output from engines
get the maximum number of conformers
construct the new container
- first axis = # mols
- second axis = max # conformers
- remaining axes = determined by feature shape
fill in the container
"If gzipped, need to compute extension again"
"If CSV input, assume that first row contains labels"
Skip labels
"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
raw_df = pd.DataFrame.from_records(processed_rows)
"pandas rows are tuples (row_num, row_data)"
The standard columns for featurized data.
"compounds_df is not altered by any method after initialization, so it's"
safe to keep a copy in memory and on disk.
TODO(rbharath): Might this be inefficient?
Sort from largest to smallest scaffold sets
list-of-available-descriptors.
NNScore 2.01 is released under the GNU General Public License (see
http://www.gnu.org/licenses/gpl.html).
"If you have any questions, comments, or suggestions, please don't"
"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
HERE].
Remove rings of length 0
"To remove duplicate entries, we convert rings from a list to set, and"
then back to a list again. There's a snafu since each ring in rings is
itself a list (and lists are unhashable in python). To circumvent this
"issue, we convert each ring into a string (after sorting). For example,"
"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
"original lists, we use ast.literal_eval."
Use dictionary to maintain state about which rings are supersets.
All distances are in Angstroms. Duplicate pairs not specified. For
"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
"This one not from source sited above. Not sure where it's from, but"
"it wouldn't ever be used in the current context (""AutoGrow"")"
estimate based on eye balling Handbook of Chemistry and Physics
Reset internal state
Now load the file into a list
"Load atom data (coordinates, etc.)"
this string unique identifies each atom
so each atom can only be loaded once. No rotamers.
So you're actually reindexing everything here.
### TODO(rbharath): Disabling loading of non
Check that the range is nonempty.
"just so no PDB is empty, VMD will load them all"
write coordinates
first get available index
now add atom
Add to non-protein list
Functions to determine the bond connectivity based on distance
==============================================================
Functions to identify positive charges
======================================
Metallic atoms are assumed to be cations.
Get all the quartenary amines on non-protein residues (these are the
only non-protein groups that will be identified as positively
charged). Note that nitrogen has only 5 valence electrons (out of 8
"for a full shell), so any nitrogen with four bonds must be positively"
charged (think NH4+).
"a quartenary amine, so it's easy"
so the indices stored is just the index of the nitrogen and any
attached atoms
"maybe you only have two hydrogens added, but they're sp3 hybridized."
"Just count this as a quartenary amine, since I think the positive"
charge would be stabilized. This situation can arise with
lone-pair electron nitrogen compounds like pyrrolidine
(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
Test that the angles approximately match the tetrahedral 109
degrees
so indexes added are the nitrogen and any attached atoms.
let's check for a phosphate or anything where a phosphorus is bound
"to two oxygens, where both oxygens are bound to only one heavy atom"
(the phosphorus). I think this will get several phosphorus
substances.
now count the number of oxygens bound only to the phosphorus
"let's check for guanidino-like groups (actually H2N-C-NH2,"
where not CN3.)
if the carbon has only three atoms connected to it
"if true, carbon is connected to at least two nitrogens now,"
so we need to count the number of nitrogens that are only
connected to one heavy atom (the carbon)
Index of atom that connects this charged group to
"the rest of the molecule, ultimately to make sure"
it's sp3 hybridized. Remains -1 if no such atom exists.
TODO(rbharath): Is picking the first non-nitrogen atom
correct here?
Handle case of guanidinium cation
so there are at two nitrogens that are only
connected to the carbon (and probably some
hydrogens)
now you need to make sure connector_ind atom is sp3 hybridized
"There are only two ""guanidino"" nitrogens. Assume the"
negative charge is spread equally between the two.
a carboxylate carbon will have three items connected to it.
a carboxylate will have two oxygens connected to
"it. Now, each of the oxygens should be connected"
to only one heavy atom (so if it's connected to a
"hydrogen, that's okay)"
so it's a carboxylate! Add a negative charge.
Assume negative charge is centered between the two
oxygens.
let's check for a sulfonate or anything where a sulfur is
bound to at least three oxygens and at least three are
bound to only the sulfur (or the sulfur and a hydrogen).
the sulfur is bound to at least three oxygens now
count the number of oxygens that are only bound to the
sulfur
so there are at least three oxygens that are only
bound to the sulfur
Group atoms in the same residue together
Assign each atom a residue key.
Handle edge case of last residue.
Select those atoms which are part of the charged group.
Functions to identify aromatic rings
====================================
first identify the center point
now get the plane that defines this ring. Recall that there are
atleast 3-points in indices_of_ring by ValueError above.
# formula for plane will be ax + by + cz = d
"first, let's see if the last atom in this ring is a carbon"
connected to four atoms. That would be a quick way of
telling this is not an aromatic ring
now check the dihedral between the ring atoms to see if
it's flat
"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
ring[ind+3] range of this function is -pi to pi
now check the dihedral between the ring atoms and an atom
connected to the current atom to see if that's flat too.
"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
"ring[ind+3], range of this function is -pi to pi"
Get all the rings containing each of the atoms in the ligand
Aromatic rings are of length 5 or 6
"Due to data errors in PDB files, there are cases in which"
non-protein atoms are bonded to protein atoms. Manually remove these
"cases, by testing that ring atom indices are a subset of non-protein"
ring indices.
Aromatic rings are flat
Aromatic rings are of length <= 6
At least 3 indices are required to identify the aromatic plane.
if self.get_aromatic_marker(indices_of_ring) is None:
"raise ValueError(""None at %s for %s"" % (key,"
str(indices_of_ring)))
Tryptophan has two aromatic rings.
Functions to assign secondary structure to protein residues
===========================================================
"first, we need to know what residues are available"
print self.get_residues()
TODO(rbharath): Why magic number 8?
now make sure the first four all have the same resid and
the last four all have the same resid
TODO(rbharath): Ugly code right here...
Now give easier to use names to the atoms
Now compute the phi and psi dihedral angles
Now use those angles to determine if it's alpha or beta
A residue of index i is only going to be in an alpha helix
its CA is within 6 A of the CA of the residue i + 3
so it's in an alpha helix
so now compare that CA to all the other CA's
so it's also in an alpha helix
so this CA atom is one of the ones the first atom
might hydrogen bond with
so these two CA atoms are close enough together
that their residues are probably hydrogen bonded
Alpha helices are only alpha helices if they span at least 4
residues (to wrap around and hydrogen bond). I'm going to
"require them to span at least 5 residues, based on"
examination of many structures.
now go through each of the BETA CA atoms. A residue is only
going to be called a beta sheet if CA atom is within 6.0 A
"of another CA beta, same chain, but index difference > 2."
so it's in a beta sheet
so not comparing an atom to itself
so you're comparing it only to other BETA-sheet atoms
so require them to be on the same chain. needed to
indices can be fairly compared
so the two residues are not simply adjacent to each
other on the chain
so these to atoms are close to each other
Now some more post-processing needs to be done. Do this
again to clear up mess that may have just been created
"(single residue beta strand, for example)"
Beta sheets are usually at least 3 residues long
so they are sequential
Now update each of the atoms with this structural information
Use this list to perform sanity checks on alpha-helix and beta-sheet
labels.
check for separate count and SMILES entries for each fragment
## 3zso comes from PDBBind-CN
The ligand is also specified by pdbbind
"Currently, just verifies that nothing crashes."
## 3zp9 comes from PDBBind-CN
The ligand is also specified by pdbbind
## 3bwf comes from PDBBind-CN
The ligand is also specified by pdbbind
"The keys of these dicts are pairs of atomtypes, but the keys are"
"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
"atom types, there are N*(N+1)/2 unique pairs."
TODO(rbharath): Charges are not computed correctly for certain
ligands! (see 2y2h_ligand). Understand why this happens.
assert np.count_nonzero(np.array(electrostatics.values())) > 0
print counts
1zea is the only example that has any pi-stacking.
Lengths:
ligand_receptor_close_contacts: N*(N+1)/2
ligand_receptor_contacts: N*(N+1)/2
ligand_receptor_electrostatics: N*(N+1)/2
ligand_atom_counts: N
hbonds: 12
hydrophobics: 6
stacking: 3
pi_cation: 6
t_shaped: 3
active_site_flexibility: 6
salt_bridges: 3
rotatable_boonds_count: 1
We need to import models so they can be created by model_builder
